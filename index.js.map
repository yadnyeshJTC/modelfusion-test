{"version":3,"sources":["../src/core/DefaultRun.ts","../src/core/FunctionEventSource.ts","../src/core/ModelFusionConfiguration.ts","../src/core/PromptFunction.ts","../src/core/api/AbortError.ts","../src/core/api/ApiCallError.ts","../src/core/api/ApiFacade.ts","../src/core/api/retryNever.ts","../src/util/delay.ts","../src/util/getErrorMessage.ts","../src/core/api/RetryError.ts","../src/core/api/retryWithExponentialBackoff.ts","../src/core/api/throttleMaxConcurrency.ts","../src/core/api/throttleOff.ts","../src/core/api/AbstractApiConfiguration.ts","../src/core/api/BaseUrlApiConfiguration.ts","../src/core/cache/MemoryCache.ts","../src/core/executeFunctionCall.ts","../src/core/getFunctionCallLogger.ts","../src/util/detectRuntime.ts","../src/core/getRun.ts","../src/util/DurationMeasurement.ts","../src/util/runSafe.ts","../src/core/executeFunction.ts","../src/core/schema/JSONParseError.ts","../src/core/schema/TypeValidationError.ts","../src/core/schema/UncheckedSchema.ts","../src/core/schema/ZodSchema.ts","../src/core/schema/parseJSON.ts","../src/core/schema/validateTypes.ts","../src/util/cosineSimilarity.ts","../src/model-function/executeStandardCall.ts","../src/model-function/embed/embed.ts","../src/model-function/classify/EmbeddingSimilarityClassifier.ts","../src/model-function/classify/classify.ts","../src/model-function/generate-image/PromptTemplateImageGenerationModel.ts","../src/util/format/UInt8Utils.ts","../src/model-function/generate-image/generateImage.ts","../src/model-function/generate-speech/generateSpeech.ts","../src/util/AsyncQueue.ts","../src/model-function/executeStreamCall.ts","../src/model-function/generate-speech/streamSpeech.ts","../src/model-function/generate-text/generateText.ts","../src/model-function/generate-object/ObjectParseError.ts","../src/model-function/generate-object/ObjectFromTextGenerationModel.ts","../src/model-function/generate-text/streamText.ts","../src/util/parsePartialJson.ts","../src/util/fixJson.ts","../src/model-function/generate-object/ObjectFromTextStreamingModel.ts","../src/model-function/generate-object/ObjectStream.ts","../src/model-function/generate-object/ObjectValidationError.ts","../src/model-function/generate-object/generateObject.ts","../src/model-function/generate-object/jsonObjectPrompt.ts","../src/util/isDeepEqualData.ts","../src/model-function/generate-object/streamObject.ts","../src/tool/generate-tool-call/ToolCallParseError.ts","../src/tool/generate-tool-call/TextGenerationToolCallModel.ts","../src/tool/generate-tool-calls/ToolCallsParseError.ts","../src/tool/generate-tool-calls/TextGenerationToolCallsModel.ts","../src/model-function/generate-text/PromptTemplateTextGenerationModel.ts","../src/model-function/generate-text/PromptTemplateTextStreamingModel.ts","../src/model-function/generate-text/PromptTemplateFullTextModel.ts","../src/model-function/generate-text/TextGenerationModel.ts","../src/model-function/generate-text/prompt-template/AlpacaPromptTemplate.ts","../src/model-function/generate-text/prompt-template/InvalidPromptError.ts","../src/model-function/generate-text/prompt-template/ContentPart.ts","../src/model-function/generate-text/prompt-template/ChatMLPromptTemplate.ts","../src/model-function/generate-text/prompt-template/ChatPrompt.ts","../src/model-function/generate-text/prompt-template/InstructionPrompt.ts","../src/model-function/generate-text/prompt-template/Llama2PromptTemplate.ts","../src/model-function/generate-text/prompt-template/MistralInstructPromptTemplate.ts","../src/model-function/generate-text/prompt-template/NeuralChatPromptTemplate.ts","../src/model-function/generate-text/prompt-template/SynthiaPromptTemplate.ts","../src/model-function/generate-text/prompt-template/TextPrompt.ts","../src/model-function/generate-text/prompt-template/TextPromptTemplate.ts","../src/model-function/generate-text/prompt-template/VicunaPromptTemplate.ts","../src/model-function/generate-text/prompt-template/trimChatPrompt.ts","../src/model-function/generate-transcription/generateTranscription.ts","../src/model-function/tokenize-text/countTokens.ts","../src/model-provider/automatic1111/Automatic1111ApiConfiguration.ts","../src/model-provider/automatic1111/Automatic1111Error.ts","../src/util/format/DataContent.ts","../src/core/api/postToApi.ts","../src/model-provider/automatic1111/Automatic1111Facade.ts","../src/model-provider/automatic1111/Automatic1111ImageGenerationModel.ts","../src/core/api/callWithRetryAndThrottle.ts","../src/model-function/AbstractModel.ts","../src/model-provider/automatic1111/Automatic1111ImageGenerationPrompt.ts","../src/core/api/LoadAPIKeyError.ts","../src/core/api/loadApiKey.ts","../src/model-provider/cohere/CohereApiConfiguration.ts","../src/model-provider/cohere/CohereError.ts","../src/model-provider/cohere/CohereFacade.ts","../src/model-provider/cohere/CohereTextEmbeddingModel.ts","../src/model-provider/cohere/CohereTokenizer.ts","../src/model-provider/cohere/CohereTextGenerationModel.ts","../src/util/streaming/parseJsonStream.ts","../src/util/streaming/parseJsonStreamAsAsyncIterable.ts","../src/util/streaming/createJsonStreamResponseHandler.ts","../src/model-provider/elevenlabs/ElevenLabsApiConfiguration.ts","../src/model-provider/elevenlabs/ElevenLabsFacade.ts","../src/model-provider/elevenlabs/ElevenLabsSpeechModel.ts","../src/util/SimpleWebSocket.ts","../src/model-provider/huggingface/HuggingFaceApiConfiguration.ts","../src/model-provider/huggingface/HuggingFaceError.ts","../src/model-provider/huggingface/HuggingFaceFacade.ts","../src/model-provider/huggingface/HuggingFaceTextEmbeddingModel.ts","../src/model-provider/huggingface/HuggingFaceTextGenerationModel.ts","../src/model-provider/llamacpp/LlamaCppApiConfiguration.ts","../src/model-provider/llamacpp/LlamaCppCompletionModel.ts","../src/util/streaming/convertReadableStreamToAsyncIterable.ts","../src/util/streaming/EventSourceParserStream.ts","../src/util/streaming/parseEventSourceStream.ts","../src/model-provider/llamacpp/LlamaCppError.ts","../src/model-provider/llamacpp/LlamaCppPrompt.ts","../src/model-provider/llamacpp/LlamaCppBakLLaVA1PromptTemplate.ts","../src/model-provider/llamacpp/LlamaCppTokenizer.ts","../src/model-provider/llamacpp/convertJsonSchemaToGBNF.ts","../src/model-provider/llamacpp/LlamaCppFacade.ts","../src/model-provider/llamacpp/LlamaCppTextEmbeddingModel.ts","../src/model-provider/llamacpp/LlamaCppGrammars.ts","../src/model-provider/lmnt/LmntApiConfiguration.ts","../src/model-provider/lmnt/LmntFacade.ts","../src/model-provider/lmnt/LmntSpeechModel.ts","../src/model-provider/mistral/MistralApiConfiguration.ts","../src/model-provider/mistral/MistralChatModel.ts","../src/util/streaming/parseEventSourceStreamAsAsyncIterable.ts","../src/util/streaming/createEventSourceResponseHandler.ts","../src/model-provider/mistral/MistralChatPromptTemplate.ts","../src/model-provider/mistral/MistralError.ts","../src/model-provider/mistral/MistralFacade.ts","../src/model-provider/mistral/MistralTextEmbeddingModel.ts","../src/model-provider/ollama/OllamaApiConfiguration.ts","../src/model-provider/ollama/OllamaChatModel.ts","../src/model-provider/ollama/OllamaChatPromptTemplate.ts","../src/model-provider/ollama/OllamaError.ts","../src/model-provider/ollama/OllamaCompletionModel.ts","../src/model-provider/ollama/OllamaCompletionPrompt.ts","../src/model-provider/ollama/OllamaFacade.ts","../src/model-provider/ollama/OllamaTextEmbeddingModel.ts","../src/model-provider/openai/AbstractOpenAIChatModel.ts","../src/model-provider/openai/OpenAIApiConfiguration.ts","../src/model-provider/openai/OpenAIError.ts","../src/model-provider/openai/AbstractOpenAICompletionModel.ts","../src/model-provider/openai/AbstractOpenAITextEmbeddingModel.ts","../src/model-provider/openai/AzureOpenAIApiConfiguration.ts","../src/model-provider/openai/OpenAIChatMessage.ts","../src/model-provider/openai/OpenAIChatFunctionCallObjectGenerationModel.ts","../src/model-provider/openai/OpenAIChatPromptTemplate.ts","../src/model-provider/openai/TikTokenTokenizer.ts","../src/util/never.ts","../src/model-provider/openai/countOpenAIChatMessageTokens.ts","../src/model-provider/openai/OpenAIChatModel.ts","../src/model-provider/openai/OpenAICompletionModel.ts","../src/model-provider/openai/OpenAIFacade.ts","../src/model-provider/openai/OpenAIImageGenerationModel.ts","../src/model-provider/openai/OpenAISpeechModel.ts","../src/model-provider/openai/OpenAITextEmbeddingModel.ts","../src/model-provider/openai/OpenAITranscriptionModel.ts","../src/util/audio/getAudioFileExtension.ts","../src/model-provider/openai-compatible/FireworksAIApiConfiguration.ts","../src/model-provider/openai-compatible/OpenAICompatibleChatModel.ts","../src/model-provider/openai-compatible/OpenAICompatibleCompletionModel.ts","../src/model-provider/openai-compatible/OpenAICompatibleFacade.ts","../src/model-provider/openai-compatible/OpenAICompatibleTextEmbeddingModel.ts","../src/model-provider/openai-compatible/PerplexityApiConfiguration.ts","../src/model-provider/openai-compatible/TogetherAIApiConfiguration.ts","../src/model-provider/stability/StabilityApiConfiguration.ts","../src/model-provider/stability/StabilityError.ts","../src/model-provider/stability/StabilityFacade.ts","../src/model-provider/stability/StabilityImageGenerationModel.ts","../src/model-provider/stability/StabilityImageGenerationPrompt.ts","../src/model-provider/whispercpp/WhisperCppApiConfiguration.ts","../src/model-provider/whispercpp/WhisperCppFacade.ts","../src/model-provider/whispercpp/WhisperCppTranscriptionModel.ts","../src/observability/helicone/HeliconeOpenAIApiConfiguration.ts","../src/retriever/retrieve.ts","../src/text-chunk/split/splitOnSeparator.ts","../src/text-chunk/split/splitRecursively.ts","../src/text-chunk/split/splitTextChunks.ts","../src/tool/NoSuchToolDefinitionError.ts","../src/tool/Tool.ts","../src/tool/ObjectGeneratorTool.ts","../src/tool/ToolCallArgumentsValidationError.ts","../src/tool/ToolCallError.ts","../src/tool/ToolCallGenerationError.ts","../src/tool/ToolExecutionError.ts","../src/tool/WebSearchTool.ts","../src/tool/execute-tool/executeTool.ts","../src/tool/generate-tool-call/generateToolCall.ts","../src/tool/generate-tool-call/jsonToolCallPrompt.ts","../src/tool/generate-tool-calls/generateToolCalls.ts","../src/tool/execute-tool/safeExecuteToolCall.ts","../src/tool/run-tool/runTool.ts","../src/tool/run-tools/runTools.ts","../src/util/streaming/createEventSourceStream.ts","../src/vector-index/VectorIndexRetriever.ts","../src/vector-index/memory/MemoryVectorIndex.ts","../src/vector-index/upsertIntoVectorIndex.ts"],"sourcesContent":["import { nanoid as createId } from \"nanoid\";\nimport { ErrorHandler } from \"../util/ErrorHandler\";\nimport { FunctionEvent } from \"./FunctionEvent\";\nimport { FunctionEventSource } from \"./FunctionEventSource\";\nimport { FunctionObserver } from \"./FunctionObserver\";\nimport { Run } from \"./Run\";\n\nimport { ModelCallFinishedEvent } from \"../model-function/ModelCallEvent\";\n\nexport class DefaultRun implements Run {\n  readonly runId: string;\n  readonly sessionId?: string;\n  readonly userId?: string;\n\n  readonly abortSignal?: AbortSignal;\n\n  readonly errorHandler: ErrorHandler;\n\n  readonly events: FunctionEvent[] = [];\n\n  private functionEventSource: FunctionEventSource;\n\n  constructor({\n    runId = `run-${createId()}`,\n    sessionId,\n    userId,\n    abortSignal,\n    observers,\n    errorHandler,\n  }: {\n    runId?: string;\n    sessionId?: string;\n    userId?: string;\n    abortSignal?: AbortSignal;\n    observers?: FunctionObserver[];\n    errorHandler?: ErrorHandler;\n  } = {}) {\n    this.runId = runId;\n    this.sessionId = sessionId;\n    this.userId = userId;\n    this.abortSignal = abortSignal;\n\n    this.errorHandler = errorHandler ?? ((error) => console.error(error));\n\n    this.functionEventSource = new FunctionEventSource({\n      observers: observers ?? [],\n      errorHandler: this.errorHandler.bind(this),\n    });\n  }\n\n  readonly functionObserver = {\n    onFunctionEvent: (event: FunctionEvent) => {\n      this.events.push(event);\n      this.functionEventSource.notify(event);\n    },\n  };\n\n  getSuccessfulModelCalls() {\n    return this.events.filter(\n      (\n        event\n      ): event is ModelCallFinishedEvent & { result: { status: \"success\" } } =>\n        \"model\" in event &&\n        \"result\" in event &&\n        \"status\" in event.result &&\n        event.result.status === \"success\"\n    );\n  }\n}\n","import { ErrorHandler } from \"../util/ErrorHandler\";\nimport { FunctionEvent } from \"./FunctionEvent\";\nimport { FunctionObserver } from \"./FunctionObserver\";\n\nexport class FunctionEventSource {\n  readonly observers: FunctionObserver[];\n  readonly errorHandler: ErrorHandler;\n\n  constructor({\n    observers,\n    errorHandler,\n  }: {\n    observers: FunctionObserver[];\n    errorHandler?: ErrorHandler;\n  }) {\n    this.observers = observers;\n    this.errorHandler = errorHandler ?? ((error) => console.error(error));\n  }\n\n  notify(event: FunctionEvent) {\n    for (const observer of this.observers) {\n      try {\n        observer.onFunctionEvent(event);\n      } catch (error) {\n        this.errorHandler(error);\n      }\n    }\n  }\n}\n","import { FunctionObserver } from \"./FunctionObserver\";\nimport { LogFormat } from \"./LogFormat\";\n\nlet globalLogFormat: LogFormat = undefined;\nlet globalFunctionObservers: FunctionObserver[] = [];\n\nexport function setFunctionObservers(\n  functionObservers: FunctionObserver[]\n): void {\n  globalFunctionObservers = functionObservers;\n}\n\nexport function getFunctionObservers(): FunctionObserver[] {\n  return globalFunctionObservers;\n}\n\nexport function setLogFormat(format: LogFormat): void {\n  globalLogFormat = format;\n}\n\nexport function getLogFormat(): LogFormat {\n  return globalLogFormat;\n}\n","export type PromptFunction<INPUT, PROMPT> = (() => PromiseLike<{\n  input: INPUT;\n  prompt: PROMPT;\n}>) & {\n  [promptFunctionMarker]: true;\n};\n\nconst promptFunctionMarker = Symbol(\"promptFunction\");\n\n/**\n * Expands a prompt into its full form if it is a PromptFunction, otherwise returns the prompt as is.\n * @param {PROMPT | PromptFunction<unknown, PROMPT>} prompt - The prompt to expand.\n * @returns {Promise<{input: unknown; prompt: PROMPT;}>} - The expanded prompt.\n */\nexport async function expandPrompt<PROMPT>(\n  prompt: PROMPT | PromptFunction<unknown, PROMPT>\n): Promise<{\n  input: unknown;\n  prompt: PROMPT;\n}> {\n  return isPromptFunction(prompt) ? await prompt() : { input: prompt, prompt };\n}\n\n/**\n * Marks a function as a PromptFunction by setting a unique symbol.\n * @param {() => PromiseLike<{input: INPUT; prompt: PROMPT;}>} fn - The function to mark.\n * @returns {PromptFunction<INPUT, PROMPT>} - The marked function.\n */\nexport function markAsPromptFunction<INPUT, PROMPT>(\n  fn: () => PromiseLike<{\n    input: INPUT;\n    prompt: PROMPT;\n  }>\n): PromptFunction<INPUT, PROMPT> {\n  // Set the promptFunctionMarker on the function\n  // eslint-disable-next-line @typescript-eslint/no-explicit-any\n  (fn as any)[promptFunctionMarker] = true;\n\n  return fn as unknown as PromptFunction<INPUT, PROMPT>;\n}\n\n/**\n * Checks if a function is a PromptFunction by checking for the unique symbol.\n * @param {unknown} fn - The function to check.\n * @returns {boolean} - True if the function is a PromptFunction, false otherwise.\n */\nexport function isPromptFunction<INPUT, PROMPT>(\n  fn: unknown\n): fn is PromptFunction<INPUT, PROMPT> {\n  // eslint-disable-next-line @typescript-eslint/no-explicit-any\n  const hasMarker = (fn as any)[promptFunctionMarker] === true;\n  const isFunction = typeof fn === \"function\";\n\n  return hasMarker && isFunction;\n}\n","export class AbortError extends Error {\n  constructor(message = \"Aborted\") {\n    super(message);\n  }\n}\n","export class ApiCallError extends Error {\n  readonly url: string;\n  readonly requestBodyValues: unknown;\n  readonly statusCode?: number;\n  readonly responseBody?: string;\n  readonly cause?: unknown;\n  readonly isRetryable: boolean;\n  readonly data?: unknown;\n\n  constructor({\n    message,\n    url,\n    requestBodyValues,\n    statusCode,\n    responseBody,\n    cause,\n    isRetryable = statusCode != null &&\n      (statusCode === 429 || statusCode >= 500),\n    data,\n  }: {\n    message: string;\n    url: string;\n    requestBodyValues: unknown;\n    statusCode?: number;\n    responseBody?: string;\n    cause?: unknown;\n    isRetryable?: boolean;\n    data?: unknown;\n  }) {\n    super(message);\n\n    this.name = \"ApiCallError\";\n\n    this.url = url;\n    this.requestBodyValues = requestBodyValues;\n    this.statusCode = statusCode;\n    this.responseBody = responseBody;\n    this.cause = cause;\n    this.isRetryable = isRetryable;\n    this.data = data;\n  }\n\n  toJSON() {\n    return {\n      name: this.name,\n      message: this.message,\n      url: this.url,\n      requestBodyValues: this.requestBodyValues,\n      statusCode: this.statusCode,\n      responseBody: this.responseBody,\n      cause: this.cause,\n      isRetryable: this.isRetryable,\n      data: this.data,\n    };\n  }\n}\n","export * from \"./retryNever\";\nexport * from \"./retryWithExponentialBackoff\";\nexport * from \"./throttleMaxConcurrency\";\nexport * from \"./throttleOff\";\n","/**\n * The `retryNever` strategy never retries a failed API call.\n */\nexport const retryNever =\n  () =>\n  async <OUTPUT>(f: () => PromiseLike<OUTPUT>) =>\n    f();\n","export async function delay(delayInMs: number): Promise<void> {\n  return new Promise((resolve) => setTimeout(resolve, delayInMs));\n}\n","export function getErrorMessage(error: unknown | undefined) {\n  if (error == null) {\n    return \"unknown error\";\n  }\n\n  if (typeof error === \"string\") {\n    return error;\n  }\n\n  if (error instanceof Error) {\n    return error.message;\n  }\n\n  return JSON.stringify(error);\n}\n","export type RetryErrorReason =\n  | \"maxTriesExceeded\"\n  | \"errorNotRetryable\"\n  | \"abort\";\n\nexport class RetryError extends Error {\n  // note: property order determines debugging output\n  readonly reason: RetryErrorReason;\n  readonly lastError: unknown;\n  readonly errors: Array<unknown>;\n\n  constructor({\n    message,\n    reason,\n    errors,\n  }: {\n    message: string;\n    reason: RetryErrorReason;\n    errors: Array<unknown>;\n  }) {\n    super(message);\n\n    this.name = \"RetryError\";\n    this.reason = reason;\n    this.errors = errors;\n\n    // separate our last error to make debugging via log easier:\n    this.lastError = errors[errors.length - 1];\n  }\n\n  toJSON() {\n    return {\n      name: this.name,\n      message: this.message,\n      reason: this.reason,\n      lastError: this.lastError,\n      errors: this.errors,\n    };\n  }\n}\n","import { delay } from \"../../util/delay\";\nimport { getErrorMessage } from \"../../util/getErrorMessage\";\nimport { ApiCallError } from \"./ApiCallError\";\nimport { RetryError } from \"./RetryError\";\nimport { RetryFunction } from \"./RetryFunction\";\n\n/**\n * The `retryWithExponentialBackoff` strategy retries a failed API call with an exponential backoff.\n * You can configure the maximum number of tries, the initial delay, and the backoff factor.\n */\nexport const retryWithExponentialBackoff =\n  ({\n    maxTries = 3,\n    initialDelayInMs = 2000,\n    backoffFactor = 2,\n  } = {}): RetryFunction =>\n  async <OUTPUT>(f: () => PromiseLike<OUTPUT>) =>\n    _retryWithExponentialBackoff(f, {\n      maxTries,\n      delayInMs: initialDelayInMs,\n      backoffFactor,\n    });\n\nasync function _retryWithExponentialBackoff<OUTPUT>(\n  f: () => PromiseLike<OUTPUT>,\n  {\n    maxTries,\n    delayInMs,\n    backoffFactor,\n  }: { maxTries: number; delayInMs: number; backoffFactor: number },\n  errors: unknown[] = []\n): Promise<OUTPUT> {\n  try {\n    return await f();\n  } catch (error) {\n    const errorMessage = getErrorMessage(error);\n    const newErrors = [...errors, error];\n    const tryNumber = newErrors.length;\n\n    if (tryNumber >= maxTries) {\n      throw new RetryError({\n        message: `Failed after ${tryNumber} tries. Last error: ${errorMessage}`,\n        reason: \"maxTriesExceeded\",\n        errors: newErrors,\n      });\n    }\n\n    if (error instanceof Error) {\n      if (error.name === \"AbortError\") {\n        throw error;\n      }\n\n      if (\n        error instanceof ApiCallError &&\n        error.isRetryable &&\n        tryNumber < maxTries\n      ) {\n        await delay(delayInMs);\n        return _retryWithExponentialBackoff(\n          f,\n          { maxTries, delayInMs: backoffFactor * delayInMs, backoffFactor },\n          newErrors\n        );\n      }\n    }\n\n    throw new RetryError({\n      message: `Failed after ${tryNumber} attempt(s) with non-retryable error: '${errorMessage}'`,\n      reason: \"errorNotRetryable\",\n      errors: newErrors,\n    });\n  }\n}\n","import { ThrottleFunction } from \"./ThrottleFunction\";\n\nclass MaxConcurrencyThrottler {\n  private maxConcurrentCalls: number;\n  private activeCallCount: number;\n  private callQueue: Array<() => Promise<unknown>>;\n\n  constructor({ maxConcurrentCalls }: { maxConcurrentCalls: number }) {\n    this.maxConcurrentCalls = maxConcurrentCalls;\n    this.activeCallCount = 0;\n    this.callQueue = [];\n  }\n\n  async run<T>(fn: () => PromiseLike<T>): Promise<T> {\n    return new Promise((resolve, reject) => {\n      const tryExecute = async () => {\n        if (this.activeCallCount >= this.maxConcurrentCalls) return;\n\n        // mark as active and remove from queue:\n        this.activeCallCount++;\n        const idx = this.callQueue.indexOf(tryExecute);\n        if (idx !== -1) this.callQueue.splice(idx, 1);\n\n        try {\n          resolve(await fn());\n        } catch (error) {\n          reject(error);\n        } finally {\n          this.activeCallCount--;\n          if (this.callQueue.length > 0) {\n            this.callQueue[0]();\n          }\n        }\n      };\n\n      this.callQueue.push(tryExecute);\n\n      if (this.activeCallCount < this.maxConcurrentCalls) {\n        tryExecute();\n      }\n    });\n  }\n}\n\n/**\n * The `throttleMaxConcurrency` strategy limits the number of parallel API calls.\n */\nexport function throttleMaxConcurrency({\n  maxConcurrentCalls,\n}: {\n  maxConcurrentCalls: number;\n}): ThrottleFunction {\n  const throttler = new MaxConcurrencyThrottler({ maxConcurrentCalls });\n  return <T>(fn: () => PromiseLike<T>) => throttler.run(fn);\n}\n","import { ThrottleFunction } from \"./ThrottleFunction\";\n\n/**\n * The `throttleOff` strategy does not limit parallel API calls.\n */\nexport const throttleOff = (): ThrottleFunction => (fn) => fn();\n","import { RetryFunction } from \"./RetryFunction\";\nimport { ThrottleFunction } from \"./ThrottleFunction\";\nimport { ApiConfiguration, HeaderParameters } from \"./ApiConfiguration\";\nimport { CustomHeaderProvider } from \"./CustomHeaderProvider\";\n\nexport abstract class AbstractApiConfiguration implements ApiConfiguration {\n  readonly retry?: RetryFunction;\n  readonly throttle?: ThrottleFunction;\n\n  protected readonly customCallHeaders: CustomHeaderProvider;\n\n  constructor({\n    retry,\n    throttle,\n    customCallHeaders = () => ({}),\n  }: {\n    retry?: RetryFunction;\n    throttle?: ThrottleFunction;\n    customCallHeaders?: CustomHeaderProvider;\n  }) {\n    this.retry = retry;\n    this.throttle = throttle;\n    this.customCallHeaders = customCallHeaders;\n  }\n\n  abstract assembleUrl(path: string): string;\n  protected abstract fixedHeaders(\n    params: HeaderParameters\n  ): Record<string, string>;\n\n  headers(params: HeaderParameters): Record<string, string> {\n    return Object.fromEntries(\n      [\n        ...Object.entries(this.fixedHeaders(params)),\n        ...Object.entries(this.customCallHeaders(params)),\n      ].filter(\n        // remove undefined values:\n        (entry): entry is [string, string] => typeof entry[1] === \"string\"\n      )\n    );\n  }\n}\n","import { AbstractApiConfiguration } from \"./AbstractApiConfiguration\";\nimport { CustomHeaderProvider } from \"./CustomHeaderProvider\";\nimport { RetryFunction } from \"./RetryFunction\";\nimport { ThrottleFunction } from \"./ThrottleFunction\";\n\nexport type UrlParts = {\n  protocol: string;\n  host: string;\n  port: string;\n  path: string;\n};\n\nexport type BaseUrlPartsApiConfigurationOptions = {\n  baseUrl: string | UrlParts;\n  headers?: Record<string, string>;\n  customCallHeaders?: CustomHeaderProvider;\n  retry?: RetryFunction;\n  throttle?: ThrottleFunction;\n};\n\n/**\n * An API configuration that uses different URL parts and a set of headers.\n *\n * You can use it to configure custom APIs for models, e.g. your own internal OpenAI proxy with custom headers.\n */\nexport class BaseUrlApiConfiguration extends AbstractApiConfiguration {\n  readonly baseUrl: UrlParts;\n\n  protected readonly fixedHeadersValue: Record<string, string>;\n\n  constructor({\n    baseUrl,\n    headers,\n    retry,\n    throttle,\n    customCallHeaders,\n  }: BaseUrlPartsApiConfigurationOptions) {\n    super({ retry, throttle, customCallHeaders });\n    this.baseUrl = typeof baseUrl == \"string\" ? parseBaseUrl(baseUrl) : baseUrl;\n    this.fixedHeadersValue = headers ?? {};\n  }\n\n  fixedHeaders() {\n    return this.fixedHeadersValue;\n  }\n\n  assembleUrl(path: string): string {\n    let basePath = this.baseUrl.path;\n\n    // ensure base path ends without a slash\n    if (basePath.endsWith(\"/\")) {\n      basePath = basePath.slice(0, -1);\n    }\n\n    // ensure path starts with a slash\n    if (!path.startsWith(\"/\")) {\n      path = `/${path}`;\n    }\n\n    return `${this.baseUrl.protocol}://${this.baseUrl.host}:${this.baseUrl.port}${basePath}${path}`;\n  }\n}\n\nexport type PartialBaseUrlPartsApiConfigurationOptions = Omit<\n  BaseUrlPartsApiConfigurationOptions,\n  \"baseUrl\"\n> & {\n  baseUrl?: string | Partial<UrlParts>;\n};\n\nexport class BaseUrlApiConfigurationWithDefaults extends BaseUrlApiConfiguration {\n  constructor({\n    baseUrlDefaults,\n    baseUrl,\n    headers,\n    retry,\n    throttle,\n    customCallHeaders,\n  }: {\n    baseUrlDefaults: UrlParts;\n  } & PartialBaseUrlPartsApiConfigurationOptions) {\n    super({\n      baseUrl: resolveBaseUrl(baseUrl, baseUrlDefaults),\n      headers,\n      retry,\n      throttle,\n      customCallHeaders,\n    });\n  }\n}\n\nfunction parseBaseUrl(baseUrl: string): UrlParts {\n  const url = new URL(baseUrl);\n\n  return {\n    protocol: url.protocol.slice(0, -1), // remove trailing colon\n    host: url.hostname,\n    port: url.port,\n    path: url.pathname,\n  };\n}\n\nfunction resolveBaseUrl(\n  baseUrl: string | Partial<UrlParts> | undefined = {},\n  baseUrlDefaults: UrlParts\n): string | UrlParts {\n  if (typeof baseUrl == \"string\") {\n    return baseUrl;\n  } else {\n    return {\n      protocol: baseUrl.protocol ?? baseUrlDefaults.protocol,\n      host: baseUrl.host ?? baseUrlDefaults.host,\n      port: baseUrl.port ?? baseUrlDefaults.port,\n      path: baseUrl.path ?? baseUrlDefaults.path,\n    };\n  }\n}\n","import { Cache } from \"./Cache\";\n\nexport class MemoryCache implements Cache {\n  private readonly cache = new Map<string, unknown>();\n\n  private hashKey(key: {\n    functionType: string;\n    functionId?: string | undefined;\n    input: unknown;\n  }) {\n    return JSON.stringify(key);\n  }\n\n  async lookupValue(key: {\n    functionType: string;\n    functionId?: string | undefined;\n    input: unknown;\n  }): Promise<object | null> {\n    return this.cache.get(this.hashKey(key)) ?? null;\n  }\n\n  async storeValue(\n    key: {\n      functionType: string;\n      functionId?: string | undefined;\n      input: unknown;\n    },\n    value: unknown\n  ): Promise<void> {\n    this.cache.set(this.hashKey(key), value);\n  }\n}\n","import { nanoid as createId } from \"nanoid\";\nimport { FunctionEventSource } from \"./FunctionEventSource\";\nimport { FunctionCallOptions, FunctionOptions } from \"./FunctionOptions\";\nimport { getLogFormat } from \"./ModelFusionConfiguration\";\nimport { getFunctionObservers } from \"./ModelFusionConfiguration\";\nimport { AbortError } from \"./api/AbortError\";\nimport { getFunctionCallLogger } from \"./getFunctionCallLogger\";\nimport { getRun } from \"./getRun\";\nimport { startDurationMeasurement } from \"../util/DurationMeasurement\";\nimport { runSafe } from \"../util/runSafe\";\nimport { FunctionEvent } from \"./FunctionEvent\";\n\nexport async function executeFunctionCall<VALUE>({\n  options,\n  input,\n  functionType,\n  execute,\n  inputPropertyName = \"input\",\n  outputPropertyName = \"value\",\n}: {\n  options?: FunctionOptions;\n  input: unknown;\n  functionType: FunctionEvent[\"functionType\"];\n  execute: (options: FunctionCallOptions) => PromiseLike<VALUE>;\n  inputPropertyName?: string;\n  outputPropertyName?: string;\n}): Promise<VALUE> {\n  const run = await getRun(options?.run);\n\n  const eventSource = new FunctionEventSource({\n    observers: [\n      ...getFunctionCallLogger(options?.logging ?? getLogFormat()),\n      ...getFunctionObservers(),\n      ...(run?.functionObserver != null ? [run.functionObserver] : []),\n      ...(options?.observers ?? []),\n    ],\n    errorHandler: run?.errorHandler,\n  });\n\n  const durationMeasurement = startDurationMeasurement();\n\n  const startMetadata = {\n    functionType,\n\n    callId: `call-${createId()}`,\n    parentCallId: options?.callId,\n    runId: run?.runId,\n    sessionId: run?.sessionId,\n    userId: run?.userId,\n    functionId: options?.functionId,\n\n    [inputPropertyName]: input,\n\n    timestamp: durationMeasurement.startDate,\n    startTimestamp: durationMeasurement.startDate,\n  };\n\n  eventSource.notify({\n    eventType: \"started\",\n    ...startMetadata,\n  } as FunctionEvent);\n\n  const result = await runSafe(() =>\n    execute({\n      functionType,\n      functionId: options?.functionId,\n      callId: startMetadata.callId,\n      logging: options?.logging,\n      observers: options?.observers,\n      run,\n    })\n  );\n\n  const finishMetadata = {\n    eventType: \"finished\" as const,\n    ...startMetadata,\n    finishTimestamp: new Date(),\n    durationInMs: durationMeasurement.durationInMs,\n  };\n\n  if (!result.ok) {\n    if (result.isAborted) {\n      eventSource.notify({\n        ...finishMetadata,\n        eventType: \"finished\",\n        result: {\n          status: \"abort\",\n        },\n      } as FunctionEvent);\n      throw new AbortError();\n    }\n\n    eventSource.notify({\n      ...finishMetadata,\n      eventType: \"finished\",\n      result: {\n        status: \"error\",\n        error: result.error,\n      },\n    } as FunctionEvent);\n\n    throw result.error;\n  }\n\n  eventSource.notify({\n    ...finishMetadata,\n    eventType: \"finished\",\n    result: {\n      status: \"success\",\n      [outputPropertyName]: result.value,\n    },\n  } as FunctionEvent);\n\n  return result.value;\n}\n","import { FunctionOptions } from \"./FunctionOptions\";\nimport { FunctionEvent } from \"./FunctionEvent\";\nimport { FunctionObserver } from \"./FunctionObserver\";\n\nexport function getFunctionCallLogger(\n  logging: FunctionOptions[\"logging\"]\n): Array<FunctionObserver> {\n  switch (logging) {\n    case \"basic-text\":\n      return [basicTextObserver];\n\n    case \"detailed-object\":\n      return [detailedObjectObserver];\n\n    case \"detailed-json\":\n      return [detailedJsonObserver];\n\n    case \"off\":\n    default:\n      return [];\n  }\n}\n\nconst basicTextObserver: FunctionObserver = {\n  onFunctionEvent(event: FunctionEvent) {\n    const text = `[${event.timestamp.toISOString()}] ${event.callId}${\n      event.functionId != null ? ` (${event.functionId})` : \"\"\n    } - ${event.functionType} ${event.eventType}`;\n\n    // log based on event type:\n    switch (event.eventType) {\n      case \"started\": {\n        console.log(text);\n        break;\n      }\n      case \"finished\": {\n        console.log(`${text} in ${event.durationInMs}ms`);\n        break;\n      }\n    }\n  },\n};\n\nconst detailedObjectObserver: FunctionObserver = {\n  onFunctionEvent(event: FunctionEvent) {\n    // Remove the \"response\" property from the result (if any):\n    if (\n      event.eventType === \"finished\" &&\n      event.result != null &&\n      \"rawResponse\" in event.result &&\n      event.result?.rawResponse != null\n    ) {\n      event = {\n        ...event,\n        result: Object.fromEntries(\n          Object.entries(event.result).filter(([k]) => k !== \"rawResponse\")\n          // eslint-disable-next-line @typescript-eslint/no-explicit-any\n        ) as any,\n      };\n    }\n\n    // filter all hard-to-read properties from event for cleaner console output:\n    // eslint-disable-next-line @typescript-eslint/no-explicit-any\n    function cleanObject(obj: any): any {\n      if (obj instanceof Date || typeof obj === \"string\") {\n        return obj;\n      }\n\n      if (Array.isArray(obj)) {\n        return obj.map((item) => cleanObject(item));\n      }\n\n      if (obj !== null && typeof obj === \"object\") {\n        return Object.fromEntries(\n          Object.entries(obj)\n            // eslint-disable-next-line @typescript-eslint/no-unused-vars\n            .map(([k, v]) => {\n              if (v === undefined) {\n                return [k, undefined];\n              } else if (v instanceof Uint8Array) {\n                return [k, \"omitted<Uint8Array>\"];\n              } else if (\n                Array.isArray(v) &&\n                v.length > 20 &&\n                v.every((v) => typeof v === \"number\")\n              ) {\n                return [k, \"omitted<Array<number>>\"];\n              } else {\n                return [k, cleanObject(v)];\n              }\n            })\n            // eslint-disable-next-line @typescript-eslint/no-unused-vars\n            .filter(([_, v]) => v !== undefined)\n        );\n      }\n      return obj;\n    }\n\n    // Clean the event object\n    const cleanedEvent = cleanObject(event);\n\n    console.log(cleanedEvent);\n  },\n};\n\nconst detailedJsonObserver: FunctionObserver = {\n  onFunctionEvent(event: FunctionEvent) {\n    // Remove the \"response\" property from the result (if any):\n    if (\n      event.eventType === \"finished\" &&\n      event.result != null &&\n      \"rawResponse\" in event.result &&\n      event.result?.rawResponse != null\n    ) {\n      event = {\n        ...event,\n        result: Object.fromEntries(\n          Object.entries(event.result).filter(([k]) => k !== \"rawResponse\")\n          // eslint-disable-next-line @typescript-eslint/no-explicit-any\n        ) as any,\n      };\n    }\n\n    // filter all undefined properties from event for cleaner console output:\n    event = Object.fromEntries(\n      // eslint-disable-next-line @typescript-eslint/no-unused-vars\n      Object.entries(event).filter(([_, v]) => v !== undefined)\n    ) as FunctionEvent;\n\n    console.log(JSON.stringify(event));\n  },\n};\n","export function detectRuntime() {\n  // eslint-disable-next-line @typescript-eslint/no-explicit-any\n  const globalThisAny = globalThis as any;\n\n  if (globalThisAny.EdgeRuntime) {\n    return \"vercel-edge\";\n  }\n\n  if (globalThis.navigator?.userAgent === \"Cloudflare-Workers\") {\n    return \"cloudflare-workers\";\n  }\n\n  if (globalThis.process?.release?.name === \"node\") {\n    return \"node\";\n  }\n\n  if (globalThis.window) {\n    return \"browser\";\n  }\n\n  return null;\n}\n","import { detectRuntime } from \"../util/detectRuntime\";\nimport { Run } from \"./Run\";\n\ninterface RunStorage {\n  getStore: () => Run | undefined;\n  run: (context: Run, callback: () => void) => void;\n}\n\nlet runStorage: RunStorage | undefined;\n\nasync function ensureLoaded() {\n  if (detectRuntime() === \"node\" && !runStorage) {\n    // Note: using \"async_hooks\" instead of \"node:async_hooks\" to avoid webpack fallback problems.\n    // Note: we try both import and require to support both ESM and CJS.\n\n    // eslint-disable-next-line @typescript-eslint/no-explicit-any\n    let AsyncLocalStorage: any;\n\n    try {\n      AsyncLocalStorage = (await import(\"async_hooks\")).AsyncLocalStorage;\n    } catch (error) {\n      try {\n        // eslint-disable-next-line @typescript-eslint/no-var-requires\n        AsyncLocalStorage = require(\"async_hooks\").AsyncLocalStorage;\n      } catch (error) {\n        throw new Error(`Failed to load 'async_hooks' module dynamically.`);\n      }\n    }\n\n    runStorage = new AsyncLocalStorage();\n  }\n\n  return Promise.resolve();\n}\n\n/**\n * Returns the run stored in an AsyncLocalStorage if running in Node.js. It can be set with `withRun()`.\n */\nexport async function getRun(run?: Run): Promise<Run | undefined> {\n  await ensureLoaded();\n  return run ?? runStorage?.getStore();\n}\n\n/**\n * Stores the run in an AsyncLocalStorage if running in Node.js. It can be retrieved with `getRun()`.\n */\nexport async function withRun(\n  run: Run,\n  callback: (run: Run) => PromiseLike<void>\n) {\n  await ensureLoaded();\n  if (runStorage != null) {\n    await runStorage.run(run, () => callback(run));\n  } else {\n    await callback(run);\n  }\n}\n","export function startDurationMeasurement(): DurationMeasurement {\n  // certain environments may not have the performance API:\n  return globalThis.performance != null\n    ? new PerformanceNowDurationMeasurement()\n    : new DateDurationMeasurement();\n}\n\nexport interface DurationMeasurement {\n  startEpochSeconds: number;\n  startDate: Date;\n  durationInMs: number;\n}\n\nclass PerformanceNowDurationMeasurement implements DurationMeasurement {\n  private readonly startTime = globalThis.performance.now();\n\n  get startEpochSeconds() {\n    return Math.floor(\n      (globalThis.performance.timeOrigin + this.startTime) / 1000\n    );\n  }\n\n  get startDate() {\n    return new Date(this.startEpochSeconds * 1000);\n  }\n\n  get durationInMs() {\n    return Math.ceil(globalThis.performance.now() - this.startTime);\n  }\n}\n\nclass DateDurationMeasurement implements DurationMeasurement {\n  private readonly startTime = Date.now();\n\n  get startEpochSeconds() {\n    return Math.floor(this.startTime / 1000);\n  }\n\n  get startDate() {\n    return new Date(this.startTime);\n  }\n\n  get durationInMs() {\n    return Date.now() - this.startTime;\n  }\n}\n","import { SafeResult } from \"./SafeResult\";\n\nexport const runSafe = async <OUTPUT>(\n  f: () => PromiseLike<OUTPUT>\n): Promise<SafeResult<OUTPUT>> => {\n  try {\n    return { ok: true, value: await f() };\n  } catch (error) {\n    if (error instanceof Error && error.name === \"AbortError\") {\n      return { ok: false, isAborted: true };\n    }\n\n    return { ok: false, error };\n  }\n};\n","import { FunctionCallOptions, FunctionOptions } from \"./FunctionOptions\";\nimport { executeFunctionCall } from \"./executeFunctionCall\";\n\nexport async function executeFunction<INPUT, OUTPUT>(\n  fn: (input: INPUT, options: FunctionCallOptions) => PromiseLike<OUTPUT>,\n  input: INPUT,\n  options?: FunctionOptions\n): Promise<OUTPUT> {\n  return executeFunctionCall({\n    options,\n    input,\n    functionType: \"execute-function\",\n    execute: async (options) => fn(input, options),\n  });\n}\n","import { getErrorMessage } from \"../../util/getErrorMessage\";\n\nexport class JSONParseError extends Error {\n  // note: property order determines debugging output\n  readonly text: string;\n  readonly cause: unknown;\n\n  constructor({ text, cause }: { text: string; cause: unknown }) {\n    super(\n      `JSON parsing failed: ` +\n        `Text: ${text}.\\n` +\n        `Error message: ${getErrorMessage(cause)}`\n    );\n\n    this.name = \"JSONParseError\";\n\n    this.cause = cause;\n    this.text = text;\n  }\n\n  toJSON() {\n    return {\n      name: this.name,\n      message: this.message,\n      cause: this.cause,\n      stack: this.stack,\n\n      valueText: this.text,\n    };\n  }\n}\n","import { getErrorMessage } from \"../../util/getErrorMessage\";\n\nexport class TypeValidationError extends Error {\n  readonly value: unknown;\n  readonly cause: unknown;\n\n  constructor({ value, cause }: { value: unknown; cause: unknown }) {\n    super(\n      `Type validation failed: ` +\n        `Value: ${JSON.stringify(value)}.\\n` +\n        `Error message: ${getErrorMessage(cause)}`\n    );\n\n    this.name = \"TypeValidationError\";\n\n    this.cause = cause;\n    this.value = value;\n  }\n\n  toJSON() {\n    return {\n      name: this.name,\n      message: this.message,\n      cause: this.cause,\n      stack: this.stack,\n\n      value: this.value,\n    };\n  }\n}\n","import { JsonSchemaProducer } from \"./JsonSchemaProducer\";\nimport { Schema } from \"./Schema\";\n\nexport function uncheckedSchema<OBJECT>(jsonSchema?: unknown) {\n  return new UncheckedSchema<OBJECT>(jsonSchema);\n}\n\nexport class UncheckedSchema<OBJECT>\n  implements Schema<OBJECT>, JsonSchemaProducer\n{\n  constructor(private readonly jsonSchema?: unknown) {}\n\n  validate(\n    value: unknown\n  ): { success: true; value: OBJECT } | { success: false; error: unknown } {\n    return { success: true, value: value as OBJECT };\n  }\n\n  getJsonSchema(): unknown {\n    return this.jsonSchema;\n  }\n\n  readonly _type: OBJECT;\n}\n","import { PartialDeep } from \"type-fest\";\nimport { z } from \"zod\";\nimport { zodToJsonSchema } from \"zod-to-json-schema\";\nimport { JsonSchemaProducer } from \"./JsonSchemaProducer\";\nimport { Schema } from \"./Schema\";\n\nexport function zodSchema<OBJECT>(zodSchema: z.Schema<OBJECT>) {\n  return new ZodSchema(zodSchema);\n}\n\nexport class ZodSchema<OBJECT> implements Schema<OBJECT>, JsonSchemaProducer {\n  readonly zodSchema: z.Schema<OBJECT>;\n\n  constructor(zodSchema: z.Schema<OBJECT>) {\n    this.zodSchema = zodSchema;\n  }\n\n  validate(\n    value: unknown\n  ): { success: true; value: OBJECT } | { success: false; error: unknown } {\n    const result = this.zodSchema.safeParse(value);\n\n    return result.success\n      ? { success: true, value: result.data }\n      : { success: false, error: result.error };\n  }\n\n  getJsonSchema(): unknown {\n    return zodToJsonSchema(this.zodSchema);\n  }\n\n  /**\n   * Use only for typing purposes. The value is always `undefined`.\n   */\n  readonly _type: OBJECT;\n\n  /**\n   * Use only for typing purposes. The value is always `undefined`.\n   */\n  readonly _partialType: PartialDeep<OBJECT, { recurseIntoArrays: true }>;\n}\n","import SecureJSON from \"secure-json-parse\";\nimport { JSONParseError } from \"./JSONParseError\";\nimport { Schema } from \"./Schema\";\nimport { safeValidateTypes, validateTypes } from \"./validateTypes\";\nimport { TypeValidationError } from \"./TypeValidationError\";\n\n/**\n * Parses a JSON string into an unknown object.\n *\n * @param text - The JSON string to parse.\n * @returns {unknown} - The parsed JSON object.\n */\nexport function parseJSON({ text }: { text: string }): unknown;\n/**\n * Parses a JSON string into a strongly-typed object using the provided schema.\n *\n * @template T - The type of the object to parse the JSON into.\n * @param {string} text - The JSON string to parse.\n * @param {Schema<T>} schema - The schema to use for parsing the JSON.\n * @returns {T} - The parsed object.\n */\nexport function parseJSON<T>({\n  text,\n  schema,\n}: {\n  text: string;\n  schema: Schema<T>;\n}): T;\nexport function parseJSON<T>({\n  text,\n  schema,\n}: {\n  text: string;\n  schema?: Schema<T>;\n}): T {\n  try {\n    const value = SecureJSON.parse(text);\n\n    if (schema == null) {\n      return value;\n    }\n\n    return validateTypes({ value, schema });\n  } catch (error) {\n    if (\n      error instanceof JSONParseError ||\n      error instanceof TypeValidationError\n    ) {\n      throw error;\n    }\n\n    throw new JSONParseError({ text, cause: error });\n  }\n}\n\n/**\n * Safely parses a JSON string and returns the result as an object of type `unknown`.\n *\n * @param text - The JSON string to parse.\n * @returns {object} Either an object with `success: true` and the parsed data, or an object with `success: false` and the error that occurred.\n */\nexport function safeParseJSON({\n  text,\n}: {\n  text: string;\n}):\n  | { success: true; value: unknown }\n  | { success: false; error: JSONParseError | TypeValidationError };\n/**\n * Safely parses a JSON string into a strongly-typed object, using a provided schema to validate the object.\n *\n * @template T - The type of the object to parse the JSON into.\n * @param {string} text - The JSON string to parse.\n * @param {Schema<T>} schema - The schema to use for parsing the JSON.\n * @returns An object with either a `success` flag and the parsed and typed data, or a `success` flag and an error object.\n */\nexport function safeParseJSON<T>({\n  text,\n  schema,\n}: {\n  text: string;\n  schema: Schema<T>;\n}):\n  | { success: true; value: T }\n  | { success: false; error: JSONParseError | TypeValidationError };\nexport function safeParseJSON<T>({\n  text,\n  schema,\n}: {\n  text: string;\n  schema?: Schema<T>;\n}):\n  | { success: true; value: T }\n  | { success: false; error: JSONParseError | TypeValidationError } {\n  try {\n    const value = SecureJSON.parse(text);\n\n    if (schema == null) {\n      return {\n        success: true,\n        value: value as T,\n      };\n    }\n\n    return safeValidateTypes({ value, schema });\n  } catch (error) {\n    return {\n      success: false,\n      error:\n        error instanceof JSONParseError\n          ? error\n          : new JSONParseError({ text, cause: error }),\n    };\n  }\n}\n","import { Schema } from \"./Schema\";\nimport { TypeValidationError } from \"./TypeValidationError\";\n\n/**\n * Validates the types of an unknown object using a schema and\n * return a strongly-typed object.\n *\n * @template T - The type of the object to validate.\n * @param {string} options.value - The object to validate.\n * @param {Schema<T>} options.schema - The schema to use for validating the JSON.\n * @returns {T} - The typed object.\n */\nexport function validateTypes<T>({\n  value,\n  schema,\n}: {\n  value: unknown;\n  schema: Schema<T>;\n}): T {\n  try {\n    const validationResult = schema.validate(value);\n\n    if (!validationResult.success) {\n      throw new TypeValidationError({\n        value,\n        cause: validationResult.error,\n      });\n    }\n\n    return validationResult.value;\n  } catch (error) {\n    if (error instanceof TypeValidationError) {\n      throw error;\n    }\n\n    throw new TypeValidationError({ value, cause: error });\n  }\n}\n\n/**\n * Safely validates the types of an unknown object using a schema and\n * return a strongly-typed object.\n *\n * @template T - The type of the object to validate.\n * @param {string} options.value - The JSON object to validate.\n * @param {Schema<T>} options.schema - The schema to use for validating the JSON.\n * @returns An object with either a `success` flag and the parsed and typed data, or a `success` flag and an error object.\n */\nexport function safeValidateTypes<T>({\n  value,\n  schema,\n}: {\n  value: unknown;\n  schema: Schema<T>;\n}):\n  | { success: true; value: T }\n  | { success: false; error: TypeValidationError } {\n  try {\n    const validationResult = schema.validate(value);\n\n    if (validationResult.success) {\n      return validationResult;\n    }\n\n    return {\n      success: false,\n      error: new TypeValidationError({\n        value,\n        cause: validationResult.error,\n      }),\n    };\n  } catch (error) {\n    return {\n      success: false,\n      error:\n        error instanceof TypeValidationError\n          ? error\n          : new TypeValidationError({ value, cause: error }),\n    };\n  }\n}\n","/**\n * Calculates the cosine similarity between two vectors. They must have the same length.\n *\n * @param a The first vector.\n * @param b The second vector.\n *\n * @returns The cosine similarity between the two vectors.\n *\n * @see https://en.wikipedia.org/wiki/Cosine_similarity\n */\nexport function cosineSimilarity(a: number[], b: number[]) {\n  if (a.length !== b.length) {\n    throw new Error(\n      `Vectors must have the same length (a: ${a.length}, b: ${b.length})`\n    );\n  }\n\n  return dotProduct(a, b) / (magnitude(a) * magnitude(b));\n}\n\nfunction dotProduct(a: number[], b: number[]) {\n  return a.reduce(\n    (acc: number, val: number, i: number) => acc + val * b[i]!,\n    0\n  );\n}\n\nfunction magnitude(a: number[]) {\n  return Math.sqrt(dotProduct(a, a));\n}\n","import { nanoid as createId } from \"nanoid\";\nimport { FunctionEventSource } from \"../core/FunctionEventSource\";\nimport { FunctionCallOptions, FunctionOptions } from \"../core/FunctionOptions\";\nimport { getLogFormat } from \"../core/ModelFusionConfiguration\";\nimport { getFunctionObservers } from \"../core/ModelFusionConfiguration\";\nimport { AbortError } from \"../core/api/AbortError\";\nimport { getFunctionCallLogger } from \"../core/getFunctionCallLogger\";\nimport { getRun } from \"../core/getRun\";\nimport { startDurationMeasurement } from \"../util/DurationMeasurement\";\nimport { runSafe } from \"../util/runSafe\";\nimport { Model, ModelSettings } from \"./Model\";\nimport {\n  ModelCallFinishedEvent,\n  ModelCallStartedEvent,\n} from \"./ModelCallEvent\";\nimport { ModelCallMetadata } from \"./ModelCallMetadata\";\n\nexport async function executeStandardCall<\n  VALUE,\n  MODEL extends Model<ModelSettings>,\n>({\n  model,\n  options,\n  input,\n  functionType,\n  generateResponse,\n}: {\n  model: MODEL;\n  options?: FunctionOptions;\n  input: unknown;\n  functionType: ModelCallStartedEvent[\"functionType\"];\n  generateResponse: (options: FunctionCallOptions) => PromiseLike<{\n    rawResponse: unknown;\n    extractedValue: VALUE;\n    usage?: unknown;\n  }>;\n}): Promise<{\n  value: VALUE;\n  rawResponse: unknown;\n  metadata: ModelCallMetadata;\n}> {\n  const run = await getRun(options?.run);\n  const settings = model.settings;\n\n  const eventSource = new FunctionEventSource({\n    observers: [\n      ...getFunctionCallLogger(options?.logging ?? getLogFormat()),\n      ...getFunctionObservers(),\n      ...(settings.observers ?? []),\n      ...(run?.functionObserver != null ? [run.functionObserver] : []),\n      ...(options?.observers ?? []),\n    ],\n    errorHandler: run?.errorHandler,\n  });\n\n  const durationMeasurement = startDurationMeasurement();\n\n  const startMetadata = {\n    functionType,\n\n    callId: `call-${createId()}`,\n    parentCallId: options?.callId,\n    runId: run?.runId,\n    sessionId: run?.sessionId,\n    userId: run?.userId,\n    functionId: options?.functionId,\n\n    model: model.modelInformation,\n    settings: model.settingsForEvent,\n    input,\n\n    timestamp: durationMeasurement.startDate,\n    startTimestamp: durationMeasurement.startDate,\n  };\n\n  eventSource.notify({\n    eventType: \"started\",\n    ...startMetadata,\n  } as ModelCallStartedEvent);\n\n  const result = await runSafe(() =>\n    generateResponse({\n      functionType,\n      functionId: options?.functionId,\n      callId: startMetadata.callId,\n      logging: options?.logging,\n      observers: options?.observers,\n      cache: options?.cache,\n      run,\n    })\n  );\n\n  const finishMetadata = {\n    eventType: \"finished\" as const,\n    ...startMetadata,\n    finishTimestamp: new Date(),\n    durationInMs: durationMeasurement.durationInMs,\n  };\n\n  if (!result.ok) {\n    if (result.isAborted) {\n      eventSource.notify({\n        ...finishMetadata,\n        eventType: \"finished\",\n        result: {\n          status: \"abort\",\n        },\n      } as ModelCallFinishedEvent);\n      throw new AbortError();\n    }\n\n    eventSource.notify({\n      ...finishMetadata,\n      eventType: \"finished\",\n      result: {\n        status: \"error\",\n        error: result.error,\n      },\n    } as ModelCallFinishedEvent);\n\n    throw result.error;\n  }\n\n  const rawResponse = result.value.rawResponse;\n  const value = result.value.extractedValue;\n  const usage = result.value.usage;\n\n  eventSource.notify({\n    ...finishMetadata,\n    eventType: \"finished\",\n    result: {\n      status: \"success\",\n      usage,\n      rawResponse,\n      value,\n    },\n  } as ModelCallFinishedEvent);\n\n  return {\n    value,\n    rawResponse,\n    metadata: {\n      model: model.modelInformation,\n\n      callId: finishMetadata.callId,\n      runId: finishMetadata.runId,\n      sessionId: finishMetadata.sessionId,\n      userId: finishMetadata.userId,\n      functionId: finishMetadata.functionId,\n\n      startTimestamp: startMetadata.startTimestamp,\n      finishTimestamp: finishMetadata.finishTimestamp,\n      durationInMs: finishMetadata.durationInMs,\n\n      usage,\n    },\n  };\n}\n","import { FunctionOptions } from \"../../core/FunctionOptions\";\nimport { Vector } from \"../../core/Vector\";\nimport { ModelCallMetadata } from \"../ModelCallMetadata\";\nimport { executeStandardCall } from \"../executeStandardCall\";\nimport { EmbeddingModel, EmbeddingModelSettings } from \"./EmbeddingModel\";\n\n/**\n * Generate embeddings for multiple values.\n *\n * @see https://modelfusion.dev/guide/function/embed\n *\n * @example\n * const embeddings = await embedMany({\n *   embedder: openai.TextEmbedder(...),\n *   values: [\n *     \"At first, Nox didn't know what to do with the pup.\",\n *     \"He keenly observed and absorbed everything around him, from the birds in the sky to the trees in the forest.\",\n *   ]\n * });\n *\n * @param {EmbeddingModel<VALUE, EmbeddingModelSettings>} model - The model to use for generating embeddings.\n * @param {VALUE[]} values - The values to generate embeddings for.\n *\n * @returns {Promise<Vector[]>} - A promise that resolves to an array of vectors representing the embeddings.\n */\nexport async function embedMany<VALUE>(\n  args: {\n    model: EmbeddingModel<VALUE, EmbeddingModelSettings>;\n    values: VALUE[];\n    fullResponse?: false;\n  } & FunctionOptions\n): Promise<Vector[]>;\nexport async function embedMany<VALUE>(\n  args: {\n    model: EmbeddingModel<VALUE, EmbeddingModelSettings>;\n    values: VALUE[];\n    fullResponse: true;\n  } & FunctionOptions\n): Promise<{\n  embeddings: Vector[];\n  rawResponse: unknown;\n  metadata: ModelCallMetadata;\n}>;\nexport async function embedMany<VALUE>({\n  model,\n  values,\n  fullResponse,\n  ...options\n}: {\n  model: EmbeddingModel<VALUE, EmbeddingModelSettings>;\n  values: VALUE[];\n  fullResponse?: boolean;\n} & FunctionOptions): Promise<\n  | Vector[]\n  | {\n      embeddings: Vector[];\n      rawResponse: unknown;\n      metadata: ModelCallMetadata;\n    }\n> {\n  const callResponse = await executeStandardCall({\n    functionType: \"embed\",\n    input: values,\n    model,\n    options,\n    generateResponse: async (options) => {\n      // split the values into groups that are small enough to be sent in one call:\n      const maxValuesPerCall = model.maxValuesPerCall;\n      const valueGroups: VALUE[][] = [];\n\n      if (maxValuesPerCall == null) {\n        valueGroups.push(values);\n      } else {\n        for (let i = 0; i < values.length; i += maxValuesPerCall) {\n          valueGroups.push(values.slice(i, i + maxValuesPerCall));\n        }\n      }\n\n      // call the model for each group:\n      let responses: Array<{ rawResponse: unknown; embeddings: Vector[] }>;\n      if (model.isParallelizable) {\n        responses = await Promise.all(\n          valueGroups.map((valueGroup) =>\n            model.doEmbedValues(valueGroup, options)\n          )\n        );\n      } else {\n        responses = [];\n        for (const valueGroup of valueGroups) {\n          const response = await model.doEmbedValues(valueGroup, options);\n          responses.push(response);\n        }\n      }\n\n      const rawResponses = responses.map((response) => response.rawResponse);\n      const embeddings: Array<Vector> = [];\n      for (const response of responses) {\n        embeddings.push(...response.embeddings);\n      }\n\n      return {\n        rawResponse: rawResponses,\n        extractedValue: embeddings,\n      };\n    },\n  });\n\n  return fullResponse\n    ? {\n        embeddings: callResponse.value,\n        rawResponse: callResponse.rawResponse,\n        metadata: callResponse.metadata,\n      }\n    : callResponse.value;\n}\n\n/**\n * Generate an embedding for a single value.\n *\n * @see https://modelfusion.dev/guide/function/embed\n *\n * @example\n * const embedding = await embed({\n *   model: openai.TextEmbedder(...),\n *   value: \"At first, Nox didn't know what to do with the pup.\"\n * });\n *\n * @param {EmbeddingModel<VALUE, EmbeddingModelSettings>} model - The model to use for generating the embedding.\n * @param {VALUE} value - The value to generate an embedding for.\n *\n * @returns {Promise<Vector>} - A promise that resolves to a vector representing the embedding.\n */\nexport async function embed<VALUE>(\n  args: {\n    model: EmbeddingModel<VALUE, EmbeddingModelSettings>;\n    value: VALUE;\n    fullResponse?: false;\n  } & FunctionOptions\n): Promise<Vector>;\nexport async function embed<VALUE>(\n  args: {\n    model: EmbeddingModel<VALUE, EmbeddingModelSettings>;\n    value: VALUE;\n    fullResponse: true;\n  } & FunctionOptions\n): Promise<{\n  embedding: Vector;\n  rawResponse: unknown;\n  metadata: ModelCallMetadata;\n}>;\nexport async function embed<VALUE>({\n  model,\n  value,\n  fullResponse,\n  ...options\n}: {\n  model: EmbeddingModel<VALUE, EmbeddingModelSettings>;\n  value: VALUE;\n  fullResponse?: boolean;\n} & FunctionOptions): Promise<\n  | Vector\n  | { embedding: Vector; rawResponse: unknown; metadata: ModelCallMetadata }\n> {\n  const callResponse = await executeStandardCall({\n    functionType: \"embed\",\n    input: value,\n    model,\n    options,\n    generateResponse: async (options) => {\n      const result = await model.doEmbedValues([value], options);\n      return {\n        rawResponse: result.rawResponse,\n        extractedValue: result.embeddings[0],\n      };\n    },\n  });\n\n  return fullResponse\n    ? {\n        embedding: callResponse.value,\n        rawResponse: callResponse.rawResponse,\n        metadata: callResponse.metadata,\n      }\n    : callResponse.value;\n}\n","import { FunctionCallOptions } from \"../../core/FunctionOptions\";\nimport { Vector } from \"../../core/Vector\";\nimport { cosineSimilarity } from \"../../util/cosineSimilarity\";\nimport { EmbeddingModel } from \"../embed/EmbeddingModel\";\nimport { embed, embedMany } from \"../embed/embed\";\nimport { Classifier, ClassifierSettings } from \"./Classifier\";\n\nexport interface ValueCluster<VALUE, NAME extends string> {\n  name: NAME;\n  values: VALUE[];\n}\n\nexport interface EmbeddingSimilarityClassifierSettings<\n  VALUE,\n  CLUSTERS extends Array<ValueCluster<VALUE, string>>,\n> extends ClassifierSettings {\n  clusters: CLUSTERS;\n  embeddingModel: EmbeddingModel<VALUE>;\n  similarityThreshold: number;\n}\n\n/**\n * Classifies values based on their distance to the values from a set of clusters.\n * When the distance is below a certain threshold, the value is classified as belonging to the cluster,\n * and the cluster name is returned. Otherwise, the value is classified as null.\n */\nexport class EmbeddingSimilarityClassifier<\n  VALUE,\n  CLUSTERS extends Array<ValueCluster<VALUE, string>>,\n> implements\n    Classifier<\n      VALUE,\n      ClusterNames<CLUSTERS> | null,\n      EmbeddingSimilarityClassifierSettings<VALUE, CLUSTERS>\n    >\n{\n  readonly settings: EmbeddingSimilarityClassifierSettings<VALUE, CLUSTERS>;\n\n  readonly modelInformation = {\n    provider: \"modelfusion\",\n    modelName: \"EmbeddingSimilarityClassifier\",\n  };\n\n  private embeddings:\n    | Array<{\n        embedding: Vector;\n        clusterValue: VALUE;\n        clusterName: string;\n      }>\n    | undefined;\n\n  constructor(\n    settings: EmbeddingSimilarityClassifierSettings<VALUE, CLUSTERS>\n  ) {\n    this.settings = settings;\n  }\n\n  async getEmbeddings(options: FunctionCallOptions) {\n    if (this.embeddings != null) {\n      return this.embeddings;\n    }\n\n    const embeddings: Array<{\n      embedding: Vector;\n      clusterValue: VALUE;\n      clusterName: string;\n    }> = [];\n\n    for (const cluster of this.settings.clusters) {\n      const clusterEmbeddings = await embedMany({\n        model: this.settings.embeddingModel,\n        values: cluster.values,\n        ...options,\n      });\n\n      for (let i = 0; i < clusterEmbeddings.length; i++) {\n        embeddings.push({\n          embedding: clusterEmbeddings[i],\n          clusterValue: cluster.values[i],\n          clusterName: cluster.name,\n        });\n      }\n    }\n\n    this.embeddings = embeddings; // lazy caching\n\n    return embeddings;\n  }\n\n  async doClassify(value: VALUE, options: FunctionCallOptions) {\n    const valueEmbedding = await embed({\n      model: this.settings.embeddingModel,\n      value,\n      ...options,\n    });\n\n    const clusterEmbeddings = await this.getEmbeddings(options);\n\n    const allMatches: Array<{\n      similarity: number;\n      clusterValue: VALUE;\n      clusterName: string;\n    }> = [];\n\n    for (const embedding of clusterEmbeddings) {\n      const similarity = cosineSimilarity(valueEmbedding, embedding.embedding);\n\n      if (similarity >= this.settings.similarityThreshold) {\n        allMatches.push({\n          similarity,\n          clusterValue: embedding.clusterValue,\n          clusterName: embedding.clusterName,\n        });\n      }\n    }\n\n    // sort (highest similarity first)\n    allMatches.sort((a, b) => b.similarity - a.similarity);\n\n    return {\n      class:\n        allMatches.length > 0\n          ? (allMatches[0].clusterName as unknown as ClusterNames<CLUSTERS>)\n          : null,\n      rawResponse: undefined,\n    };\n  }\n\n  get settingsForEvent(): Partial<\n    EmbeddingSimilarityClassifierSettings<VALUE, CLUSTERS>\n  > {\n    const eventSettingProperties: Array<string> = [\n      \"clusters\",\n      \"embeddingModel\",\n      \"similarityThreshold\",\n    ] satisfies (keyof EmbeddingSimilarityClassifierSettings<\n      VALUE,\n      CLUSTERS\n    >)[];\n\n    return Object.fromEntries(\n      Object.entries(this.settings).filter(([key]) =>\n        eventSettingProperties.includes(key)\n      )\n    );\n  }\n\n  withSettings(\n    additionalSettings: Partial<\n      EmbeddingSimilarityClassifierSettings<VALUE, CLUSTERS>\n    >\n  ) {\n    return new EmbeddingSimilarityClassifier(\n      Object.assign({}, this.settings, additionalSettings)\n    ) as this;\n  }\n}\n\ntype ClusterNames<CLUSTERS> =\n  CLUSTERS extends Array<ValueCluster<unknown, infer NAME>> ? NAME : never;\n","import { FunctionOptions } from \"../../core/FunctionOptions\";\nimport { ModelCallMetadata } from \"../ModelCallMetadata\";\nimport { executeStandardCall } from \"../executeStandardCall\";\nimport { Classifier, ClassifierSettings } from \"./Classifier\";\n\nexport async function classify<VALUE, CLASS extends string | null>(\n  args: {\n    model: Classifier<VALUE, CLASS, ClassifierSettings>;\n    value: VALUE;\n    fullResponse?: false;\n  } & FunctionOptions\n): Promise<CLASS>;\nexport async function classify<VALUE, CLASS extends string | null>(\n  args: {\n    model: Classifier<VALUE, CLASS, ClassifierSettings>;\n    value: VALUE;\n    fullResponse: true;\n  } & FunctionOptions\n): Promise<{\n  class: CLASS;\n  rawResponse: unknown;\n  metadata: ModelCallMetadata;\n}>;\nexport async function classify<VALUE, CLASS extends string | null>({\n  model,\n  value,\n  fullResponse,\n  ...options\n}: {\n  model: Classifier<VALUE, CLASS, ClassifierSettings>;\n  value: VALUE;\n  fullResponse?: boolean;\n} & FunctionOptions): Promise<\n  CLASS | { class: CLASS; rawResponse: unknown; metadata: ModelCallMetadata }\n> {\n  const callResponse = await executeStandardCall({\n    functionType: \"classify\",\n    input: value,\n    model,\n    options,\n    generateResponse: async (options) => {\n      const result = await model.doClassify(value, options);\n      return {\n        rawResponse: result.rawResponse,\n        extractedValue: result.class,\n      };\n    },\n  });\n\n  return fullResponse\n    ? {\n        class: callResponse.value,\n        rawResponse: callResponse.rawResponse,\n        metadata: callResponse.metadata,\n      }\n    : callResponse.value;\n}\n","import { FunctionCallOptions } from \"../../core/FunctionOptions\";\nimport { PromptTemplate } from \"../PromptTemplate\";\nimport {\n  ImageGenerationModel,\n  ImageGenerationModelSettings,\n} from \"./ImageGenerationModel\";\n\nexport class PromptTemplateImageGenerationModel<\n  PROMPT,\n  MODEL_PROMPT,\n  SETTINGS extends ImageGenerationModelSettings,\n  MODEL extends ImageGenerationModel<MODEL_PROMPT, SETTINGS>,\n> implements ImageGenerationModel<PROMPT, SETTINGS>\n{\n  readonly model: MODEL;\n  readonly promptTemplate: PromptTemplate<PROMPT, MODEL_PROMPT>;\n\n  constructor({\n    model,\n    promptTemplate,\n  }: {\n    model: MODEL;\n    promptTemplate: PromptTemplate<PROMPT, MODEL_PROMPT>;\n  }) {\n    this.model = model;\n    this.promptTemplate = promptTemplate;\n  }\n\n  get modelInformation() {\n    return this.model.modelInformation;\n  }\n\n  get settings() {\n    return this.model.settings;\n  }\n\n  doGenerateImages(prompt: PROMPT, options: FunctionCallOptions) {\n    const mappedPrompt = this.promptTemplate.format(prompt);\n    return this.model.doGenerateImages(mappedPrompt, options);\n  }\n\n  get settingsForEvent(): Partial<SETTINGS> {\n    return this.model.settingsForEvent;\n  }\n\n  withPromptTemplate<INPUT_PROMPT>(\n    promptTemplate: PromptTemplate<INPUT_PROMPT, PROMPT>\n  ): PromptTemplateImageGenerationModel<INPUT_PROMPT, PROMPT, SETTINGS, this> {\n    return new PromptTemplateImageGenerationModel<\n      INPUT_PROMPT,\n      PROMPT,\n      SETTINGS,\n      this\n    >({ model: this, promptTemplate });\n  }\n\n  withSettings(additionalSettings: Partial<SETTINGS>): this {\n    return new PromptTemplateImageGenerationModel({\n      model: this.model.withSettings(additionalSettings),\n      promptTemplate: this.promptTemplate,\n    }) as this;\n  }\n}\n","/*\n * MIT License\n *\n * Copyright (c) Sindre Sorhus <sindresorhus@gmail.com> (https://sindresorhus.com)\n *\n * Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n *\n * The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n */\n\n// copied from: https://github.com/sindresorhus/uint8array-extras/blob/main/index.js\n\nexport function base64ToUint8Array(base64String: string) {\n  return Uint8Array.from(\n    globalThis.atob(base64UrlToBase64(base64String)),\n    (x) => x.codePointAt(0)!\n  );\n}\n\n// Reference: https://phuoc.ng/collection/this-vs-that/concat-vs-push/\nconst MAX_BLOCK_SIZE = 65_535;\n\nexport function uint8ArrayToBase64(array: Uint8Array) {\n  let base64;\n\n  if (array.length < MAX_BLOCK_SIZE) {\n    // Required as `btoa` and `atob` don't properly support Unicode: https://developer.mozilla.org/en-US/docs/Glossary/Base64#the_unicode_problem\n    base64 = globalThis.btoa(String.fromCodePoint(...array));\n  } else {\n    base64 = \"\";\n    for (const value of array) {\n      base64 += String.fromCodePoint(value);\n    }\n\n    base64 = globalThis.btoa(base64);\n  }\n\n  return base64;\n}\n\nfunction base64UrlToBase64(base64url: string) {\n  return base64url.replaceAll(\"-\", \"+\").replaceAll(\"_\", \"/\");\n}\n","import { FunctionOptions } from \"../../core/FunctionOptions\";\nimport { base64ToUint8Array } from \"../../util/format/UInt8Utils\";\nimport { ModelCallMetadata } from \"../ModelCallMetadata\";\nimport { executeStandardCall } from \"../executeStandardCall\";\nimport {\n  ImageGenerationModel,\n  ImageGenerationModelSettings,\n} from \"./ImageGenerationModel\";\n\n/**\n * Generates an image using a prompt.\n *\n * The prompt depends on the model. For example, OpenAI image models expect a string prompt,\n * and Stability AI models expect an array of text prompts with optional weights.\n *\n * @see https://modelfusion.dev/guide/function/generate-image\n *\n * @example\n * const image = await generateImage({\n *   imageGenerator: stability.ImageGenerator(...),\n *   prompt: [\n *     { text: \"the wicked witch of the west\" },\n *     { text: \"style of early 19th century painting\", weight: 0.5 },\n *   ]\n * });\n *\n * @param {ImageGenerationModel<PROMPT, ImageGenerationModelSettings>} model - The image generation model to be used.\n * @param {PROMPT} prompt - The prompt to be used for image generation.\n *\n * @returns {Promise} - Returns a promise that resolves to the generated image.\n * The image is a Uint8Array containing the image data in PNG format.\n */\nexport async function generateImage<PROMPT>(\n  args: {\n    model: ImageGenerationModel<PROMPT, ImageGenerationModelSettings>;\n    prompt: PROMPT;\n    fullResponse?: false;\n  } & FunctionOptions\n): Promise<Uint8Array>;\nexport async function generateImage<PROMPT>(\n  args: {\n    model: ImageGenerationModel<PROMPT, ImageGenerationModelSettings>;\n    prompt: PROMPT;\n    fullResponse: true;\n  } & FunctionOptions\n): Promise<{\n  image: Uint8Array;\n  imageBase64: string;\n  images: Uint8Array[];\n  imagesBase64: string[];\n  rawResponse: unknown;\n  metadata: ModelCallMetadata;\n}>;\nexport async function generateImage<PROMPT>({\n  model,\n  prompt,\n  fullResponse,\n  ...options\n}: {\n  model: ImageGenerationModel<PROMPT, ImageGenerationModelSettings>;\n  prompt: PROMPT;\n  fullResponse?: boolean;\n} & FunctionOptions): Promise<\n  | Uint8Array\n  | string\n  | {\n      image: Uint8Array;\n      imageBase64: string;\n      images: Uint8Array[];\n      imagesBase64: string[];\n      rawResponse: unknown;\n      metadata: ModelCallMetadata;\n    }\n> {\n  const callResponse = await executeStandardCall({\n    functionType: \"generate-image\",\n    input: prompt,\n    model,\n    options,\n    generateResponse: async (options) => {\n      const result = await model.doGenerateImages(prompt, options);\n\n      return {\n        rawResponse: result.rawResponse,\n        extractedValue: result.base64Images,\n      };\n    },\n  });\n\n  const imagesBase64 = callResponse.value;\n  const images = imagesBase64.map(base64ToUint8Array);\n\n  return fullResponse\n    ? {\n        image: images[0],\n        imageBase64: imagesBase64[0],\n        images,\n        imagesBase64,\n        rawResponse: callResponse.rawResponse,\n        metadata: callResponse.metadata,\n      }\n    : images[0];\n}\n","import { FunctionOptions } from \"../../core/FunctionOptions\";\nimport { ModelCallMetadata } from \"../ModelCallMetadata\";\nimport { executeStandardCall } from \"../executeStandardCall\";\nimport {\n  SpeechGenerationModel,\n  SpeechGenerationModelSettings,\n} from \"./SpeechGenerationModel\";\n\n/**\n * Synthesizes speech from text. Also called text-to-speech (TTS).\n *\n * @see https://modelfusion.dev/guide/function/generate-speech\n *\n * @example\n * const speech = await generateSpeech({\n *   model: lmnt.SpeechGenerator(...),\n *   text: \"Good evening, ladies and gentlemen! Exciting news on the airwaves tonight \" +\n *    \"as The Rolling Stones unveil 'Hackney Diamonds.'\n * });\n *\n * @param {SpeechGenerationModel<SpeechGenerationModelSettings>} model - The speech generation model.\n * @param {string} text - The text to be converted to speech.\n *\n * @returns {Promise<Uint8Array>} - A promise that resolves to a Uint8Array containing the synthesized speech.\n */\nexport async function generateSpeech(\n  args: {\n    model: SpeechGenerationModel<SpeechGenerationModelSettings>;\n    text: string;\n    fullResponse?: false;\n  } & FunctionOptions\n): Promise<Uint8Array>;\nexport async function generateSpeech(\n  args: {\n    model: SpeechGenerationModel<SpeechGenerationModelSettings>;\n    text: string;\n    fullResponse: true;\n  } & FunctionOptions\n): Promise<{\n  speech: Uint8Array;\n  rawResponse: unknown;\n  metadata: ModelCallMetadata;\n}>;\nexport async function generateSpeech({\n  model,\n  text,\n  fullResponse,\n  ...options\n}: {\n  model: SpeechGenerationModel<SpeechGenerationModelSettings>;\n  text: string;\n  fullResponse?: boolean;\n} & FunctionOptions): Promise<\n  | Uint8Array\n  | { speech: Uint8Array; rawResponse: unknown; metadata: ModelCallMetadata }\n> {\n  const callResponse = await executeStandardCall({\n    functionType: \"generate-speech\",\n    input: text,\n    model,\n    options,\n    generateResponse: async (options) => {\n      const response = await model.doGenerateSpeechStandard(text, options);\n\n      return {\n        rawResponse: response,\n        extractedValue: response,\n      };\n    },\n  });\n\n  return fullResponse\n    ? {\n        speech: callResponse.value,\n        rawResponse: callResponse.rawResponse,\n        metadata: callResponse.metadata,\n      }\n    : callResponse.value;\n}\n","/**\n * `AsyncQueue` is a class that represents an asynchronous queue.\n * It allows values to be pushed onto it and consumed (pulled) by an iterator.\n * The queue is async-iterable, making it compatible with async/await syntax.\n *\n * @template T The type of elements contained in the queue.\n * @example\n * const queue = new AsyncQueue<number>();\n * queue.push(1);\n * (async () => {\n *   for await (const value of queue) {\n *     console.log(value);\n *   }\n * })();\n */\nexport class AsyncQueue<T> implements AsyncIterable<T> {\n  private values = Array<\n    { type: \"value\"; value: T } | { type: \"error\"; error: unknown }\n  >();\n  private pendingResolvers: Array<() => void> = [];\n  private closed: boolean = false;\n\n  private processPendingResolvers(): void {\n    while (this.pendingResolvers.length > 0) {\n      this.pendingResolvers.shift()?.();\n    }\n  }\n\n  /**\n   * Pushes an element onto the queue. If the queue is closed, an error is thrown.\n   *\n   * @param {T} value - The element to add to the queue.\n   * @throws {Error} Throws an error if the queue is closed.\n   * @example\n   * queue.push(2);\n   */\n  push(value: T): void {\n    if (this.closed) {\n      throw new Error(\"Cannot push value to closed queue.\");\n    }\n\n    this.values.push({ type: \"value\", value });\n    this.processPendingResolvers();\n  }\n\n  error(error: unknown): void {\n    if (this.closed) {\n      throw new Error(\"Cannot push error to closed queue.\");\n    }\n\n    this.values.push({ type: \"error\", error });\n    this.processPendingResolvers();\n  }\n\n  /**\n   * Closes the queue, preventing more elements from being pushed onto it.\n   *\n   * @example\n   * queue.close();\n   */\n  close(): void {\n    this.closed = true;\n    this.processPendingResolvers();\n  }\n\n  /**\n   * Creates and returns an async iterator that allows the queue to be consumed.\n   * You can create multiple iterators for the same queue.\n   *\n   * @returns {AsyncIterator<T>} An async iterator for the queue.\n   * @example\n   * (async () => {\n   *   for await (const value of queue) {\n   *     console.log(value);\n   *   }\n   * })();\n   */\n  [Symbol.asyncIterator](): AsyncIterator<T> {\n    let position = 0;\n\n    return {\n      next: (): Promise<IteratorResult<T>> =>\n        new Promise((resolve, reject) => {\n          const attemptResolve = () => {\n            if (position < this.values.length) {\n              const entry = this.values[position++];\n              switch (entry.type) {\n                case \"value\":\n                  resolve({ value: entry.value, done: false });\n                  break;\n                case \"error\":\n                  reject(entry.error);\n                  break;\n              }\n            } else if (this.closed) {\n              // The queue is closed, and there are no more values. Complete the iteration.\n              resolve({ value: undefined as any, done: true }); // eslint-disable-line @typescript-eslint/no-explicit-any\n            } else {\n              // No values are currently available, and the queue is not closed.\n              // The consumer is now pending and will be resolved when a new value is pushed\n              // or when the queue is closed.\n              this.pendingResolvers.push(attemptResolve);\n            }\n          };\n\n          attemptResolve();\n        }),\n    };\n  }\n}\n","import { nanoid as createId } from \"nanoid\";\nimport { FunctionEventSource } from \"../core/FunctionEventSource\";\nimport { FunctionCallOptions, FunctionOptions } from \"../core/FunctionOptions\";\nimport { getLogFormat } from \"../core/ModelFusionConfiguration\";\nimport { getFunctionObservers } from \"../core/ModelFusionConfiguration\";\nimport { AbortError } from \"../core/api/AbortError\";\nimport { getFunctionCallLogger } from \"../core/getFunctionCallLogger\";\nimport { getRun } from \"../core/getRun\";\nimport { AsyncQueue } from \"../util/AsyncQueue\";\nimport { startDurationMeasurement } from \"../util/DurationMeasurement\";\nimport { runSafe } from \"../util/runSafe\";\nimport { Delta } from \"./Delta\";\nimport { Model, ModelSettings } from \"./Model\";\nimport {\n  ModelCallFinishedEvent,\n  ModelCallStartedEvent,\n} from \"./ModelCallEvent\";\nimport { ModelCallMetadata } from \"./ModelCallMetadata\";\n\nexport async function executeStreamCall<\n  DELTA_VALUE,\n  VALUE,\n  MODEL extends Model<ModelSettings>,\n>({\n  model,\n  options,\n  input,\n  functionType,\n  startStream,\n  processDelta,\n  processFinished,\n  onDone,\n}: {\n  model: MODEL;\n  options?: FunctionOptions;\n  input: unknown;\n  functionType: ModelCallStartedEvent[\"functionType\"];\n  startStream: (\n    options: FunctionCallOptions\n  ) => PromiseLike<AsyncIterable<Delta<DELTA_VALUE>>>;\n  processDelta: (\n    delta: Delta<DELTA_VALUE> & { type: \"delta\" }\n  ) => VALUE | undefined;\n  processFinished?: () => VALUE | undefined;\n  onDone?: () => void;\n}): Promise<{\n  value: AsyncIterable<VALUE>;\n  metadata: Omit<ModelCallMetadata, \"durationInMs\" | \"finishTimestamp\">;\n}> {\n  const run = await getRun(options?.run);\n  const settings = model.settings;\n\n  const eventSource = new FunctionEventSource({\n    observers: [\n      ...getFunctionCallLogger(options?.logging ?? getLogFormat()),\n      ...getFunctionObservers(),\n      ...(settings.observers ?? []),\n      ...(run?.functionObserver != null ? [run.functionObserver] : []),\n      ...(options?.observers ?? []),\n    ],\n    errorHandler: run?.errorHandler,\n  });\n\n  const durationMeasurement = startDurationMeasurement();\n\n  const startMetadata = {\n    functionType,\n\n    callId: `call-${createId()}`,\n    parentCallId: options?.callId,\n    runId: run?.runId,\n    sessionId: run?.sessionId,\n    userId: run?.userId,\n    functionId: options?.functionId,\n\n    model: model.modelInformation,\n    settings: model.settingsForEvent,\n    input,\n\n    timestamp: durationMeasurement.startDate,\n    startTimestamp: durationMeasurement.startDate,\n  };\n\n  eventSource.notify({\n    eventType: \"started\",\n    ...startMetadata,\n  } as ModelCallStartedEvent);\n\n  const result = await runSafe(async () => {\n    const deltaIterable = await startStream({\n      functionType,\n      functionId: options?.functionId,\n      callId: startMetadata.callId,\n      logging: options?.logging,\n      observers: options?.observers,\n      run,\n    });\n\n    // Return a queue that can be iterated over several times:\n    const responseQueue = new AsyncQueue<VALUE>();\n\n    // run async:\n    (async function () {\n      try {\n        const loopResult = await runSafe(async () => {\n          for await (const event of deltaIterable) {\n            if (event?.type === \"error\") {\n              const error = event.error;\n\n              const finishMetadata = {\n                eventType: \"finished\" as const,\n                ...startMetadata,\n                finishTimestamp: new Date(),\n                durationInMs: durationMeasurement.durationInMs,\n              };\n\n              eventSource.notify(\n                error instanceof AbortError\n                  ? ({\n                      ...finishMetadata,\n                      result: { status: \"abort\" },\n                    } as ModelCallFinishedEvent)\n                  : ({\n                      ...finishMetadata,\n                      result: { status: \"error\", error },\n                    } as ModelCallFinishedEvent)\n              );\n\n              throw error;\n            }\n\n            if (event?.type === \"delta\") {\n              const value = processDelta(event);\n              if (value !== undefined) {\n                responseQueue.push(value);\n              }\n            }\n          }\n\n          if (processFinished != null) {\n            const value = processFinished();\n\n            if (value !== undefined) {\n              responseQueue.push(value);\n            }\n          }\n        });\n\n        // deal with abort or errors that happened during streaming:\n        if (!loopResult.ok) {\n          const finishMetadata = {\n            eventType: \"finished\" as const,\n            ...startMetadata,\n            finishTimestamp: new Date(),\n            durationInMs: durationMeasurement.durationInMs,\n          };\n\n          if (loopResult.isAborted) {\n            eventSource.notify({\n              ...finishMetadata,\n              eventType: \"finished\",\n              result: {\n                status: \"abort\",\n              },\n            } as ModelCallFinishedEvent);\n\n            responseQueue.error(new AbortError());\n\n            return; // error is handled through queue\n          }\n\n          eventSource.notify({\n            ...finishMetadata,\n            eventType: \"finished\",\n            result: {\n              status: \"error\",\n              error: loopResult.error,\n            },\n          } as ModelCallFinishedEvent);\n\n          responseQueue.error(loopResult.error);\n\n          return; // error is handled through queue\n        }\n\n        onDone?.();\n\n        const finishMetadata = {\n          eventType: \"finished\" as const,\n          ...startMetadata,\n          finishTimestamp: new Date(),\n          durationInMs: durationMeasurement.durationInMs,\n        };\n\n        eventSource.notify({\n          ...finishMetadata,\n          result: {\n            status: \"success\",\n          },\n        } as ModelCallFinishedEvent);\n      } finally {\n        // always close the queue when done, no matter where a potential error happened:\n        responseQueue.close();\n      }\n    })();\n\n    return {\n      stream: responseQueue,\n    };\n  });\n\n  if (!result.ok) {\n    const finishMetadata = {\n      eventType: \"finished\" as const,\n      ...startMetadata,\n      finishTimestamp: new Date(),\n      durationInMs: durationMeasurement.durationInMs,\n    };\n\n    if (result.isAborted) {\n      eventSource.notify({\n        ...finishMetadata,\n        eventType: \"finished\",\n        result: {\n          status: \"abort\",\n        },\n      } as ModelCallFinishedEvent);\n\n      throw new AbortError();\n    }\n\n    eventSource.notify({\n      ...finishMetadata,\n      eventType: \"finished\",\n      result: {\n        status: \"error\",\n        error: result.error,\n      },\n    } as ModelCallFinishedEvent);\n\n    throw result.error;\n  }\n\n  return {\n    value: result.value.stream,\n    metadata: startMetadata,\n  };\n}\n","import { FunctionOptions } from \"../../core/FunctionOptions\";\nimport { AsyncQueue } from \"../../util/AsyncQueue\";\nimport { ModelCallMetadata } from \"../ModelCallMetadata\";\nimport { executeStreamCall } from \"../executeStreamCall\";\nimport {\n  SpeechGenerationModelSettings,\n  StreamingSpeechGenerationModel,\n} from \"./SpeechGenerationModel\";\n\n/**\n * Stream synthesized speech from text. Also called text-to-speech (TTS).\n * Duplex streaming where both the input and output are streamed is supported.\n *\n * @see https://modelfusion.dev/guide/function/generate-speech\n *\n * @example\n * const textStream = await streamText(...);\n *\n * const speechStream = await streamSpeech({\n *   model: elevenlabs.SpeechGenerator(...),\n *   text: textStream\n * });\n *\n * for await (const speechPart of speechStream) {\n *   // ...\n * }\n *\n * @param {StreamingSpeechGenerationModel<SpeechGenerationModelSettings>} model - The speech generation model.\n * @param {AsyncIterable<string> | string} text - The text to be converted to speech. Can be a string or an async iterable of strings.\n * @param {FunctionOptions} [options] - Optional function options.\n *\n * @returns {AsyncIterableResultPromise<Uint8Array>} An async iterable promise that contains the synthesized speech chunks.\n */\nexport async function streamSpeech(\n  args: {\n    model: StreamingSpeechGenerationModel<SpeechGenerationModelSettings>;\n    text: AsyncIterable<string> | string;\n    fullResponse?: false;\n  } & FunctionOptions\n): Promise<AsyncIterable<Uint8Array>>;\nexport async function streamSpeech(\n  args: {\n    model: StreamingSpeechGenerationModel<SpeechGenerationModelSettings>;\n    text: AsyncIterable<string> | string;\n    fullResponse: true;\n  } & FunctionOptions\n): Promise<{\n  speechStream: AsyncIterable<Uint8Array>;\n  metadata: Omit<ModelCallMetadata, \"durationInMs\" | \"finishTimestamp\">;\n}>;\nexport async function streamSpeech({\n  model,\n  text,\n  fullResponse,\n  ...options\n}: {\n  model: StreamingSpeechGenerationModel<SpeechGenerationModelSettings>;\n  text: AsyncIterable<string> | string;\n  fullResponse?: boolean;\n} & FunctionOptions): Promise<\n  | AsyncIterable<Uint8Array>\n  | {\n      speechStream: AsyncIterable<Uint8Array>;\n      metadata: Omit<ModelCallMetadata, \"durationInMs\" | \"finishTimestamp\">;\n    }\n> {\n  let textStream: AsyncIterable<string>;\n\n  // simulate a stream with a single value for a string input:\n  if (typeof text === \"string\") {\n    const queue = new AsyncQueue<string>();\n    queue.push(text);\n    queue.close();\n    textStream = queue;\n  } else {\n    textStream = text;\n  }\n\n  const callResponse = await executeStreamCall({\n    functionType: \"stream-speech\",\n    input: text,\n    model,\n    options,\n    startStream: async (options) =>\n      model.doGenerateSpeechStreamDuplex(textStream, options),\n    processDelta: (delta) => delta.deltaValue,\n  });\n\n  return fullResponse\n    ? {\n        speechStream: callResponse.value,\n        metadata: callResponse.metadata,\n      }\n    : callResponse.value;\n}\n","import { FunctionOptions } from \"../../core/FunctionOptions\";\nimport { PromptFunction, expandPrompt } from \"../../core/PromptFunction\";\nimport { executeStandardCall } from \"../executeStandardCall\";\nimport { ModelCallMetadata } from \"../ModelCallMetadata\";\nimport {\n  TextGenerationModel,\n  TextGenerationModelSettings,\n} from \"./TextGenerationModel\";\nimport {\n  TextGenerationFinishReason,\n  TextGenerationResult,\n} from \"./TextGenerationResult\";\n\n/**\n * Generate text for a prompt and return it as a string.\n *\n * The prompt depends on the model used.\n * For instance, OpenAI completion models expect a string prompt,\n * whereas OpenAI chat models expect an array of chat messages.\n *\n * @see https://modelfusion.dev/guide/function/generate-text\n *\n * @example\n * const text = await generateText({\n *   model: openai.CompletionTextGenerator(...),\n *   prompt: \"Write a short story about a robot learning to love:\\n\\n\"\n * });\n *\n * @param {TextGenerationModel<PROMPT, TextGenerationModelSettings>} model - The text generation model to use.\n * @param {PROMPT} prompt - The prompt to use for text generation.\n *\n * @returns {Promise<string>} - A promise that resolves to the generated text.\n */\nexport async function generateText<PROMPT>(\n  args: {\n    model: TextGenerationModel<PROMPT, TextGenerationModelSettings>;\n    prompt: PROMPT | PromptFunction<unknown, PROMPT>;\n    fullResponse?: false;\n  } & FunctionOptions\n): Promise<string>;\nexport async function generateText<PROMPT>(\n  args: {\n    model: TextGenerationModel<PROMPT, TextGenerationModelSettings>;\n    prompt: PROMPT | PromptFunction<unknown, PROMPT>;\n    fullResponse: true;\n  } & FunctionOptions\n): Promise<{\n  text: string;\n  finishReason: TextGenerationFinishReason;\n  texts: string[];\n  textGenerationResults: TextGenerationResult[];\n  rawResponse: unknown;\n  metadata: ModelCallMetadata;\n}>;\nexport async function generateText<PROMPT>({\n  model,\n  prompt,\n  fullResponse,\n  ...options\n}: {\n  model: TextGenerationModel<PROMPT, TextGenerationModelSettings>;\n  prompt: PROMPT | PromptFunction<unknown, PROMPT>;\n  fullResponse?: boolean;\n} & FunctionOptions): Promise<\n  | string\n  | {\n      text: string;\n      finishReason: TextGenerationFinishReason;\n      texts: string[];\n      textGenerationResults: TextGenerationResult[];\n      rawResponse: unknown;\n      metadata: ModelCallMetadata;\n    }\n> {\n  const expandedPrompt = await expandPrompt(prompt);\n\n  const callResponse = await executeStandardCall({\n    functionType: \"generate-text\",\n    input: expandedPrompt,\n    model,\n    options,\n    generateResponse: async (options) => {\n      async function getGeneratedTexts() {\n        if (options?.cache == null) {\n          return {\n            ...(await model.doGenerateTexts(expandedPrompt.prompt, options)),\n            cache: undefined,\n          };\n        }\n\n        let cacheErrors: unknown[] | undefined = undefined;\n\n        const cacheKey = {\n          functionType: \"generate-text\",\n          functionId: options?.functionId,\n          input: {\n            model,\n            settings: model.settingsForEvent, // TODO should include full model information\n            prompt: expandedPrompt.prompt,\n          },\n        };\n\n        try {\n          const cachedRawResponse = await options.cache.lookupValue(cacheKey);\n\n          if (cachedRawResponse != null) {\n            return {\n              ...model.restoreGeneratedTexts(cachedRawResponse),\n              cache: { status: \"hit\" },\n            };\n          }\n        } catch (err) {\n          cacheErrors = [err];\n        }\n\n        const result = await model.doGenerateTexts(\n          expandedPrompt.prompt,\n          options\n        );\n\n        try {\n          await options.cache.storeValue(cacheKey, result.rawResponse);\n        } catch (err) {\n          cacheErrors = [...(cacheErrors ?? []), err];\n        }\n\n        return {\n          ...result,\n          cache: { status: \"miss\", errors: cacheErrors },\n        };\n      }\n\n      const result = await getGeneratedTexts();\n\n      const shouldTrimWhitespace = model.settings.trimWhitespace ?? true;\n\n      const textGenerationResults = shouldTrimWhitespace\n        ? result.textGenerationResults.map((textGeneration) => ({\n            text: textGeneration.text.trim(),\n            finishReason: textGeneration.finishReason,\n          }))\n        : result.textGenerationResults;\n\n      // TODO add cache information\n      return {\n        rawResponse: result.rawResponse,\n        extractedValue: textGenerationResults,\n        usage: result.usage,\n      };\n    },\n  });\n\n  const textGenerationResults = callResponse.value;\n  const firstResult = textGenerationResults[0];\n\n  return fullResponse\n    ? {\n        text: firstResult.text,\n        finishReason: firstResult.finishReason,\n        texts: textGenerationResults.map(\n          (textGeneration) => textGeneration.text\n        ),\n        textGenerationResults,\n        rawResponse: callResponse.rawResponse,\n        metadata: callResponse.metadata,\n      }\n    : firstResult.text;\n}\n","import { getErrorMessage } from \"../../util/getErrorMessage\";\n\nexport class ObjectParseError extends Error {\n  readonly cause: unknown;\n  readonly valueText: string;\n\n  constructor({ valueText, cause }: { valueText: string; cause: unknown }) {\n    super(\n      `Object parsing failed. ` +\n        `Value: ${valueText}.\\n` +\n        `Error message: ${getErrorMessage(cause)}`\n    );\n\n    this.name = \"ObjectParseError\";\n\n    this.cause = cause;\n    this.valueText = valueText;\n  }\n\n  toJSON() {\n    return {\n      name: this.name,\n      cause: this.cause,\n      message: this.message,\n      stack: this.stack,\n\n      valueText: this.valueText,\n    };\n  }\n}\n","import { FunctionOptions } from \"../../core/FunctionOptions\";\nimport { JsonSchemaProducer } from \"../../core/schema/JsonSchemaProducer\";\nimport { Schema } from \"../../core/schema/Schema\";\nimport {\n  TextGenerationModel,\n  TextGenerationModelSettings,\n} from \"../generate-text/TextGenerationModel\";\nimport { generateText } from \"../generate-text/generateText\";\nimport { ObjectFromTextPromptTemplate } from \"./ObjectFromTextPromptTemplate\";\nimport { ObjectGenerationModel } from \"./ObjectGenerationModel\";\nimport { ObjectParseError } from \"./ObjectParseError\";\n\nexport class ObjectFromTextGenerationModel<\n  SOURCE_PROMPT,\n  TARGET_PROMPT,\n  MODEL extends TextGenerationModel<TARGET_PROMPT, TextGenerationModelSettings>,\n> implements ObjectGenerationModel<SOURCE_PROMPT, MODEL[\"settings\"]>\n{\n  protected readonly model: MODEL;\n  protected readonly template: ObjectFromTextPromptTemplate<\n    SOURCE_PROMPT,\n    TARGET_PROMPT\n  >;\n\n  constructor({\n    model,\n    template,\n  }: {\n    model: MODEL;\n    template: ObjectFromTextPromptTemplate<SOURCE_PROMPT, TARGET_PROMPT>;\n  }) {\n    this.model = model;\n    this.template = template;\n  }\n\n  get modelInformation() {\n    return this.model.modelInformation;\n  }\n\n  get settings() {\n    return this.model.settings;\n  }\n\n  get settingsForEvent(): Partial<MODEL[\"settings\"]> {\n    return this.model.settingsForEvent;\n  }\n\n  getModelWithJsonOutput(schema: Schema<unknown> & JsonSchemaProducer) {\n    if (this.template.withJsonOutput != null) {\n      return this.template.withJsonOutput({\n        model: this.model,\n        schema,\n      }) as MODEL;\n    }\n\n    return this.model;\n  }\n\n  async doGenerateObject(\n    schema: Schema<unknown> & JsonSchemaProducer,\n    prompt: SOURCE_PROMPT,\n    options?: FunctionOptions\n  ) {\n    const { rawResponse, text } = await generateText({\n      model: this.getModelWithJsonOutput(schema),\n      prompt: this.template.createPrompt(prompt, schema),\n      fullResponse: true,\n      ...options,\n    });\n\n    try {\n      return {\n        rawResponse,\n        value: this.template.extractObject(text),\n        valueText: text,\n      };\n    } catch (error) {\n      throw new ObjectParseError({\n        valueText: text,\n        cause: error,\n      });\n    }\n  }\n\n  withSettings(additionalSettings: Partial<MODEL[\"settings\"]>): this {\n    return new ObjectFromTextGenerationModel({\n      model: this.model.withSettings(additionalSettings),\n      template: this.template,\n    }) as this;\n  }\n}\n","import { FunctionOptions } from \"../../core/FunctionOptions\";\nimport { PromptFunction, expandPrompt } from \"../../core/PromptFunction\";\nimport { ModelCallMetadata } from \"../ModelCallMetadata\";\nimport { executeStreamCall } from \"../executeStreamCall\";\nimport { TextStreamingModel } from \"./TextGenerationModel\";\n\n/**\n * Stream the generated text for a prompt as an async iterable.\n *\n * The prompt depends on the model used.\n * For instance, OpenAI completion models expect a string prompt,\n * whereas OpenAI chat models expect an array of chat messages.\n *\n * @see https://modelfusion.dev/guide/function/generate-text\n *\n * @example\n * const textStream = await streamText({\n *   model: openai.CompletionTextGenerator(...),\n *   prompt: \"Write a short story about a robot learning to love:\\n\\n\"\n * });\n *\n * for await (const textPart of textStream) {\n *   // ...\n * }\n *\n * @param {TextStreamingModel<PROMPT>} model - The model to stream text from.\n * @param {PROMPT} prompt - The prompt to use for text generation.\n * @param {FunctionOptions} [options] - Optional parameters for the function.\n *\n * @returns {AsyncIterableResultPromise<string>} An async iterable promise that yields the generated text.\n */\nexport async function streamText<PROMPT>(\n  args: {\n    model: TextStreamingModel<PROMPT>;\n    prompt: PROMPT | PromptFunction<unknown, PROMPT>;\n    fullResponse?: false;\n  } & FunctionOptions\n): Promise<AsyncIterable<string>>;\nexport async function streamText<PROMPT>(\n  args: {\n    model: TextStreamingModel<PROMPT>;\n    prompt: PROMPT | PromptFunction<unknown, PROMPT>;\n    fullResponse: true;\n  } & FunctionOptions\n): Promise<{\n  textStream: AsyncIterable<string>;\n  textPromise: PromiseLike<string>;\n  metadata: Omit<ModelCallMetadata, \"durationInMs\" | \"finishTimestamp\">;\n}>;\n\nexport async function streamText<PROMPT>({\n  model,\n  prompt,\n  fullResponse,\n  ...options\n}: {\n  model: TextStreamingModel<PROMPT>;\n  prompt: PROMPT | PromptFunction<unknown, PROMPT>;\n  fullResponse?: boolean;\n} & FunctionOptions): Promise<\n  | AsyncIterable<string>\n  | {\n      textStream: AsyncIterable<string>;\n      textPromise: PromiseLike<string>;\n      metadata: Omit<ModelCallMetadata, \"durationInMs\" | \"finishTimestamp\">;\n    }\n> {\n  const shouldTrimWhitespace = model.settings.trimWhitespace ?? true;\n\n  let accumulatedText = \"\";\n  let isFirstDelta = true;\n  let trailingWhitespace = \"\";\n\n  let resolveText: (value: string) => void;\n  const textPromise = new Promise<string>((resolve) => {\n    resolveText = resolve;\n  });\n\n  const expandedPrompt = await expandPrompt(prompt);\n\n  const callResponse = await executeStreamCall({\n    functionType: \"stream-text\",\n    input: expandedPrompt,\n    model,\n    options,\n    startStream: async (options) =>\n      model.doStreamText(expandedPrompt.prompt, options),\n    processDelta: (delta) => {\n      let textDelta = model.extractTextDelta(delta.deltaValue);\n\n      if (textDelta == null || textDelta.length === 0) {\n        return undefined;\n      }\n\n      if (shouldTrimWhitespace) {\n        textDelta = isFirstDelta\n          ? // remove leading whitespace:\n            textDelta.trimStart()\n          : // restore trailing whitespace from previous chunk:\n            trailingWhitespace + textDelta;\n\n        // trim trailing whitespace and store it for the next chunk:\n        const trailingWhitespaceMatch = textDelta.match(/\\s+$/);\n        trailingWhitespace = trailingWhitespaceMatch\n          ? trailingWhitespaceMatch[0]\n          : \"\";\n        textDelta = textDelta.trimEnd();\n      }\n\n      isFirstDelta = false;\n      accumulatedText += textDelta;\n\n      return textDelta;\n    },\n    onDone: () => {\n      resolveText(accumulatedText);\n    },\n  });\n\n  return fullResponse\n    ? {\n        textStream: callResponse.value,\n        textPromise,\n        metadata: callResponse.metadata,\n      }\n    : callResponse.value;\n}\n","import SecureJSON from \"secure-json-parse\";\nimport { fixJson } from \"./fixJson\";\n\nexport function parsePartialJson(\n  jsonText: string | undefined\n): unknown | undefined {\n  if (jsonText == null) {\n    return undefined;\n  }\n\n  try {\n    // first attempt a regular JSON parse:\n    return SecureJSON.parse(jsonText);\n  } catch (ignored) {\n    try {\n      // then try to fix the partial JSON and parse it:\n      const fixedJsonText = fixJson(jsonText);\n      return SecureJSON.parse(fixedJsonText);\n    } catch (ignored) {\n      // ignored\n    }\n  }\n\n  return undefined;\n}\n","type State =\n  | \"ROOT\"\n  | \"FINISH\"\n  | \"INSIDE_STRING\"\n  | \"INSIDE_STRING_ESCAPE\"\n  | \"INSIDE_LITERAL\"\n  | \"INSIDE_NUMBER\"\n  | \"INSIDE_OBJECT_START\"\n  | \"INSIDE_OBJECT_KEY\"\n  | \"INSIDE_OBJECT_AFTER_KEY\"\n  | \"INSIDE_OBJECT_BEFORE_VALUE\"\n  | \"INSIDE_OBJECT_AFTER_VALUE\"\n  | \"INSIDE_OBJECT_AFTER_COMMA\"\n  | \"INSIDE_ARRAY_START\"\n  | \"INSIDE_ARRAY_AFTER_VALUE\"\n  | \"INSIDE_ARRAY_AFTER_COMMA\";\n\n// Implemented as a scanner with additional fixing\n// that performs a single linear time scan pass over the partial JSON.\n//\n// The states should ideally match relevant states from the JSON spec:\n// https://www.json.org/json-en.html\n//\n// Please note that invalid JSON is not considered/covered, because it\n// is assumed that the resulting JSON will be processed by a standard\n// JSON parser that will detect any invalid JSON.\nexport function fixJson(input: string): string {\n  const stack: State[] = [\"ROOT\"];\n  let lastValidIndex = -1;\n  let literalStart: number | null = null;\n\n  function processValueStart(char: string, i: number, swapState: State) {\n    {\n      switch (char) {\n        case '\"': {\n          lastValidIndex = i;\n          stack.pop();\n          stack.push(swapState);\n          stack.push(\"INSIDE_STRING\");\n          break;\n        }\n\n        case \"f\":\n        case \"t\":\n        case \"n\": {\n          lastValidIndex = i;\n          literalStart = i;\n          stack.pop();\n          stack.push(swapState);\n          stack.push(\"INSIDE_LITERAL\");\n          break;\n        }\n\n        case \"-\": {\n          stack.pop();\n          stack.push(swapState);\n          stack.push(\"INSIDE_NUMBER\");\n          break;\n        }\n        case \"0\":\n        case \"1\":\n        case \"2\":\n        case \"3\":\n        case \"4\":\n        case \"5\":\n        case \"6\":\n        case \"7\":\n        case \"8\":\n        case \"9\": {\n          lastValidIndex = i;\n          stack.pop();\n          stack.push(swapState);\n          stack.push(\"INSIDE_NUMBER\");\n          break;\n        }\n\n        case \"{\": {\n          lastValidIndex = i;\n          stack.pop();\n          stack.push(swapState);\n          stack.push(\"INSIDE_OBJECT_START\");\n          break;\n        }\n\n        case \"[\": {\n          lastValidIndex = i;\n          stack.pop();\n          stack.push(swapState);\n          stack.push(\"INSIDE_ARRAY_START\");\n          break;\n        }\n      }\n    }\n  }\n\n  function processAfterObjectValue(char: string, i: number) {\n    switch (char) {\n      case \",\": {\n        stack.pop();\n        stack.push(\"INSIDE_OBJECT_AFTER_COMMA\");\n        break;\n      }\n      case \"}\": {\n        lastValidIndex = i;\n        stack.pop();\n        break;\n      }\n    }\n  }\n\n  function processAfterArrayValue(char: string, i: number) {\n    switch (char) {\n      case \",\": {\n        stack.pop();\n        stack.push(\"INSIDE_ARRAY_AFTER_COMMA\");\n        break;\n      }\n      case \"]\": {\n        lastValidIndex = i;\n        stack.pop();\n        break;\n      }\n    }\n  }\n\n  for (let i = 0; i < input.length; i++) {\n    const char = input[i];\n    const currentState = stack[stack.length - 1];\n\n    switch (currentState) {\n      case \"ROOT\":\n        processValueStart(char, i, \"FINISH\");\n        break;\n\n      case \"INSIDE_OBJECT_START\": {\n        switch (char) {\n          case '\"': {\n            stack.pop();\n            stack.push(\"INSIDE_OBJECT_KEY\");\n            break;\n          }\n          case \"}\": {\n            stack.pop();\n            break;\n          }\n        }\n        break;\n      }\n\n      case \"INSIDE_OBJECT_AFTER_COMMA\": {\n        switch (char) {\n          case '\"': {\n            stack.pop();\n            stack.push(\"INSIDE_OBJECT_KEY\");\n            break;\n          }\n        }\n        break;\n      }\n\n      case \"INSIDE_OBJECT_KEY\": {\n        switch (char) {\n          case '\"': {\n            stack.pop();\n            stack.push(\"INSIDE_OBJECT_AFTER_KEY\");\n            break;\n          }\n        }\n        break;\n      }\n\n      case \"INSIDE_OBJECT_AFTER_KEY\": {\n        switch (char) {\n          case \":\": {\n            stack.pop();\n            stack.push(\"INSIDE_OBJECT_BEFORE_VALUE\");\n\n            break;\n          }\n        }\n        break;\n      }\n\n      case \"INSIDE_OBJECT_BEFORE_VALUE\": {\n        processValueStart(char, i, \"INSIDE_OBJECT_AFTER_VALUE\");\n        break;\n      }\n\n      case \"INSIDE_OBJECT_AFTER_VALUE\": {\n        processAfterObjectValue(char, i);\n        break;\n      }\n\n      case \"INSIDE_STRING\": {\n        switch (char) {\n          case '\"': {\n            stack.pop();\n            lastValidIndex = i;\n            break;\n          }\n\n          case \"\\\\\": {\n            stack.push(\"INSIDE_STRING_ESCAPE\");\n            break;\n          }\n\n          default: {\n            lastValidIndex = i;\n          }\n        }\n\n        break;\n      }\n\n      case \"INSIDE_ARRAY_START\": {\n        switch (char) {\n          case \"]\": {\n            lastValidIndex = i;\n            stack.pop();\n            break;\n          }\n\n          default: {\n            lastValidIndex = i;\n            processValueStart(char, i, \"INSIDE_ARRAY_AFTER_VALUE\");\n            break;\n          }\n        }\n        break;\n      }\n\n      case \"INSIDE_ARRAY_AFTER_VALUE\": {\n        switch (char) {\n          case \",\": {\n            stack.pop();\n            stack.push(\"INSIDE_ARRAY_AFTER_COMMA\");\n            break;\n          }\n\n          case \"]\": {\n            lastValidIndex = i;\n            stack.pop();\n            break;\n          }\n\n          default: {\n            lastValidIndex = i;\n            break;\n          }\n        }\n\n        break;\n      }\n\n      case \"INSIDE_ARRAY_AFTER_COMMA\": {\n        processValueStart(char, i, \"INSIDE_ARRAY_AFTER_VALUE\");\n        break;\n      }\n\n      case \"INSIDE_STRING_ESCAPE\": {\n        stack.pop();\n        lastValidIndex = i;\n\n        break;\n      }\n\n      case \"INSIDE_NUMBER\": {\n        switch (char) {\n          case \"0\":\n          case \"1\":\n          case \"2\":\n          case \"3\":\n          case \"4\":\n          case \"5\":\n          case \"6\":\n          case \"7\":\n          case \"8\":\n          case \"9\": {\n            lastValidIndex = i;\n            break;\n          }\n\n          case \"e\":\n          case \"E\":\n          case \"-\":\n          case \".\": {\n            break;\n          }\n\n          case \",\": {\n            stack.pop();\n\n            if (stack[stack.length - 1] === \"INSIDE_ARRAY_AFTER_VALUE\") {\n              processAfterArrayValue(char, i);\n            }\n\n            if (stack[stack.length - 1] === \"INSIDE_OBJECT_AFTER_VALUE\") {\n              processAfterObjectValue(char, i);\n            }\n\n            break;\n          }\n\n          case \"}\": {\n            stack.pop();\n\n            if (stack[stack.length - 1] === \"INSIDE_OBJECT_AFTER_VALUE\") {\n              processAfterObjectValue(char, i);\n            }\n\n            break;\n          }\n\n          case \"]\": {\n            stack.pop();\n\n            if (stack[stack.length - 1] === \"INSIDE_ARRAY_AFTER_VALUE\") {\n              processAfterArrayValue(char, i);\n            }\n\n            break;\n          }\n\n          default: {\n            stack.pop();\n            break;\n          }\n        }\n\n        break;\n      }\n\n      case \"INSIDE_LITERAL\": {\n        const partialLiteral = input.substring(literalStart!, i + 1);\n\n        if (\n          !\"false\".startsWith(partialLiteral) &&\n          !\"true\".startsWith(partialLiteral) &&\n          !\"null\".startsWith(partialLiteral)\n        ) {\n          stack.pop();\n\n          if (stack[stack.length - 1] === \"INSIDE_OBJECT_AFTER_VALUE\") {\n            processAfterObjectValue(char, i);\n          } else if (stack[stack.length - 1] === \"INSIDE_ARRAY_AFTER_VALUE\") {\n            processAfterArrayValue(char, i);\n          }\n        } else {\n          lastValidIndex = i;\n        }\n\n        break;\n      }\n    }\n  }\n\n  let result = input.slice(0, lastValidIndex + 1);\n\n  for (let i = stack.length - 1; i >= 0; i--) {\n    const state = stack[i];\n\n    switch (state) {\n      case \"INSIDE_STRING\": {\n        result += '\"';\n        break;\n      }\n\n      case \"INSIDE_OBJECT_KEY\":\n      case \"INSIDE_OBJECT_AFTER_KEY\":\n      case \"INSIDE_OBJECT_AFTER_COMMA\":\n      case \"INSIDE_OBJECT_START\":\n      case \"INSIDE_OBJECT_BEFORE_VALUE\":\n      case \"INSIDE_OBJECT_AFTER_VALUE\": {\n        result += \"}\";\n        break;\n      }\n\n      case \"INSIDE_ARRAY_START\":\n      case \"INSIDE_ARRAY_AFTER_COMMA\":\n      case \"INSIDE_ARRAY_AFTER_VALUE\": {\n        result += \"]\";\n        break;\n      }\n\n      case \"INSIDE_LITERAL\": {\n        const partialLiteral = input.substring(literalStart!, input.length);\n\n        if (\"true\".startsWith(partialLiteral)) {\n          result += \"true\".slice(partialLiteral.length);\n        } else if (\"false\".startsWith(partialLiteral)) {\n          result += \"false\".slice(partialLiteral.length);\n        } else if (\"null\".startsWith(partialLiteral)) {\n          result += \"null\".slice(partialLiteral.length);\n        }\n      }\n    }\n  }\n\n  return result;\n}\n","import { FunctionOptions } from \"../../core/FunctionOptions\";\nimport { JsonSchemaProducer } from \"../../core/schema/JsonSchemaProducer\";\nimport { Schema } from \"../../core/schema/Schema\";\nimport { Delta } from \"../Delta\";\nimport { streamText } from \"../generate-text/streamText\";\nimport { AsyncQueue } from \"../../util/AsyncQueue\";\nimport { parsePartialJson } from \"../../util/parsePartialJson\";\nimport {\n  TextGenerationModelSettings,\n  TextStreamingModel,\n} from \"../generate-text/TextGenerationModel\";\nimport { ObjectFromTextGenerationModel } from \"./ObjectFromTextGenerationModel\";\nimport { ObjectFromTextPromptTemplate } from \"./ObjectFromTextPromptTemplate\";\nimport { ObjectStreamingModel } from \"./ObjectGenerationModel\";\n\nexport class ObjectFromTextStreamingModel<\n    SOURCE_PROMPT,\n    TARGET_PROMPT,\n    MODEL extends TextStreamingModel<\n      TARGET_PROMPT,\n      TextGenerationModelSettings\n    >,\n  >\n  extends ObjectFromTextGenerationModel<SOURCE_PROMPT, TARGET_PROMPT, MODEL>\n  implements ObjectStreamingModel<SOURCE_PROMPT, MODEL[\"settings\"]>\n{\n  constructor(options: {\n    model: MODEL;\n    template: ObjectFromTextPromptTemplate<SOURCE_PROMPT, TARGET_PROMPT>;\n  }) {\n    super(options);\n  }\n\n  async doStreamObject(\n    schema: Schema<unknown> & JsonSchemaProducer,\n    prompt: SOURCE_PROMPT,\n    options?: FunctionOptions\n  ) {\n    const textStream = await streamText({\n      model: this.getModelWithJsonOutput(schema),\n      prompt: this.template.createPrompt(prompt, schema),\n      ...options,\n    });\n\n    const queue = new AsyncQueue<Delta<string>>();\n\n    // run async on purpose:\n    (async () => {\n      try {\n        for await (const deltaText of textStream) {\n          queue.push({ type: \"delta\", deltaValue: deltaText });\n        }\n      } catch (error) {\n        queue.push({ type: \"error\", error });\n      } finally {\n        queue.close();\n      }\n    })();\n\n    return queue;\n  }\n\n  extractObjectTextDelta(delta: unknown): string {\n    return delta as string;\n  }\n\n  parseAccumulatedObjectText(accumulatedText: string): unknown {\n    return parsePartialJson(accumulatedText);\n  }\n\n  withSettings(additionalSettings: Partial<MODEL[\"settings\"]>): this {\n    return new ObjectFromTextStreamingModel({\n      model: this.model.withSettings(additionalSettings),\n      template: this.template,\n    }) as this;\n  }\n}\n","import type { PartialDeep } from \"type-fest\";\nimport { Schema } from \"../../core/schema/Schema\";\nimport { parsePartialJson } from \"../../util/parsePartialJson\";\n\nexport type ObjectStream<OBJECT> = AsyncIterable<{\n  partialObject: PartialDeep<OBJECT, { recurseIntoArrays: true }>;\n  partialText: string;\n  textDelta: string;\n}>;\n\n/**\n * Response for ObjectStream. The object stream is encoded as a text stream.\n *\n * Example:\n * ```ts\n * return new ObjectStreamResponse(objectStream);\n * ```\n */\nexport class ObjectStreamResponse extends Response {\n  constructor(stream: ObjectStream<unknown>, init?: ResponseInit) {\n    super(ObjectStreamToTextStream(stream), {\n      ...init,\n      status: 200,\n      headers: { \"Content-Type\": \"text/plain; charset=utf-8\" },\n    });\n  }\n}\n\n/**\n * Convert a Response to a lightweight ObjectStream. The response must be created\n * using ObjectStreamResponse on the server.\n *\n * @see ObjectStreamResponse\n */\nexport async function* ObjectStreamFromResponse<OBJECT>({\n  response,\n}: {\n  schema: Schema<OBJECT>;\n  response: Response;\n}) {\n  let text = \"\";\n\n  const reader = response.body!.getReader();\n\n  // eslint-disable-next-line no-constant-condition\n  while (true) {\n    const { done, value } = await reader.read();\n\n    if (done) break;\n\n    text += new TextDecoder().decode(value);\n\n    const partialObject = parsePartialJson(text) as OBJECT;\n\n    yield { partialObject };\n  }\n}\n\nfunction ObjectStreamToTextStream(stream: ObjectStream<unknown>) {\n  const textEncoder = new TextEncoder();\n  return new ReadableStream({\n    async start(controller) {\n      try {\n        for await (const { textDelta } of stream) {\n          controller.enqueue(textEncoder.encode(textDelta));\n        }\n      } finally {\n        controller.close();\n      }\n    },\n  });\n}\n","import { getErrorMessage } from \"../../util/getErrorMessage\";\n\nexport class ObjectValidationError extends Error {\n  readonly cause: unknown;\n  readonly valueText: string;\n  readonly value: unknown;\n\n  constructor({\n    value,\n    valueText,\n    cause,\n  }: {\n    value: unknown;\n    valueText: string;\n    cause: unknown;\n  }) {\n    super(\n      `Object validation failed. ` +\n        `Value: ${valueText}.\\n` +\n        `Error message: ${getErrorMessage(cause)}`\n    );\n\n    this.name = \"ObjectValidationError\";\n\n    this.cause = cause;\n    this.value = value;\n    this.valueText = valueText;\n  }\n\n  toJSON() {\n    return {\n      name: this.name,\n      message: this.message,\n      cause: this.cause,\n      stack: this.stack,\n\n      value: this.value,\n      valueText: this.valueText,\n    };\n  }\n}\n","import {\n  PromptFunction,\n  expandPrompt,\n  isPromptFunction,\n} from \"../../core/PromptFunction\";\nimport { FunctionOptions } from \"../../core/FunctionOptions\";\nimport { JsonSchemaProducer } from \"../../core/schema/JsonSchemaProducer\";\nimport { Schema } from \"../../core/schema/Schema\";\nimport { ModelCallMetadata } from \"../ModelCallMetadata\";\nimport { executeStandardCall } from \"../executeStandardCall\";\nimport {\n  ObjectGenerationModel,\n  ObjectGenerationModelSettings,\n} from \"./ObjectGenerationModel\";\nimport { ObjectValidationError } from \"./ObjectValidationError\";\n\n/**\n * Generate a typed object for a prompt and a schema.\n *\n * @see https://modelfusion.dev/guide/function/generate-object\n *\n * @example\n * const sentiment = await generateObject({\n *   model: openai.ChatTextGenerator(...).asFunctionCallObjectGenerationModel(...),\n *\n *   schema: zodSchema(z.object({\n *     sentiment: z\n *       .enum([\"positive\", \"neutral\", \"negative\"])\n *       .describe(\"Sentiment.\"),\n *   })),\n *\n *   prompt: [\n *     openai.ChatMessage.system(\n *       \"You are a sentiment evaluator. \" +\n *         \"Analyze the sentiment of the following product review:\"\n *     ),\n *     openai.ChatMessage.user(\n *       \"After I opened the package, I was met by a very unpleasant smell \" +\n *         \"that did not disappear even after washing. Never again!\"\n *     ),\n *   ]\n * });\n *\n * @param {ObjectGenerationModel<PROMPT, SETTINGS>} options.model - The model to generate the object.\n * @param {Schema<OBJECT>} options.schema - The schema to be used.\n * @param {PROMPT | ((schema: Schema<OBJECT>) => PROMPT)} options.prompt\n * The prompt to be used.\n * You can also pass a function that takes the schema as an argument and returns the prompt.\n *\n * @returns {Promise<OBJECT>} - Returns a promise that resolves to the generated object.\n */\nexport async function generateObject<\n  OBJECT,\n  PROMPT,\n  SETTINGS extends ObjectGenerationModelSettings,\n>(\n  args: {\n    model: ObjectGenerationModel<PROMPT, SETTINGS>;\n    schema: Schema<OBJECT> & JsonSchemaProducer;\n    prompt:\n      | PROMPT\n      | PromptFunction<unknown, PROMPT>\n      | ((schema: Schema<OBJECT>) => PROMPT | PromptFunction<unknown, PROMPT>);\n    fullResponse?: false;\n  } & FunctionOptions\n): Promise<OBJECT>;\nexport async function generateObject<\n  OBJECT,\n  PROMPT,\n  SETTINGS extends ObjectGenerationModelSettings,\n>(\n  args: {\n    model: ObjectGenerationModel<PROMPT, SETTINGS>;\n    schema: Schema<OBJECT> & JsonSchemaProducer;\n    prompt:\n      | PROMPT\n      | PromptFunction<unknown, PROMPT>\n      | ((schema: Schema<OBJECT>) => PROMPT | PromptFunction<unknown, PROMPT>);\n    fullResponse: true;\n  } & FunctionOptions\n): Promise<{\n  value: OBJECT;\n  rawResponse: unknown;\n  metadata: ModelCallMetadata;\n}>;\nexport async function generateObject<\n  OBJECT,\n  PROMPT,\n  SETTINGS extends ObjectGenerationModelSettings,\n>({\n  model,\n  schema,\n  prompt,\n  fullResponse,\n  ...options\n}: {\n  model: ObjectGenerationModel<PROMPT, SETTINGS>;\n  schema: Schema<OBJECT> & JsonSchemaProducer;\n  prompt:\n    | PROMPT\n    | PromptFunction<unknown, PROMPT>\n    | ((schema: Schema<OBJECT>) => PROMPT | PromptFunction<unknown, PROMPT>);\n  fullResponse?: boolean;\n} & FunctionOptions): Promise<\n  | OBJECT\n  | {\n      value: OBJECT;\n      rawResponse: unknown;\n      metadata: ModelCallMetadata;\n    }\n> {\n  // Resolve the prompt if it is a function (and not a PromptFunction)\n  const resolvedPrompt =\n    typeof prompt === \"function\" && !isPromptFunction(prompt)\n      ? (prompt as (schema: Schema<OBJECT>) => PROMPT)(schema)\n      : prompt;\n\n  const expandedPrompt = await expandPrompt(resolvedPrompt);\n\n  const callResponse = await executeStandardCall({\n    functionType: \"generate-object\",\n    input: {\n      schema,\n      ...expandedPrompt,\n    },\n    model,\n    options,\n    generateResponse: async (options) => {\n      const result = await model.doGenerateObject(\n        schema,\n        expandedPrompt.prompt,\n        options\n      );\n\n      const parseResult = schema.validate(result.value);\n\n      if (!parseResult.success) {\n        throw new ObjectValidationError({\n          valueText: result.valueText,\n          value: result.value,\n          cause: parseResult.error,\n        });\n      }\n\n      const value = parseResult.value;\n\n      return {\n        rawResponse: result.rawResponse,\n        extractedValue: value,\n        usage: result.usage,\n      };\n    },\n  });\n\n  return fullResponse\n    ? {\n        value: callResponse.value,\n        rawResponse: callResponse.rawResponse,\n        metadata: callResponse.metadata,\n      }\n    : callResponse.value;\n}\n","import { JsonSchemaProducer } from \"../../core/schema/JsonSchemaProducer\";\nimport { Schema } from \"../../core/schema/Schema\";\nimport { parseJSON } from \"../../core/schema/parseJSON\";\nimport { InstructionPrompt } from \"../generate-text/prompt-template/InstructionPrompt\";\nimport {\n  FlexibleObjectFromTextPromptTemplate,\n  ObjectFromTextPromptTemplate,\n} from \"./ObjectFromTextPromptTemplate\";\n\nconst DEFAULT_SCHEMA_PREFIX = \"JSON schema:\";\nconst DEFAULT_SCHEMA_SUFFIX =\n  \"\\nYou MUST answer with a JSON object that matches the JSON schema above.\";\n\nexport const jsonObjectPrompt = {\n  custom<SOURCE_PROMPT, TARGET_PROMPT>(\n    createPrompt: (\n      prompt: SOURCE_PROMPT,\n      schema: Schema<unknown> & JsonSchemaProducer\n    ) => TARGET_PROMPT\n  ): ObjectFromTextPromptTemplate<SOURCE_PROMPT, TARGET_PROMPT> {\n    return { createPrompt, extractObject };\n  },\n\n  text({\n    schemaPrefix,\n    schemaSuffix,\n  }: {\n    schemaPrefix?: string;\n    schemaSuffix?: string;\n  } = {}): FlexibleObjectFromTextPromptTemplate<string, InstructionPrompt> {\n    return {\n      createPrompt: (\n        prompt: string,\n        schema: Schema<unknown> & JsonSchemaProducer\n      ) => ({\n        system: createSystemPrompt({ schema, schemaPrefix, schemaSuffix }),\n        instruction: prompt,\n      }),\n      extractObject,\n      adaptModel: (model) => model.withInstructionPrompt(),\n      withJsonOutput: ({ model, schema }) => model.withJsonOutput(schema),\n    };\n  },\n\n  instruction({\n    schemaPrefix,\n    schemaSuffix,\n  }: {\n    schemaPrefix?: string;\n    schemaSuffix?: string;\n  } = {}): FlexibleObjectFromTextPromptTemplate<\n    InstructionPrompt,\n    InstructionPrompt\n  > {\n    return {\n      createPrompt: (\n        prompt: InstructionPrompt,\n        schema: Schema<unknown> & JsonSchemaProducer\n      ) => ({\n        system: createSystemPrompt({\n          originalSystemPrompt: prompt.system,\n          schema,\n          schemaPrefix,\n          schemaSuffix,\n        }),\n        instruction: prompt.instruction,\n      }),\n      extractObject,\n      adaptModel: (model) => model.withInstructionPrompt(),\n      withJsonOutput: ({ model, schema }) => model.withJsonOutput(schema),\n    };\n  },\n};\n\nfunction createSystemPrompt({\n  originalSystemPrompt,\n  schema,\n  schemaPrefix = DEFAULT_SCHEMA_PREFIX,\n  schemaSuffix = DEFAULT_SCHEMA_SUFFIX,\n}: {\n  originalSystemPrompt?: string;\n  schema: Schema<unknown> & JsonSchemaProducer;\n  schemaPrefix?: string;\n  schemaSuffix?: string;\n}) {\n  return [\n    originalSystemPrompt,\n    originalSystemPrompt != null ? \"\" : null,\n    schemaPrefix,\n    JSON.stringify(schema.getJsonSchema()),\n    schemaSuffix,\n  ]\n    .filter(Boolean)\n    .join(\"\\n\");\n}\n\nfunction extractObject(response: string): unknown {\n  return parseJSON({ text: response });\n}\n","/* eslint-disable @typescript-eslint/no-explicit-any */\n/**\n * Performs a deep-equal comparison of two parsed JSON objects.\n *\n * @param {any} obj1 - The first object to compare.\n * @param {any} obj2 - The second object to compare.\n * @returns {boolean} - Returns true if the two objects are deeply equal, false otherwise.\n */\nexport function isDeepEqualData(obj1: any, obj2: any): boolean {\n  // Check for strict equality first\n  if (obj1 === obj2) return true;\n\n  // Check if either is null or undefined\n  if (obj1 == null || obj2 == null) return false;\n\n  // Check if both are objects\n  if (typeof obj1 !== \"object\" && typeof obj2 !== \"object\")\n    return obj1 === obj2;\n\n  // If they are not strictly equal, they both need to be Objects\n  if (obj1.constructor !== obj2.constructor) return false;\n\n  // Special handling for Date objects\n  if (obj1 instanceof Date && obj2 instanceof Date) {\n    return obj1.getTime() === obj2.getTime();\n  }\n\n  // Handle arrays: compare length and then perform a recursive deep comparison on each item\n  if (Array.isArray(obj1)) {\n    if (obj1.length !== obj2.length) return false;\n    for (let i = 0; i < obj1.length; i++) {\n      if (!isDeepEqualData(obj1[i], obj2[i])) return false;\n    }\n    return true; // All array elements matched\n  }\n\n  // Compare the set of keys in each object\n  const keys1 = Object.keys(obj1);\n  const keys2 = Object.keys(obj2);\n  if (keys1.length !== keys2.length) return false;\n\n  // Check each key-value pair recursively\n  for (const key of keys1) {\n    if (!keys2.includes(key)) return false;\n    if (!isDeepEqualData(obj1[key], obj2[key])) return false;\n  }\n\n  return true; // All keys and values matched\n}\n","import type { PartialDeep } from \"type-fest\";\nimport { FunctionOptions } from \"../../core/FunctionOptions\";\nimport { JsonSchemaProducer } from \"../../core/schema/JsonSchemaProducer\";\nimport { Schema } from \"../../core/schema/Schema\";\nimport { isDeepEqualData } from \"../../util/isDeepEqualData\";\nimport { ModelCallMetadata } from \"../ModelCallMetadata\";\nimport { executeStreamCall } from \"../executeStreamCall\";\nimport { ObjectStreamingModel } from \"./ObjectGenerationModel\";\nimport { ObjectStream } from \"./ObjectStream\";\nimport {\n  PromptFunction,\n  expandPrompt,\n  isPromptFunction,\n} from \"../../core/PromptFunction\";\n\n/**\n * Generate and stream an object for a prompt and a schema.\n *\n * @see https://modelfusion.dev/guide/function/generate-object\n *\n * @example\n * const objectStream = await streamObject({\n *   model: openai.ChatTextGenerator(...).asFunctionCallObjectGenerationModel(...),\n *   schema: zodSchema(\n *     z.array(\n *       z.object({\n *         name: z.string(),\n *         class: z\n *           .string()\n *           .describe(\"Character class, e.g. warrior, mage, or thief.\"),\n *         description: z.string(),\n *       })\n *     ),\n *   prompt: [\n *     openai.ChatMessage.user(\n *       \"Generate 3 character descriptions for a fantasy role playing game.\"\n *     ),\n *   ]\n * });\n *\n * for await (const { partialObject } of objectStream) {\n *   // ...\n * }\n *\n * @param {ObjectStreamingModel<PROMPT>} options.model - The model that generates the object\n * @param {Schema<OBJECT>} options.schema - The schema of the object to be generated.\n * @param {PROMPT | ((schema: Schema<OBJECT>) => PROMPT)} options.prompt\n * The prompt to be used.\n * You can also pass a function that takes the schema as an argument and returns the prompt.\n */\nexport async function streamObject<OBJECT, PROMPT>(\n  args: {\n    model: ObjectStreamingModel<PROMPT>;\n    schema: Schema<OBJECT> & JsonSchemaProducer;\n    prompt:\n      | PROMPT\n      | PromptFunction<unknown, PROMPT>\n      | ((schema: Schema<OBJECT>) => PROMPT | PromptFunction<unknown, PROMPT>);\n    fullResponse?: false;\n  } & FunctionOptions\n): Promise<ObjectStream<OBJECT>>;\nexport async function streamObject<OBJECT, PROMPT>(\n  args: {\n    model: ObjectStreamingModel<PROMPT>;\n    schema: Schema<OBJECT> & JsonSchemaProducer;\n    prompt:\n      | PROMPT\n      | PromptFunction<unknown, PROMPT>\n      | ((schema: Schema<OBJECT>) => PROMPT | PromptFunction<unknown, PROMPT>);\n    fullResponse: true;\n  } & FunctionOptions\n): Promise<{\n  objectStream: ObjectStream<OBJECT>;\n  objectPromise: PromiseLike<OBJECT>;\n  metadata: Omit<ModelCallMetadata, \"durationInMs\" | \"finishTimestamp\">;\n}>;\nexport async function streamObject<OBJECT, PROMPT>({\n  model,\n  schema,\n  prompt,\n  fullResponse,\n  ...options\n}: {\n  model: ObjectStreamingModel<PROMPT>;\n  schema: Schema<OBJECT> & JsonSchemaProducer;\n  prompt:\n    | PROMPT\n    | PromptFunction<unknown, PROMPT>\n    | ((schema: Schema<OBJECT>) => PROMPT | PromptFunction<unknown, PROMPT>);\n  fullResponse?: boolean;\n} & FunctionOptions): Promise<\n  | ObjectStream<OBJECT>\n  | {\n      objectStream: ObjectStream<OBJECT>;\n      objectPromise: PromiseLike<OBJECT>;\n      metadata: Omit<ModelCallMetadata, \"durationInMs\" | \"finishTimestamp\">;\n    }\n> {\n  // Resolve the prompt if it is a function (and not a PromptFunction)\n  const resolvedPrompt =\n    typeof prompt === \"function\" && !isPromptFunction(prompt)\n      ? (prompt as (schema: Schema<OBJECT>) => PROMPT)(schema)\n      : prompt;\n\n  const expandedPrompt = await expandPrompt(resolvedPrompt);\n\n  let accumulatedText = \"\";\n  let accumulatedTextDelta = \"\";\n  let latestObject: unknown | undefined;\n\n  let resolveObject: (value: OBJECT) => void;\n  let rejectObject: (reason: unknown) => void;\n  const objectPromise = new Promise<OBJECT>((resolve, reject) => {\n    resolveObject = resolve;\n    rejectObject = reject;\n  });\n\n  const callResponse = await executeStreamCall<\n    unknown,\n    {\n      partialObject: PartialDeep<OBJECT>;\n      partialText: string;\n      textDelta: string;\n    },\n    ObjectStreamingModel<PROMPT>\n  >({\n    functionType: \"stream-object\",\n    input: {\n      schema,\n      ...expandedPrompt,\n    },\n    model,\n    options,\n    startStream: async (options) =>\n      model.doStreamObject(schema, expandedPrompt.prompt, options),\n\n    processDelta(delta) {\n      const textDelta = model.extractObjectTextDelta(delta.deltaValue);\n\n      if (textDelta == null) {\n        return undefined;\n      }\n\n      accumulatedText += textDelta;\n      accumulatedTextDelta += textDelta;\n\n      const currentObject = model.parseAccumulatedObjectText(accumulatedText);\n\n      // only send a new part into the stream when the partial object has changed:\n      if (!isDeepEqualData(latestObject, currentObject)) {\n        latestObject = currentObject;\n\n        // reset delta accumulation:\n        const currentAccumulatedTextDelta = accumulatedTextDelta;\n        accumulatedTextDelta = \"\";\n\n        // TODO add type checking\n        return {\n          partialObject: latestObject as PartialDeep<\n            OBJECT,\n            { recurseIntoArrays: true }\n          >,\n          partialText: accumulatedText,\n          textDelta: currentAccumulatedTextDelta,\n        };\n      }\n\n      return undefined;\n    },\n\n    // The last object is processed and returned, even if it was already returned previously.\n    // The reason is that the full text delta should be returned (and no characters should be omitted).\n    processFinished() {\n      return {\n        partialObject: latestObject as PartialDeep<\n          OBJECT,\n          { recurseIntoArrays: true }\n        >,\n        partialText: accumulatedText,\n        textDelta: accumulatedTextDelta,\n      };\n    },\n\n    onDone() {\n      // process the final result (full type validation):\n      const parseResult = schema.validate(latestObject);\n\n      if (parseResult.success) {\n        resolveObject(parseResult.value);\n      } else {\n        rejectObject(parseResult.error);\n      }\n    },\n  });\n\n  return fullResponse\n    ? {\n        objectStream: callResponse.value,\n        objectPromise: objectPromise,\n        metadata: callResponse.metadata,\n      }\n    : callResponse.value;\n}\n","import { getErrorMessage } from \"../../util/getErrorMessage\";\n\nexport class ToolCallParseError extends Error {\n  readonly toolName: string;\n  readonly valueText: string;\n  readonly cause: unknown;\n\n  constructor({\n    toolName,\n    valueText,\n    cause,\n  }: {\n    toolName: string;\n    valueText: string;\n    cause: unknown;\n  }) {\n    super(\n      `Tool call parsing failed for '${toolName}'. ` +\n        `Value: ${valueText}.\\n` +\n        `Error message: ${getErrorMessage(cause)}`\n    );\n\n    this.name = \"ToolCallParseError\";\n\n    this.toolName = toolName;\n    this.cause = cause;\n    this.valueText = valueText;\n  }\n\n  toJSON() {\n    return {\n      name: this.name,\n      cause: this.cause,\n      message: this.message,\n      stack: this.stack,\n\n      toolName: this.toolName,\n      valueText: this.valueText,\n    };\n  }\n}\n","import { FunctionOptions } from \"../../core/FunctionOptions\";\nimport { JsonSchemaProducer } from \"../../core/schema/JsonSchemaProducer\";\nimport { Schema } from \"../../core/schema/Schema\";\nimport {\n  TextGenerationModel,\n  TextGenerationModelSettings,\n} from \"../../model-function/generate-text/TextGenerationModel\";\nimport { generateText } from \"../../model-function/generate-text/generateText\";\nimport { ToolDefinition } from \"../ToolDefinition\";\nimport { ToolCallGenerationModel } from \"./ToolCallGenerationModel\";\nimport { ToolCallParseError } from \"./ToolCallParseError\";\nimport { ToolCallPromptTemplate } from \"./ToolCallPromptTemplate\";\n\nexport class TextGenerationToolCallModel<\n  SOURCE_PROMPT,\n  TARGET_PROMPT,\n  MODEL extends TextGenerationModel<TARGET_PROMPT, TextGenerationModelSettings>,\n> implements ToolCallGenerationModel<SOURCE_PROMPT, MODEL[\"settings\"]>\n{\n  private readonly model: MODEL;\n  private readonly template: ToolCallPromptTemplate<\n    SOURCE_PROMPT,\n    TARGET_PROMPT\n  >;\n\n  constructor({\n    model,\n    template,\n  }: {\n    model: MODEL;\n    template: ToolCallPromptTemplate<SOURCE_PROMPT, TARGET_PROMPT>;\n  }) {\n    this.model = model;\n    this.template = template;\n  }\n\n  get modelInformation() {\n    return this.model.modelInformation;\n  }\n\n  get settings() {\n    return this.model.settings;\n  }\n\n  get settingsForEvent(): Partial<MODEL[\"settings\"]> {\n    return this.model.settingsForEvent;\n  }\n\n  getModelWithJsonOutput(schema: Schema<unknown> & JsonSchemaProducer) {\n    if (this.template.withJsonOutput != null) {\n      return this.template.withJsonOutput({\n        model: this.model,\n        schema,\n      }) as MODEL;\n    }\n\n    return this.model;\n  }\n\n  async doGenerateToolCall(\n    tool: ToolDefinition<string, unknown>,\n    prompt: SOURCE_PROMPT,\n    options?: FunctionOptions\n  ) {\n    const { rawResponse, text, metadata } = await generateText({\n      model: this.getModelWithJsonOutput(tool.parameters),\n      prompt: this.template.createPrompt(prompt, tool),\n      fullResponse: true,\n      ...options,\n    });\n\n    try {\n      return {\n        rawResponse,\n        toolCall: this.template.extractToolCall(text, tool),\n        usage: metadata?.usage as\n          | {\n              promptTokens: number;\n              completionTokens: number;\n              totalTokens: number;\n            }\n          | undefined,\n      };\n    } catch (error) {\n      throw new ToolCallParseError({\n        toolName: tool.name,\n        valueText: text,\n        cause: error,\n      });\n    }\n  }\n\n  withSettings(additionalSettings: Partial<MODEL[\"settings\"]>): this {\n    return new TextGenerationToolCallModel({\n      model: this.model.withSettings(additionalSettings),\n      template: this.template,\n    }) as this;\n  }\n}\n","import { getErrorMessage } from \"../../util/getErrorMessage\";\n\nexport class ToolCallsParseError extends Error {\n  readonly valueText: string;\n  readonly cause: unknown;\n\n  constructor({ valueText, cause }: { valueText: string; cause: unknown }) {\n    super(\n      `Tool calls parsing failed. ` +\n        `Value: ${valueText}.\\n` +\n        `Error message: ${getErrorMessage(cause)}`\n    );\n\n    this.name = \"ToolCallsParseError\";\n\n    this.cause = cause;\n    this.valueText = valueText;\n  }\n\n  toJSON() {\n    return {\n      name: this.name,\n      cause: this.cause,\n      message: this.message,\n      stack: this.stack,\n\n      valueText: this.valueText,\n    };\n  }\n}\n","import { FunctionOptions } from \"../../core/FunctionOptions\";\nimport { TextGenerationModel } from \"../../model-function/generate-text/TextGenerationModel\";\nimport { generateText } from \"../../model-function/generate-text/generateText\";\nimport { ToolDefinition } from \"../ToolDefinition\";\nimport {\n  ToolCallsGenerationModel,\n  ToolCallsGenerationModelSettings,\n} from \"./ToolCallsGenerationModel\";\nimport { ToolCallsPromptTemplate } from \"./ToolCallsPromptTemplate\";\nimport { ToolCallsParseError } from \"./ToolCallsParseError\";\n\nexport class TextGenerationToolCallsModel<\n  SOURCE_PROMPT,\n  TARGET_PROMPT,\n  MODEL extends TextGenerationModel<\n    TARGET_PROMPT,\n    ToolCallsGenerationModelSettings\n  >,\n> implements ToolCallsGenerationModel<SOURCE_PROMPT, MODEL[\"settings\"]>\n{\n  private readonly model: MODEL;\n  private readonly template: ToolCallsPromptTemplate<\n    SOURCE_PROMPT,\n    TARGET_PROMPT\n  >;\n\n  constructor({\n    model,\n    template,\n  }: {\n    model: MODEL;\n    template: ToolCallsPromptTemplate<SOURCE_PROMPT, TARGET_PROMPT>;\n  }) {\n    this.model = model;\n    this.template = template;\n  }\n\n  get modelInformation() {\n    return this.model.modelInformation;\n  }\n\n  get settings() {\n    return this.model.settings;\n  }\n\n  get settingsForEvent(): Partial<MODEL[\"settings\"]> {\n    return this.model.settingsForEvent;\n  }\n\n  async doGenerateToolCalls(\n    tools: Array<ToolDefinition<string, unknown>>,\n    prompt: SOURCE_PROMPT,\n    options?: FunctionOptions\n  ) {\n    const {\n      rawResponse,\n      text: generatedText,\n      metadata,\n    } = await generateText({\n      model: this.model,\n      prompt: this.template.createPrompt(prompt, tools),\n      fullResponse: true,\n      ...options,\n    });\n\n    try {\n      const { text, toolCalls } =\n        this.template.extractToolCallsAndText(generatedText);\n\n      return {\n        rawResponse,\n        text,\n        toolCalls,\n        usage: metadata?.usage as\n          | {\n              promptTokens: number;\n              completionTokens: number;\n              totalTokens: number;\n            }\n          | undefined,\n      };\n    } catch (error) {\n      throw new ToolCallsParseError({\n        valueText: generatedText,\n        cause: error,\n      });\n    }\n  }\n\n  withSettings(additionalSettings: Partial<MODEL[\"settings\"]>): this {\n    return new TextGenerationToolCallsModel({\n      model: this.model.withSettings(additionalSettings),\n      template: this.template,\n    }) as this;\n  }\n}\n","import { FunctionCallOptions } from \"../../core/FunctionOptions\";\nimport { JsonSchemaProducer } from \"../../core/schema/JsonSchemaProducer\";\nimport { Schema } from \"../../core/schema/Schema\";\nimport { TextGenerationToolCallModel } from \"../../tool/generate-tool-call/TextGenerationToolCallModel\";\nimport { ToolCallPromptTemplate } from \"../../tool/generate-tool-call/ToolCallPromptTemplate\";\nimport { TextGenerationToolCallsModel } from \"../../tool/generate-tool-calls/TextGenerationToolCallsModel\";\nimport { ToolCallsPromptTemplate } from \"../../tool/generate-tool-calls/ToolCallsPromptTemplate\";\nimport { ObjectFromTextGenerationModel } from \"../generate-object/ObjectFromTextGenerationModel\";\nimport { ObjectFromTextPromptTemplate } from \"../generate-object/ObjectFromTextPromptTemplate\";\nimport {\n  TextGenerationModel,\n  TextGenerationModelSettings,\n} from \"./TextGenerationModel\";\nimport { TextGenerationPromptTemplate } from \"./TextGenerationPromptTemplate\";\n\nexport class PromptTemplateTextGenerationModel<\n  PROMPT,\n  MODEL_PROMPT,\n  SETTINGS extends TextGenerationModelSettings,\n  MODEL extends TextGenerationModel<MODEL_PROMPT, SETTINGS>,\n> implements TextGenerationModel<PROMPT, SETTINGS>\n{\n  readonly model: MODEL;\n  readonly promptTemplate: TextGenerationPromptTemplate<PROMPT, MODEL_PROMPT>;\n\n  constructor({\n    model,\n    promptTemplate,\n  }: {\n    model: MODEL;\n    promptTemplate: TextGenerationPromptTemplate<PROMPT, MODEL_PROMPT>;\n  }) {\n    this.model = model;\n    this.promptTemplate = promptTemplate;\n  }\n\n  get modelInformation() {\n    return this.model.modelInformation;\n  }\n\n  get settings() {\n    return this.model.settings;\n  }\n\n  get tokenizer(): MODEL[\"tokenizer\"] {\n    return this.model.tokenizer;\n  }\n\n  get contextWindowSize(): MODEL[\"contextWindowSize\"] {\n    return this.model.contextWindowSize;\n  }\n\n  get countPromptTokens(): MODEL[\"countPromptTokens\"] extends undefined\n    ? undefined\n    : (prompt: PROMPT) => PromiseLike<number> {\n    const originalCountPromptTokens = this.model.countPromptTokens?.bind(\n      this.model\n    );\n\n    if (originalCountPromptTokens === undefined) {\n      return undefined as MODEL[\"countPromptTokens\"] extends undefined\n        ? undefined\n        : (prompt: PROMPT) => PromiseLike<number>;\n    }\n\n    return ((prompt: PROMPT) =>\n      originalCountPromptTokens(\n        this.promptTemplate.format(prompt)\n      )) as MODEL[\"countPromptTokens\"] extends undefined\n      ? undefined\n      : (prompt: PROMPT) => PromiseLike<number>;\n  }\n\n  doGenerateTexts(prompt: PROMPT, options?: FunctionCallOptions) {\n    const mappedPrompt = this.promptTemplate.format(prompt);\n    return this.model.doGenerateTexts(mappedPrompt, options);\n  }\n\n  restoreGeneratedTexts(rawResponse: unknown) {\n    return this.model.restoreGeneratedTexts(rawResponse);\n  }\n\n  get settingsForEvent(): Partial<SETTINGS> {\n    return this.model.settingsForEvent;\n  }\n\n  asToolCallGenerationModel<INPUT_PROMPT>(\n    promptTemplate: ToolCallPromptTemplate<INPUT_PROMPT, PROMPT>\n  ) {\n    return new TextGenerationToolCallModel({\n      model: this,\n      template: promptTemplate,\n    });\n  }\n\n  asToolCallsOrTextGenerationModel<INPUT_PROMPT>(\n    promptTemplate: ToolCallsPromptTemplate<INPUT_PROMPT, PROMPT>\n  ) {\n    return new TextGenerationToolCallsModel({\n      model: this,\n      template: promptTemplate,\n    });\n  }\n\n  asObjectGenerationModel<INPUT_PROMPT>(\n    promptTemplate: ObjectFromTextPromptTemplate<INPUT_PROMPT, PROMPT>\n  ) {\n    return new ObjectFromTextGenerationModel({\n      model: this,\n      template: promptTemplate,\n    });\n  }\n\n  withJsonOutput(schema: Schema<unknown> & JsonSchemaProducer): this {\n    return new PromptTemplateTextGenerationModel({\n      model: this.model.withJsonOutput(schema),\n      promptTemplate: this.promptTemplate,\n    }) as this;\n  }\n\n  withSettings(additionalSettings: Partial<SETTINGS>): this {\n    return new PromptTemplateTextGenerationModel({\n      model: this.model.withSettings(additionalSettings),\n      promptTemplate: this.promptTemplate,\n    }) as this;\n  }\n}\n","import { FunctionCallOptions } from \"../../core/FunctionOptions\";\nimport { JsonSchemaProducer } from \"../../core/schema/JsonSchemaProducer\";\nimport { Schema } from \"../../core/schema/Schema\";\nimport { ObjectFromTextPromptTemplate } from \"../generate-object/ObjectFromTextPromptTemplate\";\nimport { ObjectFromTextStreamingModel } from \"../generate-object/ObjectFromTextStreamingModel\";\nimport { PromptTemplateTextGenerationModel } from \"./PromptTemplateTextGenerationModel\";\nimport {\n  TextGenerationModelSettings,\n  TextStreamingModel,\n} from \"./TextGenerationModel\";\nimport { TextGenerationPromptTemplate } from \"./TextGenerationPromptTemplate\";\n\nexport class PromptTemplateTextStreamingModel<\n    PROMPT,\n    MODEL_PROMPT,\n    SETTINGS extends TextGenerationModelSettings,\n    MODEL extends TextStreamingModel<MODEL_PROMPT, SETTINGS>,\n  >\n  extends PromptTemplateTextGenerationModel<\n    PROMPT,\n    MODEL_PROMPT,\n    SETTINGS,\n    MODEL\n  >\n  implements TextStreamingModel<PROMPT, SETTINGS>\n{\n  constructor(options: {\n    model: MODEL;\n    promptTemplate: TextGenerationPromptTemplate<PROMPT, MODEL_PROMPT>;\n  }) {\n    super(options);\n  }\n\n  doStreamText(prompt: PROMPT, options?: FunctionCallOptions) {\n    const mappedPrompt = this.promptTemplate.format(prompt);\n    return this.model.doStreamText(mappedPrompt, options);\n  }\n\n  extractTextDelta(delta: unknown) {\n    return this.model.extractTextDelta(delta);\n  }\n\n  asObjectGenerationModel<INPUT_PROMPT>(\n    promptTemplate: ObjectFromTextPromptTemplate<INPUT_PROMPT, PROMPT>\n  ) {\n    return new ObjectFromTextStreamingModel({\n      model: this,\n      template: promptTemplate,\n    });\n  }\n\n  withJsonOutput(schema: Schema<unknown> & JsonSchemaProducer): this {\n    return new PromptTemplateTextStreamingModel({\n      model: this.model.withJsonOutput(schema),\n      promptTemplate: this.promptTemplate,\n    }) as this;\n  }\n\n  withSettings(additionalSettings: Partial<SETTINGS>): this {\n    return new PromptTemplateTextStreamingModel({\n      model: this.model.withSettings(additionalSettings),\n      promptTemplate: this.promptTemplate,\n    }) as this;\n  }\n}\n","import { FunctionOptions } from \"../../core/FunctionOptions\";\nimport { ToolDefinition } from \"../../tool/ToolDefinition\";\nimport { ToolCallGenerationModel } from \"../../tool/generate-tool-call/ToolCallGenerationModel\";\nimport { ToolCallsGenerationModel } from \"../../tool/generate-tool-calls/ToolCallsGenerationModel\";\nimport { PromptTemplateTextStreamingModel } from \"./PromptTemplateTextStreamingModel\";\nimport {\n  TextGenerationModelSettings,\n  TextStreamingModel,\n} from \"./TextGenerationModel\";\nimport { TextGenerationPromptTemplate } from \"./TextGenerationPromptTemplate\";\n\nexport class PromptTemplateFullTextModel<\n    PROMPT,\n    MODEL_PROMPT,\n    SETTINGS extends TextGenerationModelSettings,\n    MODEL extends TextStreamingModel<MODEL_PROMPT, SETTINGS> &\n      ToolCallGenerationModel<MODEL_PROMPT, SETTINGS> &\n      ToolCallsGenerationModel<MODEL_PROMPT, SETTINGS>,\n  >\n  extends PromptTemplateTextStreamingModel<\n    PROMPT,\n    MODEL_PROMPT,\n    SETTINGS,\n    MODEL\n  >\n  implements\n    TextStreamingModel<PROMPT, SETTINGS>,\n    ToolCallGenerationModel<PROMPT, SETTINGS>,\n    ToolCallsGenerationModel<PROMPT, SETTINGS>\n{\n  constructor(options: {\n    model: MODEL;\n    promptTemplate: TextGenerationPromptTemplate<PROMPT, MODEL_PROMPT>;\n  }) {\n    super(options);\n  }\n\n  doGenerateToolCall(\n    tool: ToolDefinition<string, unknown>,\n    prompt: PROMPT,\n    options?: FunctionOptions | undefined\n  ): PromiseLike<{\n    rawResponse: unknown;\n    toolCall: { id: string; args: unknown } | null;\n    usage?:\n      | { promptTokens: number; completionTokens: number; totalTokens: number }\n      | undefined;\n  }> {\n    const mappedPrompt = this.promptTemplate.format(prompt);\n    return this.model.doGenerateToolCall(tool, mappedPrompt, options);\n  }\n\n  doGenerateToolCalls(\n    tools: ToolDefinition<string, unknown>[],\n    prompt: PROMPT,\n    options?: FunctionOptions | undefined\n  ): PromiseLike<{\n    rawResponse: unknown;\n    text: string | null;\n    toolCalls: { id: string; name: string; args: unknown }[] | null;\n    usage?:\n      | { promptTokens: number; completionTokens: number; totalTokens: number }\n      | undefined;\n  }> {\n    const mappedPrompt = this.promptTemplate.format(prompt);\n    return this.model.doGenerateToolCalls(tools, mappedPrompt, options);\n  }\n\n  withSettings(additionalSettings: Partial<SETTINGS>): this {\n    return new PromptTemplateFullTextModel({\n      model: this.model.withSettings(additionalSettings),\n      promptTemplate: this.promptTemplate,\n    }) as this;\n  }\n}\n","import { FunctionCallOptions } from \"../../core/FunctionOptions\";\nimport { JsonSchemaProducer } from \"../../core/schema/JsonSchemaProducer\";\nimport { Schema } from \"../../core/schema/Schema\";\nimport { Delta } from \"../Delta\";\nimport { Model, ModelSettings } from \"../Model\";\nimport { BasicTokenizer, FullTokenizer } from \"../tokenize-text/Tokenizer\";\nimport { TextGenerationPromptTemplate } from \"./TextGenerationPromptTemplate\";\nimport { TextGenerationResult } from \"./TextGenerationResult\";\nimport { ChatPrompt } from \"./prompt-template/ChatPrompt\";\nimport { InstructionPrompt } from \"./prompt-template/InstructionPrompt\";\n\nexport const textGenerationModelProperties = [\n  \"maxGenerationTokens\",\n  \"stopSequences\",\n  \"numberOfGenerations\",\n  \"trimWhitespace\",\n] as const;\n\nexport interface TextGenerationModelSettings extends ModelSettings {\n  /**\n   * Specifies the maximum number of tokens (words, punctuation, parts of words) that the model can generate in a single response.\n   * It helps to control the length of the output.\n   *\n   * Does nothing if the model does not support this setting.\n   *\n   * Example: `maxGenerationTokens: 1000`\n   */\n  maxGenerationTokens?: number | undefined;\n\n  /**\n   * Stop sequences to use.\n   * Stop sequences are an array of strings or a single string that the model will recognize as end-of-text indicators.\n   * The model stops generating more content when it encounters any of these strings.\n   * This is particularly useful in scripted or formatted text generation, where a specific end point is required.\n   * Stop sequences not included in the generated text.\n   *\n   * Does nothing if the model does not support this setting.\n   *\n   * Example: `stopSequences: ['\\n', 'END']`\n   */\n  stopSequences?: string[] | undefined;\n\n  /**\n   * Number of texts to generate.\n   *\n   * Specifies the number of responses or completions the model should generate for a given prompt.\n   * This is useful when you need multiple different outputs or ideas for a single prompt.\n   * The model will generate 'n' distinct responses, each based on the same initial prompt.\n   * In a streaming model this will result in both responses streamed back in real time.\n   *\n   * Does nothing if the model does not support this setting.\n   *\n   * Example: `numberOfGenerations: 3` // The model will produce 3 different responses.\n   */\n  numberOfGenerations?: number;\n\n  /**\n   * When true, the leading and trailing white space and line terminator characters\n   * are removed from the generated text.\n   *\n   * Default: true.\n   */\n  trimWhitespace?: boolean;\n}\n\nexport interface HasContextWindowSize {\n  contextWindowSize: number;\n}\n\nexport interface HasTokenizer<PROMPT> {\n  tokenizer: BasicTokenizer | FullTokenizer;\n\n  countPromptTokens(prompt: PROMPT): PromiseLike<number>;\n}\n\nexport interface TextGenerationModel<\n  PROMPT,\n  SETTINGS extends TextGenerationModelSettings = TextGenerationModelSettings,\n> extends Model<SETTINGS> {\n  readonly contextWindowSize: number | undefined;\n\n  readonly tokenizer: BasicTokenizer | FullTokenizer | undefined;\n\n  /**\n   * Optional. Implement if you have a tokenizer and want to count the number of tokens in a prompt.\n   */\n  readonly countPromptTokens:\n    | ((prompt: PROMPT) => PromiseLike<number>)\n    | undefined;\n\n  doGenerateTexts(\n    prompt: PROMPT,\n    options?: FunctionCallOptions\n  ): PromiseLike<{\n    rawResponse: unknown;\n    textGenerationResults: TextGenerationResult[];\n    usage?: {\n      promptTokens: number;\n      completionTokens: number;\n      totalTokens: number;\n    };\n  }>;\n\n  restoreGeneratedTexts(rawResponse: unknown): {\n    rawResponse: unknown;\n    textGenerationResults: TextGenerationResult[];\n    usage?: {\n      promptTokens: number;\n      completionTokens: number;\n      totalTokens: number;\n    };\n  };\n\n  /**\n   * When possible, limit the output generation to the specified JSON schema,\n   * or super sets of it (e.g. JSON in general).\n   */\n  withJsonOutput(schema: Schema<unknown> & JsonSchemaProducer): this;\n}\n\nexport interface TextGenerationBaseModel<\n  PROMPT,\n  SETTINGS extends TextGenerationModelSettings = TextGenerationModelSettings,\n> extends TextGenerationModel<PROMPT, SETTINGS> {\n  /**\n   * Returns this model with a text prompt template.\n   */\n  withTextPrompt(): TextGenerationModel<string, SETTINGS>;\n\n  /**\n   * Returns this model with an instruction prompt template.\n   */\n  withInstructionPrompt(): TextGenerationModel<InstructionPrompt, SETTINGS>;\n\n  /**\n   * Returns this model with a chat prompt template.\n   */\n  withChatPrompt(): TextGenerationModel<ChatPrompt, SETTINGS>;\n\n  withPromptTemplate<INPUT_PROMPT>(\n    promptTemplate: TextGenerationPromptTemplate<INPUT_PROMPT, PROMPT>\n  ): TextGenerationModel<INPUT_PROMPT, SETTINGS>;\n}\n\nexport interface TextStreamingModel<\n  PROMPT,\n  SETTINGS extends TextGenerationModelSettings = TextGenerationModelSettings,\n> extends TextGenerationModel<PROMPT, SETTINGS> {\n  doStreamText(\n    prompt: PROMPT,\n    options?: FunctionCallOptions\n  ): PromiseLike<AsyncIterable<Delta<unknown>>>;\n\n  extractTextDelta(delta: unknown): string | undefined;\n}\n\nexport interface TextStreamingBaseModel<\n  PROMPT,\n  SETTINGS extends TextGenerationModelSettings = TextGenerationModelSettings,\n> extends TextStreamingModel<PROMPT, SETTINGS> {\n  /**\n   * Returns this model with a text prompt template.\n   */\n  withTextPrompt(): TextStreamingModel<string, SETTINGS>;\n\n  /**\n   * Returns this model with an instruction prompt template.\n   */\n  withInstructionPrompt(): TextStreamingModel<InstructionPrompt, SETTINGS>;\n\n  /**\n   * Returns this model with a chat prompt template.\n   */\n  withChatPrompt(): TextStreamingModel<ChatPrompt, SETTINGS>;\n\n  withPromptTemplate<INPUT_PROMPT>(\n    promptTemplate: TextGenerationPromptTemplate<INPUT_PROMPT, PROMPT>\n  ): TextStreamingModel<INPUT_PROMPT, SETTINGS>;\n}\n","import { TextGenerationPromptTemplate } from \"../TextGenerationPromptTemplate\";\nimport { ChatPrompt } from \"./ChatPrompt\";\nimport { validateContentIsString } from \"./ContentPart\";\nimport { InstructionPrompt } from \"./InstructionPrompt\";\n\nconst DEFAULT_SYSTEM_PROMPT_INPUT =\n  \"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\";\nconst DEFAULT_SYSTEM_PROMPT_NO_INPUT =\n  \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\";\n\n/**\n * Formats a text prompt as an Alpaca prompt.\n */\nexport function text(): TextGenerationPromptTemplate<string, string> {\n  return {\n    stopSequences: [],\n    format(prompt) {\n      let text = DEFAULT_SYSTEM_PROMPT_NO_INPUT;\n      text += \"\\n\\n### Instruction:\\n\";\n      text += prompt;\n      text += \"\\n\\n### Response:\\n\";\n\n      return text;\n    },\n  };\n}\n\n/**\n * Formats an instruction prompt as an Alpaca prompt.\n *\n * If the instruction has a system prompt, it overrides the default system prompt\n * (which can impact the results, because the model may be trained on the default system prompt).\n *\n * Prompt template with input:\n * ```\n * Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n *\n * ### Instruction:\n *\n * {instruction}\n *\n * ### Input:\n *\n * {input}\n *\n * ### Response:\n *\n * ```\n *\n * Prompt template without input:\n * ```\n * Below is an instruction that describes a task. Write a response that appropriately completes the request.\n *\n * ### Instruction:\n *\n * {instruction}\n *\n * ### Response:\n *\n * ```\n *\n * @see https://github.com/tatsu-lab/stanford_alpaca#data-release\n */\nexport function instruction(): TextGenerationPromptTemplate<\n  InstructionPrompt & { input?: string }, // optional input supported by Alpaca\n  string\n> {\n  return {\n    stopSequences: [],\n    format(prompt) {\n      let text =\n        prompt.system ??\n        (prompt.input != null\n          ? DEFAULT_SYSTEM_PROMPT_INPUT\n          : DEFAULT_SYSTEM_PROMPT_NO_INPUT);\n\n      text += \"\\n\\n### Instruction:\\n\";\n\n      if (prompt.system != null) {\n        text += `${prompt.system}\\n`;\n      }\n\n      text += validateContentIsString(prompt.instruction, prompt);\n\n      if (prompt.input != null) {\n        text += `\\n\\n### Input:\\n${prompt.input}`;\n      }\n\n      text += \"\\n\\n### Response:\\n\";\n\n      if (prompt.responsePrefix != null) {\n        text += `${prompt.responsePrefix}\\n`;\n      }\n\n      return text;\n    },\n  };\n}\n\n/**\n * Not supported by Alpaca.\n */\nexport function chat(): TextGenerationPromptTemplate<ChatPrompt, string> {\n  throw new Error(\"Chat prompts are not supported by the Alpaca format.\");\n}\n","/**\n * Error thrown when a prompt validation fails.\n */\nexport class InvalidPromptError extends Error {\n  readonly prompt: unknown;\n\n  constructor(message: string, prompt: unknown) {\n    super(message);\n\n    this.name = \"InvalidPromptError\";\n    this.prompt = prompt;\n  }\n\n  toJSON() {\n    return {\n      name: this.name,\n      message: this.message,\n      stack: this.stack,\n\n      prompt: this.prompt,\n    };\n  }\n}\n","import { DataContent } from \"../../../util/format/DataContent\";\nimport { InvalidPromptError } from \"./InvalidPromptError\";\n\nexport interface TextPart {\n  type: \"text\";\n\n  /**\n   * The text content.\n   */\n  text: string;\n}\n\nexport interface ImagePart {\n  type: \"image\";\n\n  /**\n   * Image data. Can either be a base64-encoded string, a Uint8Array, an ArrayBuffer, or a Buffer.\n   */\n  image: DataContent;\n\n  /**\n   * Optional mime type of the image.\n   */\n  mimeType?: string;\n}\n\nexport interface ToolCallPart {\n  type: \"tool-call\";\n\n  id: string;\n  name: string;\n  args: unknown;\n}\n\nexport interface ToolResponsePart {\n  type: \"tool-response\";\n\n  id: string;\n  response: unknown;\n}\n\nexport function validateContentIsString(\n  content: string | unknown,\n  prompt: unknown\n): string {\n  if (typeof content !== \"string\") {\n    throw new InvalidPromptError(\n      \"Only text prompts are are supported by this prompt template.\",\n      prompt\n    );\n  }\n\n  return content;\n}\n","import { TextGenerationPromptTemplate } from \"../TextGenerationPromptTemplate\";\nimport { ChatPrompt } from \"./ChatPrompt\";\nimport { validateContentIsString } from \"./ContentPart\";\nimport { InstructionPrompt } from \"./InstructionPrompt\";\nimport { InvalidPromptError } from \"./InvalidPromptError\";\n\nconst START_SEGMENT = \"<|im_start|>\";\nconst END_SEGMENT = \"<|im_end|>\";\n\nfunction segmentStart(role: \"system\" | \"user\" | \"assistant\") {\n  return `${START_SEGMENT}${role}\\n`;\n}\n\nfunction segment(\n  role: \"system\" | \"user\" | \"assistant\",\n  text: string | undefined\n) {\n  return text == null ? \"\" : `${segmentStart(role)}${text}${END_SEGMENT}\\n`;\n}\n\n/**\n * Formats a text prompt using the ChatML format.\n */\nexport function text(): TextGenerationPromptTemplate<string, string> {\n  return {\n    stopSequences: [END_SEGMENT],\n    format(prompt) {\n      // prompt and then prefix start of assistant response:\n      return segment(\"user\", prompt) + segmentStart(\"assistant\");\n    },\n  };\n}\n\n/**\n * Formats an instruction prompt using the ChatML format.\n *\n * ChatML prompt template:\n * ```\n * <|im_start|>system\n * ${ system prompt }<|im_end|>\n * <|im_start|>user\n * ${ instruction }<|im_end|>\n * <|im_start|>assistant\n * ${response prefix}\n * ```\n */\nexport function instruction(): TextGenerationPromptTemplate<\n  InstructionPrompt,\n  string\n> {\n  return {\n    stopSequences: [END_SEGMENT],\n    format(prompt) {\n      const instruction = validateContentIsString(prompt.instruction, prompt);\n\n      return (\n        segment(\"system\", prompt.system) +\n        segment(\"user\", instruction) +\n        segmentStart(\"assistant\") +\n        (prompt.responsePrefix ?? \"\")\n      );\n    },\n  };\n}\n\n/**\n * Formats a chat prompt using the ChatML format.\n *\n * ChatML prompt template:\n * ```\n * <|im_start|>system\n * You are a helpful assistant that answers questions about the world.<|im_end|>\n * <|im_start|>user\n * What is the capital of France?<|im_end|>\n * <|im_start|>assistant\n * Paris<|im_end|>\n * ```\n */\nexport function chat(): TextGenerationPromptTemplate<ChatPrompt, string> {\n  return {\n    format(prompt) {\n      let text = prompt.system != null ? segment(\"system\", prompt.system) : \"\";\n\n      for (const { role, content } of prompt.messages) {\n        switch (role) {\n          case \"user\": {\n            text += segment(\"user\", validateContentIsString(content, prompt));\n            break;\n          }\n          case \"assistant\": {\n            text += segment(\n              \"assistant\",\n              validateContentIsString(content, prompt)\n            );\n            break;\n          }\n          case \"tool\": {\n            throw new InvalidPromptError(\n              \"Tool messages are not supported.\",\n              prompt\n            );\n          }\n          default: {\n            const _exhaustiveCheck: never = role;\n            throw new Error(`Unsupported role: ${_exhaustiveCheck}`);\n          }\n        }\n      }\n\n      // prefix start of assistant response:\n      text += segmentStart(\"assistant\");\n\n      return text;\n    },\n    stopSequences: [END_SEGMENT],\n  };\n}\n","import {\n  PromptFunction,\n  markAsPromptFunction,\n} from \"../../../core/PromptFunction\";\nimport { ToolCallResult } from \"../../../tool/ToolCallResult\";\nimport {\n  ImagePart,\n  TextPart,\n  ToolCallPart,\n  ToolResponsePart,\n} from \"./ContentPart\";\n\n/**\n * A chat prompt is a combination of a system message and a list\n * of user, assistant, and tool messages.\n *\n * The user messages can contain multi-modal content.\n * The assistant messages can contain tool calls.\n *\n * Note: Not all models and prompt formats support multi-modal inputs and tool calls.\n * The validation happens at runtime.\n *\n * @example\n * ```ts\n * const chatPrompt: ChatPrompt = {\n *   system: \"You are a celebrated poet.\",\n *   messages: [\n *    { role: \"user\", content: \"Write a short story about a robot learning to love.\" },\n *    { role: \"assistant\", content: \"Once upon a time, there was a robot who learned to love.\" },\n *    { role: \"user\", content: \"That's a great start!\" },\n *  ],\n * };\n * ```\n */\nexport interface ChatPrompt {\n  system?: string;\n  messages: Array<ChatMessage>;\n}\n\nexport type UserContent = string | Array<TextPart | ImagePart>;\nexport type AssistantContent = string | Array<TextPart | ToolCallPart>;\nexport type ToolContent = Array<ToolResponsePart>;\n\n/**\n * A message in a chat prompt.\n *\n * @see ChatPrompt\n */\nexport type ChatMessage =\n  | { role: \"user\"; content: UserContent }\n  | { role: \"assistant\"; content: AssistantContent }\n  | { role: \"tool\"; content: ToolContent };\n\nexport const ChatMessage = {\n  user({ text }: { text: string }): ChatMessage {\n    return {\n      role: \"user\" as const,\n      content: text,\n    };\n  },\n\n  tool({\n    toolResults,\n  }: {\n    toolResults: ToolCallResult<string, unknown, unknown>[] | null;\n  }): ChatMessage {\n    return {\n      role: \"tool\" as const,\n      content: createToolContent({ toolResults }),\n    };\n  },\n\n  assistant({\n    text,\n    toolResults,\n  }: {\n    text: string | null;\n    toolResults: ToolCallResult<string, unknown, unknown>[] | null;\n  }): ChatMessage {\n    return {\n      role: \"assistant\" as const,\n      content: createAssistantContent({ text, toolResults }),\n    };\n  },\n};\n\nfunction createToolContent({\n  toolResults,\n}: {\n  toolResults: ToolCallResult<string, unknown, unknown>[] | null;\n}) {\n  const toolContent: ToolContent = [];\n\n  for (const { result, toolCall } of toolResults ?? []) {\n    toolContent.push({\n      type: \"tool-response\",\n      id: toolCall.id,\n      response: result,\n    });\n  }\n\n  return toolContent;\n}\n\nfunction createAssistantContent({\n  text,\n  toolResults,\n}: {\n  text: string | null;\n  toolResults: ToolCallResult<string, unknown, unknown>[] | null;\n}) {\n  const content: AssistantContent = [];\n\n  if (text != null) {\n    content.push({ type: \"text\", text });\n  }\n\n  for (const { toolCall } of toolResults ?? []) {\n    content.push({ type: \"tool-call\", ...toolCall });\n  }\n\n  return content;\n}\n\nexport function createChatPrompt<INPUT>(\n  promptFunction: (input: INPUT) => Promise<ChatPrompt>\n): (input: INPUT) => PromptFunction<INPUT, ChatPrompt> {\n  return (input: INPUT) =>\n    markAsPromptFunction(async () => ({\n      input,\n      prompt: await promptFunction(input),\n    }));\n}\n","import {\n  PromptFunction,\n  markAsPromptFunction,\n} from \"../../../core/PromptFunction\";\nimport { ImagePart, TextPart } from \"./ContentPart\";\n\n/**\n * A single text instruction prompt. It can contain an optional system message to define\n * the role and behavior of the language model.\n *\n * The instruction can be a text instruction or a multi-modal instruction.\n *\n * @example\n * ```ts\n * {\n *   system: \"You are a celebrated poet.\", // optional\n *   instruction: \"Write a story about a robot learning to love\",\n * }\n * ```\n */\nexport interface InstructionPrompt {\n  /**\n   * Optional system message to provide context for the language model. Note that for some models,\n   * changing the system message can impact the results, because the model may be trained on the default system message.\n   */\n  system?: string;\n\n  /**\n   * The instruction for the model.\n   */\n  instruction: InstructionContent;\n\n  /**\n   * Response prefix that will be injected in the prompt at the beginning of the response.\n   * This is useful for guiding the model by starting its response with a specific text.\n   */\n  responsePrefix?: string;\n}\n\nexport type InstructionContent = string | Array<TextPart | ImagePart>;\n\nexport function createInstructionPrompt<INPUT>(\n  promptFunction: (input: INPUT) => Promise<InstructionPrompt>\n): (input: INPUT) => PromptFunction<INPUT, InstructionPrompt> {\n  return (input: INPUT) =>\n    markAsPromptFunction(async () => ({\n      input,\n      prompt: await promptFunction(input),\n    }));\n}\n","import { TextGenerationPromptTemplate } from \"../TextGenerationPromptTemplate\";\nimport { ChatPrompt } from \"./ChatPrompt\";\nimport { validateContentIsString } from \"./ContentPart\";\nimport { InstructionPrompt } from \"./InstructionPrompt\";\nimport { InvalidPromptError } from \"./InvalidPromptError\";\n\n// see https://github.com/facebookresearch/llama/blob/6c7fe276574e78057f917549435a2554000a876d/llama/generation.py#L44\nconst BEGIN_SEGMENT = \"<s>\";\nconst END_SEGMENT = \" </s>\";\nconst BEGIN_INSTRUCTION = \"[INST] \";\nconst END_INSTRUCTION = \" [/INST] \";\nconst BEGIN_SYSTEM = \"<<SYS>>\\n\";\nconst END_SYSTEM = \"\\n<</SYS>>\\n\\n\";\n\n/**\n * Formats a text prompt as a Llama 2 prompt.\n *\n * Llama 2 prompt template:\n * ```\n * <s>[INST] { instruction } [/INST]\n * ```\n *\n * @see https://www.philschmid.de/llama-2#how-to-prompt-llama-2-chat\n */\nexport function text(): TextGenerationPromptTemplate<string, string> {\n  return {\n    stopSequences: [END_SEGMENT],\n    format(prompt) {\n      return `${BEGIN_SEGMENT}${BEGIN_INSTRUCTION}${prompt}${END_INSTRUCTION}`;\n    },\n  };\n}\n\n/**\n * Formats an instruction prompt as a Llama 2 prompt.\n *\n * Llama 2 prompt template:\n * ```\n * <s>[INST] <<SYS>>\n * ${ system prompt }\n * <</SYS>>\n * ${ instruction }\n * [/INST]\n * ${ response prefix }\n * ```\n *\n * @see https://www.philschmid.de/llama-2#how-to-prompt-llama-2-chat\n */\nexport function instruction(): TextGenerationPromptTemplate<\n  InstructionPrompt,\n  string\n> {\n  return {\n    stopSequences: [END_SEGMENT],\n    format(prompt) {\n      const instruction = validateContentIsString(prompt.instruction, prompt);\n\n      return `${BEGIN_SEGMENT}${BEGIN_INSTRUCTION}${\n        prompt.system != null\n          ? `${BEGIN_SYSTEM}${prompt.system}${END_SYSTEM}`\n          : \"\"\n      }${instruction}${END_INSTRUCTION}${prompt.responsePrefix ?? \"\"}`;\n    },\n  };\n}\n\n/**\n * Formats a chat prompt as a Llama 2 prompt.\n *\n * Llama 2 prompt template:\n * ```\n * <s>[INST] <<SYS>>\n * ${ system prompt }\n * <</SYS>>\n *\n * ${ user msg 1 } [/INST] ${ model response 1 } </s><s>[INST] ${ user msg 2 } [/INST] ${ model response 2 } </s><s>[INST] ${ user msg 3 } [/INST]\n * ```\n */\nexport function chat(): TextGenerationPromptTemplate<ChatPrompt, string> {\n  return {\n    format(prompt) {\n      validateLlama2Prompt(prompt);\n\n      // get content of the first message (validated to be a user message)\n      const content = prompt.messages[0].content;\n\n      let text = `${BEGIN_SEGMENT}${BEGIN_INSTRUCTION}${\n        prompt.system != null\n          ? `${BEGIN_SYSTEM}${prompt.system}${END_SYSTEM}`\n          : \"\"\n      }${content}${END_INSTRUCTION}`;\n\n      // process remaining messages\n      for (let i = 1; i < prompt.messages.length; i++) {\n        const { role, content } = prompt.messages[i];\n        switch (role) {\n          case \"user\": {\n            const textContent = validateContentIsString(content, prompt);\n            text += `${BEGIN_SEGMENT}${BEGIN_INSTRUCTION}${textContent}${END_INSTRUCTION}`;\n            break;\n          }\n          case \"assistant\": {\n            text += `${validateContentIsString(content, prompt)}${END_SEGMENT}`;\n            break;\n          }\n          case \"tool\": {\n            throw new InvalidPromptError(\n              \"Tool messages are not supported.\",\n              prompt\n            );\n          }\n          default: {\n            const _exhaustiveCheck: never = role;\n            throw new Error(`Unsupported role: ${_exhaustiveCheck}`);\n          }\n        }\n      }\n\n      return text;\n    },\n    stopSequences: [END_SEGMENT],\n  };\n}\n\n/**\n * Checks if a Llama2 chat prompt is valid. Throws a {@link ChatPromptValidationError} if it's not.\n *\n * - The first message of the chat must be a user message.\n * - Then it must be alternating between an assistant message and a user message.\n * - The last message must always be a user message (when submitting to a model).\n *\n * The type checking is done at runtime when you submit a chat prompt to a model with a prompt template.\n *\n * @throws {@link ChatPromptValidationError}\n */\nexport function validateLlama2Prompt(chatPrompt: ChatPrompt) {\n  const messages = chatPrompt.messages;\n\n  if (messages.length < 1) {\n    throw new InvalidPromptError(\n      \"ChatPrompt should have at least one message.\",\n      chatPrompt\n    );\n  }\n\n  for (let i = 0; i < messages.length; i++) {\n    const expectedRole = i % 2 === 0 ? \"user\" : \"assistant\";\n    const role = messages[i].role;\n\n    if (role !== expectedRole) {\n      throw new InvalidPromptError(\n        `Message at index ${i} should have role '${expectedRole}', but has role '${role}'.`,\n        chatPrompt\n      );\n    }\n  }\n\n  if (messages.length % 2 === 0) {\n    throw new InvalidPromptError(\n      \"The last message must be a user message.\",\n      chatPrompt\n    );\n  }\n}\n","import { TextGenerationPromptTemplate } from \"../TextGenerationPromptTemplate\";\nimport { ChatPrompt } from \"./ChatPrompt\";\nimport { validateContentIsString } from \"./ContentPart\";\nimport { InstructionPrompt } from \"./InstructionPrompt\";\nimport { InvalidPromptError } from \"./InvalidPromptError\";\n\nconst BEGIN_SEGMENT = \"<s>\";\nconst END_SEGMENT = \"</s>\";\nconst BEGIN_INSTRUCTION = \"[INST] \";\nconst END_INSTRUCTION = \" [/INST] \";\n/**\n * Formats a text prompt as a Mistral instruct prompt.\n *\n * Mistral prompt template:\n * ```\n * <s>[INST] { instruction } [/INST]\n * ```\n *\n * @see https://docs.mistral.ai/models/#chat-template\n */\nexport function text(): TextGenerationPromptTemplate<string, string> {\n  return {\n    stopSequences: [END_SEGMENT],\n    format(prompt) {\n      return `${BEGIN_SEGMENT}${BEGIN_INSTRUCTION}${prompt}${END_INSTRUCTION}`;\n    },\n  };\n}\n\n/**\n * Formats an instruction prompt as a Mistral instruct prompt.\n *\n * Note that Mistral does not support system prompts. We emulate them.\n *\n * Mistral prompt template when system prompt is set:\n * ```\n * <s>[INST] ${ system prompt } [/INST] </s>[INST] ${instruction} [/INST] ${ response prefix }\n * ```\n *\n * Mistral prompt template when there is no system prompt:\n * ```\n * <s>[INST] ${ instruction } [/INST] ${ response prefix }\n * ```\n *\n * @see https://docs.mistral.ai/models/#chat-template\n */\nexport function instruction(): TextGenerationPromptTemplate<\n  InstructionPrompt,\n  string\n> {\n  return {\n    stopSequences: [END_SEGMENT],\n    format(prompt) {\n      const instruction = validateContentIsString(prompt.instruction, prompt);\n\n      if (prompt.system != null) {\n        return `${BEGIN_SEGMENT}${BEGIN_INSTRUCTION}${\n          prompt.system\n        }${END_INSTRUCTION}${END_SEGMENT}${BEGIN_INSTRUCTION}${instruction}${END_INSTRUCTION}${\n          prompt.responsePrefix ?? \"\"\n        }`;\n      }\n\n      return `${BEGIN_SEGMENT}${BEGIN_INSTRUCTION}${instruction}${END_INSTRUCTION}${\n        prompt.responsePrefix ?? \"\"\n      }`;\n    },\n  };\n}\n\n/**\n * Formats a chat prompt as a Mistral instruct prompt.\n *\n * Note that Mistral does not support system prompts. We emulate them.\n *\n * Mistral prompt template when system prompt is set:\n * ```\n * <s>[INST] ${ system prompt } [/INST] </s> [INST] ${ user msg 1 } [/INST] ${ model response 1 } [INST] ${ user msg 2 } [/INST] ${ model response 2 } [INST] ${ user msg 3 } [/INST]\n * ```\n *\n * Mistral prompt template when there is no system prompt:\n * ```\n * <s>[INST] ${ user msg 1 } [/INST] ${ model response 1 } </s>[INST] ${ user msg 2 } [/INST] ${ model response 2 } [INST] ${ user msg 3 } [/INST]\n * ```\n *\n * @see https://docs.mistral.ai/models/#chat-template\n */\nexport function chat(): TextGenerationPromptTemplate<ChatPrompt, string> {\n  return {\n    format(prompt) {\n      validateMistralPrompt(prompt);\n\n      let text = \"\";\n      let i = 0;\n\n      // handle the special first segment\n      if (prompt.system != null) {\n        text += `${BEGIN_SEGMENT}${BEGIN_INSTRUCTION}${prompt.system}${END_INSTRUCTION}${END_SEGMENT}`;\n      } else {\n        // get content of the first message (validated to be a user message)\n        text = `${BEGIN_SEGMENT}${BEGIN_INSTRUCTION}${prompt.messages[0].content}${END_INSTRUCTION}`;\n\n        // process 2nd message (validated to be an assistant message)\n        if (prompt.messages.length > 1) {\n          text += `${prompt.messages[1].content}${END_SEGMENT}`;\n        }\n\n        i = 2;\n      }\n\n      // process remaining messages\n      for (; i < prompt.messages.length; i++) {\n        const { role, content } = prompt.messages[i];\n        switch (role) {\n          case \"user\": {\n            const textContent = validateContentIsString(content, prompt);\n            text += `${BEGIN_INSTRUCTION}${textContent}${END_INSTRUCTION}`;\n            break;\n          }\n          case \"assistant\": {\n            text += validateContentIsString(content, prompt);\n            break;\n          }\n          case \"tool\": {\n            throw new InvalidPromptError(\n              \"Tool messages are not supported.\",\n              prompt\n            );\n          }\n          default: {\n            const _exhaustiveCheck: never = role;\n            throw new Error(`Unsupported role: ${_exhaustiveCheck}`);\n          }\n        }\n      }\n\n      return text;\n    },\n    stopSequences: [END_SEGMENT],\n  };\n}\n\n/**\n * Checks if a Mistral chat prompt is valid. Throws a {@link ChatPromptValidationError} if it's not.\n *\n * - The first message of the chat must be a user message.\n * - Then it must be alternating between an assistant message and a user message.\n * - The last message must always be a user message (when submitting to a model).\n *\n * The type checking is done at runtime when you submit a chat prompt to a model with a prompt template.\n *\n * @throws {@link ChatPromptValidationError}\n */\nexport function validateMistralPrompt(chatPrompt: ChatPrompt) {\n  const messages = chatPrompt.messages;\n\n  if (messages.length < 1) {\n    throw new InvalidPromptError(\n      \"ChatPrompt should have at least one message.\",\n      chatPrompt\n    );\n  }\n\n  for (let i = 0; i < messages.length; i++) {\n    const expectedRole = i % 2 === 0 ? \"user\" : \"assistant\";\n    const role = messages[i].role;\n\n    if (role !== expectedRole) {\n      throw new InvalidPromptError(\n        `Message at index ${i} should have role '${expectedRole}', but has role '${role}'.`,\n        chatPrompt\n      );\n    }\n  }\n\n  if (messages.length % 2 === 0) {\n    throw new InvalidPromptError(\n      \"The last message must be a user message.\",\n      chatPrompt\n    );\n  }\n}\n","import { TextGenerationPromptTemplate } from \"../TextGenerationPromptTemplate\";\nimport { ChatPrompt } from \"./ChatPrompt\";\nimport { validateContentIsString } from \"./ContentPart\";\nimport { InstructionPrompt } from \"./InstructionPrompt\";\nimport { InvalidPromptError } from \"./InvalidPromptError\";\n\nconst roleNames = {\n  system: \"System\",\n  user: \"User\",\n  assistant: \"Assistant\",\n};\n\nfunction segmentStart(role: \"system\" | \"user\" | \"assistant\") {\n  return `### ${roleNames[role]}:\\n`;\n}\n\nfunction segment(\n  role: \"system\" | \"user\" | \"assistant\",\n  text: string | undefined\n) {\n  return text == null ? \"\" : `${segmentStart(role)}${text}\\n`;\n}\n\n/**\n * Formats a text prompt as a neural chat prompt.\n *\n * @see https://huggingface.co/Intel/neural-chat-7b-v3-1#prompt-template\n */\nexport function text(): TextGenerationPromptTemplate<string, string> {\n  return {\n    stopSequences: [],\n    format(prompt) {\n      // prompt and then prefix start of assistant response:\n      return segment(\"user\", prompt) + segmentStart(\"assistant\");\n    },\n  };\n}\n\n/**\n * Formats an instruction prompt as a neural chat prompt.\n *\n * @see https://huggingface.co/Intel/neural-chat-7b-v3-1#prompt-template\n */\nexport const instruction: () => TextGenerationPromptTemplate<\n  InstructionPrompt,\n  string\n> = () => ({\n  stopSequences: [],\n  format(prompt) {\n    const instruction = validateContentIsString(prompt.instruction, prompt);\n\n    return (\n      segment(\"system\", prompt.system) +\n      segment(\"user\", instruction) +\n      segmentStart(\"assistant\") +\n      (prompt.responsePrefix ?? \"\")\n    );\n  },\n});\n\n/**\n * Formats a chat prompt as a basic text prompt.\n *\n * @param user The label of the user in the chat. Default to \"user\".\n * @param assistant The label of the assistant in the chat. Default to \"assistant\".\n * @param system The label of the system in the chat. Optional, defaults to no prefix.\n */\nexport function chat(): TextGenerationPromptTemplate<ChatPrompt, string> {\n  return {\n    format(prompt) {\n      let text = prompt.system != null ? segment(\"system\", prompt.system) : \"\";\n\n      for (const { role, content } of prompt.messages) {\n        switch (role) {\n          case \"user\": {\n            const textContent = validateContentIsString(content, prompt);\n            text += segment(\"user\", textContent);\n            break;\n          }\n          case \"assistant\": {\n            text += segment(\n              \"assistant\",\n              validateContentIsString(content, prompt)\n            );\n            break;\n          }\n          case \"tool\": {\n            throw new InvalidPromptError(\n              \"Tool messages are not supported.\",\n              prompt\n            );\n          }\n          default: {\n            const _exhaustiveCheck: never = role;\n            throw new Error(`Unsupported role: ${_exhaustiveCheck}`);\n          }\n        }\n      }\n\n      // prefix start of assistant response:\n      text += segmentStart(\"assistant\");\n\n      return text;\n    },\n    stopSequences: [`\\n${roleNames.user}:`],\n  };\n}\n","import { TextGenerationPromptTemplate } from \"../TextGenerationPromptTemplate\";\nimport { ChatPrompt } from \"./ChatPrompt\";\nimport { validateContentIsString } from \"./ContentPart\";\nimport { InstructionPrompt } from \"./InstructionPrompt\";\nimport { InvalidPromptError } from \"./InvalidPromptError\";\n\n/**\n * Formats a text prompt as a Synthia text prompt.\n *\n * Synthia prompt template:\n * ```\n * USER: text\n * ASSISTANT:\n * ```\n */\nexport const text: () => TextGenerationPromptTemplate<string, string> = () => ({\n  stopSequences: [],\n  format: (prompt) => `USER: ${prompt}\\nASSISTANT: `,\n});\n\n/**\n * Formats an instruction prompt as a Synthia prompt.\n *\n * Synthia prompt template:\n * ```\n * SYSTEM: system message\n * USER: instruction\n * ASSISTANT: response prefix\n * ```\n */\nexport const instruction = (): TextGenerationPromptTemplate<\n  InstructionPrompt,\n  string\n> => ({\n  stopSequences: [`\\nUSER:`],\n  format(prompt) {\n    let text = prompt.system != null ? `SYSTEM: ${prompt.system}\\n` : \"\";\n\n    text += `USER: ${validateContentIsString(prompt.instruction, prompt)}\\n`;\n    text += `ASSISTANT: ${prompt.responsePrefix ?? \"\"}`;\n\n    return text;\n  },\n});\n\n/**\n * Formats a chat prompt as a Synthia prompt.\n *\n * Synthia prompt template:\n * ```\n * SYSTEM: system message\n * USER: user message\n * ASSISTANT: assistant message\n * ```\n */\nexport const chat: () => TextGenerationPromptTemplate<\n  ChatPrompt,\n  string\n> = () => ({\n  format(prompt) {\n    let text = prompt.system != null ? `SYSTEM: ${prompt.system}\\n` : \"\";\n\n    for (const { role, content } of prompt.messages) {\n      switch (role) {\n        case \"user\": {\n          text += `USER: ${validateContentIsString(content, prompt)}\\n`;\n          break;\n        }\n        case \"assistant\": {\n          text += `ASSISTANT: ${validateContentIsString(content, prompt)}\\n`;\n          break;\n        }\n        case \"tool\": {\n          throw new InvalidPromptError(\n            \"Tool messages are not supported.\",\n            prompt\n          );\n        }\n        default: {\n          const _exhaustiveCheck: never = role;\n          throw new Error(`Unsupported role: ${_exhaustiveCheck}`);\n        }\n      }\n    }\n\n    // Assistant message prefix:\n    text += `ASSISTANT: `;\n\n    return text;\n  },\n  stopSequences: [`\\nUSER:`],\n});\n","import {\n  PromptFunction,\n  markAsPromptFunction,\n} from \"../../../core/PromptFunction\";\n\nexport function createTextPrompt<INPUT>(\n  promptFunction: (input: INPUT) => Promise<string>\n): (input: INPUT) => PromptFunction<INPUT, string> {\n  return (input: INPUT) =>\n    markAsPromptFunction(async () => ({\n      input,\n      prompt: await promptFunction(input),\n    }));\n}\n","import { TextGenerationPromptTemplate } from \"../TextGenerationPromptTemplate\";\nimport { ChatPrompt } from \"./ChatPrompt\";\nimport { validateContentIsString } from \"./ContentPart\";\nimport { InstructionPrompt } from \"./InstructionPrompt\";\nimport { InvalidPromptError } from \"./InvalidPromptError\";\n\n/**\n * Formats a text prompt as a basic text prompt. Does not change the text prompt in any way.\n */\nexport const text: () => TextGenerationPromptTemplate<string, string> = () => ({\n  stopSequences: [],\n  format: (prompt) => prompt,\n});\n\n/**\n * Formats an instruction prompt as a basic text prompt.\n */\nexport const instruction: () => TextGenerationPromptTemplate<\n  InstructionPrompt,\n  string\n> = () => ({\n  stopSequences: [],\n  format(prompt) {\n    let text = \"\";\n\n    if (prompt.system != null) {\n      text += `${prompt.system}\\n\\n`;\n    }\n\n    text += `${validateContentIsString(prompt.instruction, prompt)}\\n\\n`;\n\n    if (prompt.responsePrefix != null) {\n      text += prompt.responsePrefix;\n    }\n\n    return text;\n  },\n});\n\n/**\n * Formats a chat prompt as a basic text prompt.\n *\n * @param user The label of the user in the chat. Default to \"user\".\n * @param assistant The label of the assistant in the chat. Default to \"assistant\".\n * @param system The label of the system in the chat. Optional, defaults to no prefix.\n */\nexport const chat: (options?: {\n  user?: string;\n  assistant?: string;\n  system?: string;\n}) => TextGenerationPromptTemplate<ChatPrompt, string> = ({\n  user = \"user\",\n  assistant = \"assistant\",\n  system,\n} = {}) => ({\n  format(prompt) {\n    let text =\n      prompt.system != null\n        ? `${system != null ? `${system}:` : \"\"}${prompt.system}\\n\\n`\n        : \"\";\n\n    for (const { role, content } of prompt.messages) {\n      switch (role) {\n        case \"user\": {\n          text += `${user}:\\n${validateContentIsString(content, prompt)}\\n\\n`;\n          break;\n        }\n        case \"assistant\": {\n          text += `${assistant}:\\n${validateContentIsString(\n            content,\n            prompt\n          )}\\n\\n`;\n          break;\n        }\n        case \"tool\": {\n          throw new InvalidPromptError(\n            \"Tool messages are not supported.\",\n            prompt\n          );\n        }\n        default: {\n          const _exhaustiveCheck: never = role;\n          throw new Error(`Unsupported role: ${_exhaustiveCheck}`);\n        }\n      }\n    }\n\n    // Assistant message prefix:\n    text += `${assistant}:\\n`;\n\n    return text;\n  },\n  stopSequences: [`\\n${user}:`],\n});\n","import { TextGenerationPromptTemplate } from \"../TextGenerationPromptTemplate\";\nimport { ChatPrompt } from \"./ChatPrompt\";\nimport { validateContentIsString } from \"./ContentPart\";\nimport { InstructionPrompt } from \"./InstructionPrompt\";\nimport { InvalidPromptError } from \"./InvalidPromptError\";\n\n// default Vicuna 1 system message\nconst DEFAULT_SYSTEM_MESSAGE =\n  \"A chat between a curious user and an artificial intelligence assistant. \" +\n  \"The assistant gives helpful, detailed, and polite answers to the user's questions.\";\n\n/**\n * Formats a text prompt as a Vicuna prompt.\n */\nexport function text(): TextGenerationPromptTemplate<string, string> {\n  return {\n    stopSequences: [],\n    format(prompt) {\n      let text = DEFAULT_SYSTEM_MESSAGE;\n      text += \"\\n\\nUSER: \";\n      text += prompt;\n      text += \"\\n\\nASSISTANT: \";\n      return text;\n    },\n  };\n}\n\n/**\n * Formats an instruction prompt as a Vicuna prompt.\n */\nexport const instruction = (): TextGenerationPromptTemplate<\n  InstructionPrompt,\n  string\n> => ({\n  stopSequences: [`\\nUSER:`],\n  format(prompt) {\n    let text =\n      prompt.system != null\n        ? `${prompt.system}\\n\\n`\n        : `${DEFAULT_SYSTEM_MESSAGE}\\n\\n`;\n\n    text += `USER: ${validateContentIsString(prompt.instruction, prompt)}\\n`;\n    text += `ASSISTANT: `;\n\n    return text;\n  },\n});\n\n/**\n * Formats a chat prompt as a Vicuna prompt.\n *\n * Overriding the system message in the first chat message can affect model responses.\n *\n * Vicuna prompt template:\n * ```\n * A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.\n *\n * USER: {prompt}\n * ASSISTANT:\n * ```\n */\nexport function chat(): TextGenerationPromptTemplate<ChatPrompt, string> {\n  return {\n    format(prompt) {\n      let text =\n        prompt.system != null\n          ? `${prompt.system}\\n\\n`\n          : `${DEFAULT_SYSTEM_MESSAGE}\\n\\n`;\n\n      for (const { role, content } of prompt.messages) {\n        switch (role) {\n          case \"user\": {\n            const textContent = validateContentIsString(content, prompt);\n            text += `USER: ${textContent}\\n`;\n            break;\n          }\n          case \"assistant\": {\n            text += `ASSISTANT: ${validateContentIsString(content, prompt)}\\n`;\n            break;\n          }\n          case \"tool\": {\n            throw new InvalidPromptError(\n              \"Tool messages are not supported.\",\n              prompt\n            );\n          }\n          default: {\n            const _exhaustiveCheck: never = role;\n            throw new Error(`Unsupported role: ${_exhaustiveCheck}`);\n          }\n        }\n      }\n\n      // AI message prefix:\n      text += `ASSISTANT: `;\n\n      return text;\n    },\n    stopSequences: [`\\nUSER:`],\n  };\n}\n","import {\n  HasContextWindowSize,\n  HasTokenizer,\n  TextGenerationModel,\n  TextGenerationModelSettings,\n} from \"../TextGenerationModel\";\nimport { ChatPrompt } from \"./ChatPrompt\";\n\n/**\n * Keeps only the most recent messages in the prompt, while leaving enough space for the completion.\n *\n * It will remove user-ai message pairs that don't fit. The result is always a valid chat prompt.\n *\n * When the minimal chat prompt (system message + last user message) is already too long, it will only\n * return this minimal chat prompt.\n *\n * @see https://modelfusion.dev/guide/function/generate-text#limiting-the-chat-length\n */\nexport async function trimChatPrompt({\n  prompt,\n  model,\n  tokenLimit = model.contextWindowSize -\n    (model.settings.maxGenerationTokens ?? model.contextWindowSize / 4),\n}: {\n  prompt: ChatPrompt;\n  model: TextGenerationModel<ChatPrompt, TextGenerationModelSettings> &\n    HasTokenizer<ChatPrompt> &\n    HasContextWindowSize;\n  tokenLimit?: number;\n}): Promise<ChatPrompt> {\n  let minimalPrompt = {\n    system: prompt.system,\n    messages: [prompt.messages[prompt.messages.length - 1]], // last user message\n  };\n\n  // check if the minimal prompt is already too long\n  const promptTokenCount = await model.countPromptTokens(minimalPrompt);\n\n  // the minimal chat prompt is already over the token limit and cannot be trimmed further:\n  if (promptTokenCount > tokenLimit) {\n    return minimalPrompt;\n  }\n\n  // inner messages\n  const innerMessages = prompt.messages.slice(0, -1);\n\n  // taking always a pair of user-message and ai-message from the end, moving backwards\n  for (let i = innerMessages.length - 1; i >= 0; i -= 2) {\n    const assistantMessage = innerMessages[i];\n    const userMessage = innerMessages[i - 1];\n\n    // create a temporary prompt and check if it fits within the token limit\n    const attemptedPrompt = {\n      system: prompt.system,\n      messages: [userMessage, assistantMessage, ...minimalPrompt.messages],\n    };\n    const tokenCount = await model.countPromptTokens(attemptedPrompt);\n\n    if (tokenCount > tokenLimit) {\n      break;\n    }\n\n    // if it fits, its the new minimal prompt\n    minimalPrompt = attemptedPrompt;\n  }\n\n  return minimalPrompt;\n}\n","import { FunctionOptions } from \"../../core/FunctionOptions\";\nimport { AudioMimeType } from \"../../util/audio/AudioMimeType\";\nimport { DataContent } from \"../../util/format/DataContent\";\nimport { ModelCallMetadata } from \"../ModelCallMetadata\";\nimport { executeStandardCall } from \"../executeStandardCall\";\nimport {\n  TranscriptionModel,\n  TranscriptionModelSettings,\n} from \"./TranscriptionModel\";\n\n/**\n * Transcribe audio data into text. Also called speech-to-text (STT) or automatic speech recognition (ASR).\n *\n * @see https://modelfusion.dev/guide/function/generate-transcription\n *\n * @example\n * const audioData = await fs.promises.readFile(\"data/test.mp3\");\n *\n * const transcription = await generateTranscription({\n *   model: openai.Transcriber({ model: \"whisper-1\" }),\n *   mimeType: \"audio/mp3\",\n *   audioData,\n * });\n *\n * @param {TranscriptionModel<DATA, TranscriptionModelSettings>} options.model - The model to use for transcription.\n * @param {AudioMimeType} options.model - The MIME type of the audio data.\n * @param {DataContent} options.model - The audio data to transcribe. Can be a base64-encoded string, a Uint8Array, or a Buffer.\n *\n * @returns {Promise<string>} A promise that resolves to the transcribed text.\n */\nexport async function generateTranscription(\n  args: {\n    model: TranscriptionModel<TranscriptionModelSettings>;\n    mimeType: AudioMimeType | (string & {}); // eslint-disable-line @typescript-eslint/ban-types\n    audioData: DataContent;\n    fullResponse?: false;\n  } & FunctionOptions\n): Promise<string>;\nexport async function generateTranscription(\n  args: {\n    model: TranscriptionModel<TranscriptionModelSettings>;\n    mimeType: AudioMimeType | (string & {}); // eslint-disable-line @typescript-eslint/ban-types\n    audioData: DataContent;\n    fullResponse: true;\n  } & FunctionOptions\n): Promise<{\n  value: string;\n  rawResponse: unknown;\n  metadata: ModelCallMetadata;\n}>;\nexport async function generateTranscription({\n  model,\n  audioData,\n  mimeType,\n  fullResponse,\n  ...options\n}: {\n  model: TranscriptionModel<TranscriptionModelSettings>;\n  mimeType: AudioMimeType | (string & {}); // eslint-disable-line @typescript-eslint/ban-types\n  audioData: DataContent;\n  fullResponse?: boolean;\n} & FunctionOptions): Promise<\n  string | { value: string; rawResponse: unknown; metadata: ModelCallMetadata }\n> {\n  const input = { mimeType, audioData };\n\n  const callResponse = await executeStandardCall({\n    functionType: \"generate-transcription\",\n    input,\n    model,\n    options,\n    generateResponse: async (options) => {\n      const result = await model.doTranscribe(input, options);\n      return {\n        rawResponse: result.rawResponse,\n        extractedValue: result.transcription,\n      };\n    },\n  });\n\n  return fullResponse ? callResponse : callResponse.value;\n}\n","import { BasicTokenizer } from \"./Tokenizer\";\n\n/**\n * Count the number of tokens in the given text.\n */\nexport async function countTokens(tokenizer: BasicTokenizer, text: string) {\n  return (await tokenizer.tokenize(text)).length;\n}\n","import {\n  BaseUrlApiConfigurationWithDefaults,\n  PartialBaseUrlPartsApiConfigurationOptions,\n} from \"../../core/api/BaseUrlApiConfiguration\";\n\n/**\n * Creates an API configuration for the AUTOMATIC1111 Stable Diffusion Web UI API.\n * It calls the API at http://127.0.0.1:7860/sdapi/v1 by default.\n */\nexport class Automatic1111ApiConfiguration extends BaseUrlApiConfigurationWithDefaults {\n  constructor(settings: PartialBaseUrlPartsApiConfigurationOptions = {}) {\n    super({\n      ...settings,\n      baseUrlDefaults: {\n        protocol: \"http\",\n        host: \"127.0.0.1\",\n        port: \"7860\",\n        path: \"/sdapi/v1\",\n      },\n    });\n  }\n}\n","import { z } from \"zod\";\nimport { ApiCallError } from \"../../core/api/ApiCallError\";\nimport {\n  ResponseHandler,\n  createJsonErrorResponseHandler,\n} from \"../../core/api/postToApi\";\nimport { zodSchema } from \"../../core/schema/ZodSchema\";\n\nconst automatic1111ErrorDataSchema = z.object({\n  error: z.string(),\n  detail: z.string(),\n  body: z.string(),\n  errors: z.string(),\n});\n\nexport type Automatic1111ErrorData = z.infer<\n  typeof automatic1111ErrorDataSchema\n>;\n\nexport const failedAutomatic1111CallResponseHandler: ResponseHandler<ApiCallError> =\n  createJsonErrorResponseHandler({\n    errorSchema: zodSchema(automatic1111ErrorDataSchema),\n    errorToMessage: (error) => error.detail,\n  });\n","import { base64ToUint8Array, uint8ArrayToBase64 } from \"./UInt8Utils\";\n\n/**\n * Data content. Can either be a base64-encoded string, a Uint8Array, an ArrayBuffer, or a Buffer.\n */\nexport type DataContent = string | Uint8Array | ArrayBuffer | Buffer;\n\nexport function convertDataContentToBase64String(content: DataContent): string {\n  if (typeof content === \"string\") {\n    return content;\n  }\n\n  if (content instanceof ArrayBuffer) {\n    return uint8ArrayToBase64(new Uint8Array(content));\n  }\n\n  return uint8ArrayToBase64(content);\n}\n\nexport function convertDataContentToUint8Array(\n  content: DataContent\n): Uint8Array {\n  if (content instanceof Uint8Array) {\n    return content;\n  }\n\n  if (typeof content === \"string\") {\n    return base64ToUint8Array(content);\n  }\n\n  if (content instanceof ArrayBuffer) {\n    return new Uint8Array(content);\n  }\n\n  throw new Error(\n    `Invalid data content. Expected a string, Uint8Array, ArrayBuffer, or Buffer, but got ${typeof content}.`\n  );\n}\n","import { convertDataContentToUint8Array } from \"../../util/format/DataContent\";\nimport { Schema } from \"../schema/Schema\";\nimport { parseJSON, safeParseJSON } from \"../schema/parseJSON\";\nimport { ApiCallError } from \"./ApiCallError\";\n\nexport type ResponseHandler<T> = (options: {\n  url: string;\n  requestBodyValues: unknown;\n  response: Response;\n}) => PromiseLike<T>;\n\nexport const createJsonErrorResponseHandler =\n  <T>({\n    errorSchema,\n    errorToMessage,\n    isRetryable,\n  }: {\n    errorSchema: Schema<T>;\n    errorToMessage: (error: T) => string;\n    isRetryable?: (response: Response, error?: T) => boolean;\n  }): ResponseHandler<ApiCallError> =>\n  async ({ response, url, requestBodyValues }) => {\n    const responseBody = await response.text();\n\n    // Some providers return an empty response body for some errors:\n    if (responseBody.trim() === \"\") {\n      return new ApiCallError({\n        message: response.statusText,\n        url,\n        requestBodyValues,\n        statusCode: response.status,\n        responseBody,\n        isRetryable: isRetryable?.(response),\n      });\n    }\n\n    // resilient parsing in case the response is not JSON or does not match the schema:\n    try {\n      const parsedError = parseJSON({\n        text: responseBody,\n        schema: errorSchema,\n      });\n\n      return new ApiCallError({\n        message: errorToMessage(parsedError),\n        url,\n        requestBodyValues,\n        statusCode: response.status,\n        responseBody,\n        data: parsedError,\n        isRetryable: isRetryable?.(response, parsedError),\n      });\n    } catch (parseError) {\n      return new ApiCallError({\n        message: response.statusText,\n        url,\n        requestBodyValues,\n        statusCode: response.status,\n        responseBody,\n        isRetryable: isRetryable?.(response),\n      });\n    }\n  };\n\nexport const createTextErrorResponseHandler =\n  ({\n    isRetryable,\n  }: {\n    isRetryable?: (response: Response) => boolean;\n  } = {}): ResponseHandler<ApiCallError> =>\n  async ({ response, url, requestBodyValues }) => {\n    const responseBody = await response.text();\n\n    return new ApiCallError({\n      message: responseBody.trim() !== \"\" ? responseBody : response.statusText,\n      url,\n      requestBodyValues,\n      statusCode: response.status,\n      responseBody,\n      isRetryable: isRetryable?.(response),\n    });\n  };\n\nexport const createJsonResponseHandler =\n  <T>(responseSchema: Schema<T>): ResponseHandler<T> =>\n  async ({ response, url, requestBodyValues }) => {\n    const responseBody = await response.text();\n\n    const parsedResult = safeParseJSON({\n      text: responseBody,\n      schema: responseSchema,\n    });\n\n    if (!parsedResult.success) {\n      throw new ApiCallError({\n        message: \"Invalid JSON response\",\n        cause: parsedResult.error,\n        statusCode: response.status,\n        responseBody,\n        url,\n        requestBodyValues,\n      });\n    }\n\n    return parsedResult.value;\n  };\n\nexport const createTextResponseHandler =\n  (): ResponseHandler<string> =>\n  async ({ response }) =>\n    response.text();\n\nexport const createAudioMpegResponseHandler =\n  (): ResponseHandler<Uint8Array> =>\n  async ({ response, url, requestBodyValues }) => {\n    if (response.headers.get(\"Content-Type\") !== \"audio/mpeg\") {\n      throw new ApiCallError({\n        message: \"Invalid Content-Type (must be audio/mpeg)\",\n        statusCode: response.status,\n        url,\n        requestBodyValues,\n      });\n    }\n\n    return convertDataContentToUint8Array(await response.arrayBuffer());\n  };\n\nexport const postJsonToApi = async <T>({\n  url,\n  headers,\n  body,\n  failedResponseHandler,\n  successfulResponseHandler,\n  abortSignal,\n}: {\n  url: string;\n  headers?: Record<string, string>;\n  body: unknown;\n  failedResponseHandler: ResponseHandler<ApiCallError>;\n  successfulResponseHandler: ResponseHandler<T>;\n  abortSignal?: AbortSignal;\n}) =>\n  postToApi({\n    url,\n    headers: {\n      ...headers,\n      \"Content-Type\": \"application/json\",\n    },\n    body: {\n      content: JSON.stringify(body),\n      values: body,\n    },\n    failedResponseHandler,\n    successfulResponseHandler,\n    abortSignal,\n  });\n\nexport const postToApi = async <T>({\n  url,\n  headers = {},\n  body,\n  successfulResponseHandler,\n  failedResponseHandler,\n  abortSignal,\n}: {\n  url: string;\n  headers?: Record<string, string>;\n  body: {\n    content: string | FormData | Uint8Array;\n    values: unknown;\n  };\n  failedResponseHandler: ResponseHandler<Error>;\n  successfulResponseHandler: ResponseHandler<T>;\n  abortSignal?: AbortSignal;\n}) => {\n  try {\n    const response = await fetch(url, {\n      method: \"POST\",\n      headers,\n      body: body.content,\n      signal: abortSignal,\n    });\n\n    if (!response.ok) {\n      try {\n        throw await failedResponseHandler({\n          response,\n          url,\n          requestBodyValues: body.values,\n        });\n      } catch (error) {\n        if (error instanceof Error) {\n          if (error.name === \"AbortError\" || error instanceof ApiCallError) {\n            throw error;\n          }\n        }\n\n        throw new ApiCallError({\n          message: \"Failed to process error response\",\n          cause: error,\n          statusCode: response.status,\n          url,\n          requestBodyValues: body.values,\n        });\n      }\n    }\n\n    try {\n      return await successfulResponseHandler({\n        response,\n        url,\n        requestBodyValues: body.values,\n      });\n    } catch (error) {\n      if (error instanceof Error) {\n        if (error.name === \"AbortError\" || error instanceof ApiCallError) {\n          throw error;\n        }\n      }\n\n      throw new ApiCallError({\n        message: \"Failed to process successful response\",\n        cause: error,\n        statusCode: response.status,\n        url,\n        requestBodyValues: body.values,\n      });\n    }\n  } catch (error) {\n    if (error instanceof Error) {\n      if (error.name === \"AbortError\") {\n        throw error;\n      }\n    }\n\n    // unwrap original error when fetch failed (for easier debugging):\n    if (error instanceof TypeError && error.message === \"fetch failed\") {\n      // eslint-disable-next-line @typescript-eslint/no-explicit-any\n      const cause = (error as any).cause;\n\n      if (cause != null) {\n        // Failed to connect to server:\n        throw new ApiCallError({\n          message: `Cannot connect to API: ${cause.message}`,\n          cause,\n          url,\n          requestBodyValues: body.values,\n          isRetryable: true,\n        });\n      }\n    }\n\n    throw error;\n  }\n};\n","import { PartialBaseUrlPartsApiConfigurationOptions } from \"../../core/api/BaseUrlApiConfiguration\";\nimport { Automatic1111ApiConfiguration } from \"./Automatic1111ApiConfiguration\";\nimport {\n  Automatic1111ImageGenerationModel,\n  Automatic1111ImageGenerationSettings,\n} from \"./Automatic1111ImageGenerationModel\";\n\n/**\n * Creates an API configuration for the AUTOMATIC1111 Stable Diffusion Web UI API.\n * It calls the API at http://127.0.0.1:7860/sdapi/v1 by default.\n */\nexport function Api(settings: PartialBaseUrlPartsApiConfigurationOptions) {\n  return new Automatic1111ApiConfiguration(settings);\n}\n\n/**\n * Create an image generation model that calls the AUTOMATIC1111 Stable Diffusion Web UI API.\n *\n * @see https://github.com/AUTOMATIC1111/stable-diffusion-webui\n *\n * @return A new instance of ${@link Automatic1111ImageGenerationModel}.\n */\nexport function ImageGenerator(settings: Automatic1111ImageGenerationSettings) {\n  return new Automatic1111ImageGenerationModel(settings);\n}\n","import { z } from \"zod\";\nimport { FunctionCallOptions } from \"../../core/FunctionOptions\";\nimport { ApiConfiguration } from \"../../core/api/ApiConfiguration\";\nimport { callWithRetryAndThrottle } from \"../../core/api/callWithRetryAndThrottle\";\nimport {\n  createJsonResponseHandler,\n  postJsonToApi,\n} from \"../../core/api/postToApi\";\nimport { zodSchema } from \"../../core/schema/ZodSchema\";\nimport { AbstractModel } from \"../../model-function/AbstractModel\";\nimport { PromptTemplate } from \"../../model-function/PromptTemplate\";\nimport {\n  ImageGenerationModel,\n  ImageGenerationModelSettings,\n} from \"../../model-function/generate-image/ImageGenerationModel\";\nimport { PromptTemplateImageGenerationModel } from \"../../model-function/generate-image/PromptTemplateImageGenerationModel\";\nimport { Automatic1111ApiConfiguration } from \"./Automatic1111ApiConfiguration\";\nimport { failedAutomatic1111CallResponseHandler } from \"./Automatic1111Error\";\nimport {\n  Automatic1111ImageGenerationPrompt,\n  mapBasicPromptToAutomatic1111Format,\n} from \"./Automatic1111ImageGenerationPrompt\";\n\nexport interface Automatic1111ImageGenerationSettings\n  extends ImageGenerationModelSettings {\n  api?: ApiConfiguration;\n\n  /**\n   * Stable Diffusion checkpoint.\n   */\n  model: string;\n\n  height?: number;\n  width?: number;\n\n  /**\n   * Sampling method.\n   */\n  sampler?: string;\n\n  /**\n   * Sampling steps.\n   */\n  steps?: number;\n\n  /**\n   * CFG Scale.\n   */\n  cfgScale?: number;\n\n  seed?: number;\n}\n\n/**\n * Create an image generation model that calls the AUTOMATIC1111 Stable Diffusion Web UI API.\n *\n * @see https://github.com/AUTOMATIC1111/stable-diffusion-webui\n */\nexport class Automatic1111ImageGenerationModel\n  extends AbstractModel<Automatic1111ImageGenerationSettings>\n  implements\n    ImageGenerationModel<\n      Automatic1111ImageGenerationPrompt,\n      Automatic1111ImageGenerationSettings\n    >\n{\n  constructor(settings: Automatic1111ImageGenerationSettings) {\n    super({ settings });\n  }\n\n  readonly provider = \"Automatic1111\" as const;\n\n  get modelName() {\n    return this.settings.model;\n  }\n\n  async callAPI(\n    input: Automatic1111ImageGenerationPrompt,\n    callOptions: FunctionCallOptions\n  ): Promise<Automatic1111ImageGenerationResponse> {\n    const api = this.settings.api ?? new Automatic1111ApiConfiguration();\n    const abortSignal = callOptions.run?.abortSignal;\n\n    return callWithRetryAndThrottle({\n      retry: api.retry,\n      throttle: api.throttle,\n      call: async () =>\n        postJsonToApi({\n          url: api.assembleUrl(`/txt2img`),\n          headers: api.headers({\n            functionType: callOptions.functionType,\n            functionId: callOptions.functionId,\n            run: callOptions.run,\n            callId: callOptions.callId,\n          }),\n          body: {\n            prompt: input.prompt,\n            negative_prompt: input.negativePrompt,\n            seed: this.settings.seed,\n            batch_size: this.settings.numberOfGenerations,\n            height: this.settings.height,\n            width: this.settings.width,\n            cfg_scale: this.settings.cfgScale,\n            sampler_index: this.settings.sampler,\n            steps: this.settings.steps,\n            override_settings: {\n              sd_model_checkpoint: this.settings.model,\n            },\n          },\n          failedResponseHandler: failedAutomatic1111CallResponseHandler,\n          successfulResponseHandler: createJsonResponseHandler(\n            zodSchema(Automatic1111ImageGenerationResponseSchema)\n          ),\n          abortSignal,\n        }),\n    });\n  }\n\n  get settingsForEvent(): Partial<Automatic1111ImageGenerationSettings> {\n    const eventSettingProperties: Array<string> = [\n      \"height\",\n      \"width\",\n      \"sampler\",\n      \"steps\",\n      \"cfgScale\",\n      \"seed\",\n    ] satisfies (keyof Automatic1111ImageGenerationSettings)[];\n\n    return Object.fromEntries(\n      Object.entries(this.settings).filter(([key]) =>\n        eventSettingProperties.includes(key)\n      )\n    );\n  }\n\n  async doGenerateImages(\n    prompt: Automatic1111ImageGenerationPrompt,\n    options: FunctionCallOptions\n  ) {\n    const rawResponse = await this.callAPI(prompt, options);\n\n    return {\n      rawResponse,\n      base64Images: rawResponse.images,\n    };\n  }\n\n  withTextPrompt() {\n    return this.withPromptTemplate(mapBasicPromptToAutomatic1111Format());\n  }\n\n  withPromptTemplate<INPUT_PROMPT>(\n    promptTemplate: PromptTemplate<\n      INPUT_PROMPT,\n      Automatic1111ImageGenerationPrompt\n    >\n  ): PromptTemplateImageGenerationModel<\n    INPUT_PROMPT,\n    Automatic1111ImageGenerationPrompt,\n    Automatic1111ImageGenerationSettings,\n    this\n  > {\n    return new PromptTemplateImageGenerationModel({\n      model: this,\n      promptTemplate,\n    });\n  }\n\n  withSettings(\n    additionalSettings: Partial<Automatic1111ImageGenerationSettings>\n  ) {\n    return new Automatic1111ImageGenerationModel(\n      Object.assign({}, this.settings, additionalSettings)\n    ) as this;\n  }\n}\n\nconst Automatic1111ImageGenerationResponseSchema = z.object({\n  images: z.array(z.string()),\n  parameters: z.object({}),\n  info: z.string(),\n});\n\nexport type Automatic1111ImageGenerationResponse = z.infer<\n  typeof Automatic1111ImageGenerationResponseSchema\n>;\n","import { RetryFunction } from \"./RetryFunction\";\nimport { retryNever } from \"./retryNever\";\nimport { ThrottleFunction } from \"./ThrottleFunction\";\nimport { throttleOff } from \"./throttleOff\";\n\nexport const callWithRetryAndThrottle = async <OUTPUT>({\n  retry = retryNever(),\n  throttle = throttleOff(),\n  call,\n}: {\n  retry?: RetryFunction;\n  throttle?: ThrottleFunction;\n  call: () => PromiseLike<OUTPUT>;\n}): Promise<OUTPUT> => retry(async () => throttle(call));\n","import { ModelInformation } from \"./ModelInformation\";\nimport { Model, ModelSettings } from \"./Model\";\n\nexport abstract class AbstractModel<SETTINGS extends ModelSettings>\n  implements Model<SETTINGS>\n{\n  readonly settings: SETTINGS;\n\n  constructor({ settings }: { settings: SETTINGS }) {\n    this.settings = settings;\n  }\n\n  abstract readonly provider: string;\n  abstract readonly modelName: string | null;\n\n  // implemented as a separate accessor to remove all other properties from the model\n  get modelInformation(): ModelInformation {\n    return {\n      provider: this.provider,\n      modelName: this.modelName,\n    };\n  }\n\n  abstract get settingsForEvent(): Partial<SETTINGS>;\n\n  abstract withSettings(additionalSettings: Partial<SETTINGS>): this;\n}\n","import { PromptTemplate } from \"../../model-function/PromptTemplate\";\n\nexport type Automatic1111ImageGenerationPrompt = {\n  prompt: string;\n  negativePrompt?: string;\n};\n\n/**\n * Formats a basic text prompt as an Automatic1111 prompt.\n */\nexport function mapBasicPromptToAutomatic1111Format(): PromptTemplate<\n  string,\n  Automatic1111ImageGenerationPrompt\n> {\n  return {\n    format: (description) => ({ prompt: description }),\n  };\n}\n","export class LoadAPIKeyError extends Error {\n  constructor({ message }: { message: string }) {\n    super(message);\n\n    this.name = \"LoadAPIKeyError\";\n  }\n\n  toJSON() {\n    return {\n      name: this.name,\n      message: this.message,\n    };\n  }\n}\n","import { LoadAPIKeyError } from \"./LoadAPIKeyError\";\n\nexport function loadApiKey({\n  apiKey,\n  environmentVariableName,\n  apiKeyParameterName = \"apiKey\",\n  description,\n}: {\n  apiKey: string | undefined;\n  environmentVariableName: string;\n  apiKeyParameterName?: string;\n  description: string;\n}): string {\n  if (apiKey != null) {\n    return apiKey;\n  }\n\n  if (typeof process === \"undefined\") {\n    throw new LoadAPIKeyError({\n      message: `${description} API key is missing. Pass it using the '${apiKeyParameterName}' parameter into the API configuration. Environment variables is not supported in this environment.`,\n    });\n  }\n\n  apiKey = process.env[environmentVariableName];\n\n  if (apiKey == null) {\n    throw new LoadAPIKeyError({\n      message: `${description} API key is missing. Pass it using the '${apiKeyParameterName}' parameter into the API configuration or set it as an environment variable named ${environmentVariableName}.`,\n    });\n  }\n\n  return apiKey;\n}\n","import {\n  BaseUrlApiConfigurationWithDefaults,\n  PartialBaseUrlPartsApiConfigurationOptions,\n} from \"../../core/api/BaseUrlApiConfiguration\";\nimport { loadApiKey } from \"../../core/api/loadApiKey\";\n\n/**\n * Creates an API configuration for the Cohere API.\n * It calls the API at https://api.cohere.ai/v1 and uses the `COHERE_API_KEY` env variable by default.\n */\nexport class CohereApiConfiguration extends BaseUrlApiConfigurationWithDefaults {\n  constructor(\n    settings: PartialBaseUrlPartsApiConfigurationOptions & {\n      apiKey?: string;\n    } = {}\n  ) {\n    super({\n      ...settings,\n      headers: {\n        Authorization: `Bearer ${loadApiKey({\n          apiKey: settings.apiKey,\n          environmentVariableName: \"COHERE_API_KEY\",\n          description: \"Cohere\",\n        })}`,\n      },\n      baseUrlDefaults: {\n        protocol: \"https\",\n        host: \"api.cohere.ai\",\n        port: \"443\",\n        path: \"/v1\",\n      },\n    });\n  }\n}\n","import { z } from \"zod\";\nimport { createJsonErrorResponseHandler } from \"../../core/api/postToApi\";\nimport { zodSchema } from \"../../core/schema/ZodSchema\";\n\nconst cohereErrorDataSchema = z.object({\n  message: z.string(),\n});\n\nexport type CohereErrorData = z.infer<typeof cohereErrorDataSchema>;\n\nexport const failedCohereCallResponseHandler = createJsonErrorResponseHandler({\n  errorSchema: zodSchema(cohereErrorDataSchema),\n  errorToMessage: (error) => error.message,\n});\n","import { PartialBaseUrlPartsApiConfigurationOptions } from \"../../core/api/BaseUrlApiConfiguration\";\nimport { CohereApiConfiguration } from \"./CohereApiConfiguration\";\nimport {\n  CohereTextEmbeddingModel,\n  CohereTextEmbeddingModelSettings,\n} from \"./CohereTextEmbeddingModel\";\nimport {\n  CohereTextGenerationModel,\n  CohereTextGenerationModelSettings,\n} from \"./CohereTextGenerationModel\";\nimport { CohereTokenizer, CohereTokenizerSettings } from \"./CohereTokenizer\";\n\n/**\n * Creates an API configuration for the Cohere API.\n * It calls the API at https://api.cohere.ai/v1 and uses the `COHERE_API_KEY` env variable by default.\n */\nexport function Api(\n  settings: PartialBaseUrlPartsApiConfigurationOptions & {\n    apiKey?: string;\n  }\n) {\n  return new CohereApiConfiguration(settings);\n}\n\n/**\n * Create a text generation model that calls the Cohere Co.Generate API.\n *\n * @see https://docs.cohere.com/reference/generate\n *\n * @example\n * const model = cohere.TextGenerator({\n *   model: \"command\",\n *   temperature: 0.7,\n *   maxGenerationTokens: 500,\n * });\n *\n * const text = await generateText(\n *    model,\n *   \"Write a short story about a robot learning to love:\\n\\n\"\n * );\n *\n * @returns A new instance of {@link CohereTextGenerationModel}.\n */\nexport function TextGenerator(settings: CohereTextGenerationModelSettings) {\n  return new CohereTextGenerationModel(settings);\n}\n\n/**\n * Create a text embedding model that calls the Cohere Co.Embed API.\n *\n * @see https://docs.cohere.com/reference/embed\n *\n * @example\n * const embeddings = await embedMany(\n *   cohere.TextEmbedder({ model: \"embed-english-light-v2.0\" }),\n *   [\n *     \"At first, Nox didn't know what to do with the pup.\",\n *     \"He keenly observed and absorbed everything around him, from the birds in the sky to the trees in the forest.\",\n *   ]\n * );\n *\n * @returns A new instance of {@link CohereTextEmbeddingModel}.\n */\nexport function TextEmbedder(settings: CohereTextEmbeddingModelSettings) {\n  return new CohereTextEmbeddingModel(settings);\n}\n\n/**\n * Tokenizer for the Cohere models. It uses the Co.Tokenize and Co.Detokenize APIs.\n *\n * @see https://docs.cohere.com/reference/tokenize\n * @see https://docs.cohere.com/reference/detokenize-1\n *\n * @example\n * const tokenizer = cohere.Tokenizer({ model: \"command\" });\n *\n * const text = \"At first, Nox didn't know what to do with the pup.\";\n *\n * const tokenCount = await countTokens(tokenizer, text);\n * const tokens = await tokenizer.tokenize(text);\n * const tokensAndTokenTexts = await tokenizer.tokenizeWithTexts(text);\n * const reconstructedText = await tokenizer.detokenize(tokens);\n *\n * @returns A new instance of {@link CohereTokenizer}.\n */\nexport function Tokenizer(settings: CohereTokenizerSettings) {\n  return new CohereTokenizer(settings);\n}\n","import { z } from \"zod\";\nimport { FunctionCallOptions } from \"../../core/FunctionOptions\";\nimport { ApiConfiguration } from \"../../core/api/ApiConfiguration\";\nimport { callWithRetryAndThrottle } from \"../../core/api/callWithRetryAndThrottle\";\nimport {\n  createJsonResponseHandler,\n  postJsonToApi,\n} from \"../../core/api/postToApi\";\nimport { zodSchema } from \"../../core/schema/ZodSchema\";\nimport { AbstractModel } from \"../../model-function/AbstractModel\";\nimport {\n  EmbeddingModel,\n  EmbeddingModelSettings,\n} from \"../../model-function/embed/EmbeddingModel\";\nimport { FullTokenizer } from \"../../model-function/tokenize-text/Tokenizer\";\nimport { CohereApiConfiguration } from \"./CohereApiConfiguration\";\nimport { failedCohereCallResponseHandler } from \"./CohereError\";\nimport { CohereTokenizer } from \"./CohereTokenizer\";\n\nexport const COHERE_TEXT_EMBEDDING_MODELS = {\n  \"embed-english-light-v2.0\": {\n    contextWindowSize: 512,\n    dimensions: 1024,\n  },\n  \"embed-english-v2.0\": {\n    contextWindowSize: 512,\n    dimensions: 4096,\n  },\n  \"embed-multilingual-v2.0\": {\n    contextWindowSize: 256,\n    dimensions: 768,\n  },\n  \"embed-english-v3.0\": {\n    contextWindowSize: 512,\n    dimensions: 1024,\n  },\n  \"embed-english-light-v3.0\": {\n    contextWindowSize: 512,\n    dimensions: 384,\n  },\n  \"embed-multilingual-v3.0\": {\n    contextWindowSize: 512,\n    dimensions: 1024,\n  },\n  \"embed-multilingual-light-v3.0\": {\n    contextWindowSize: 512,\n    dimensions: 384,\n  },\n};\n\nexport type CohereTextEmbeddingModelType =\n  keyof typeof COHERE_TEXT_EMBEDDING_MODELS;\n\nexport interface CohereTextEmbeddingModelSettings\n  extends EmbeddingModelSettings {\n  api?: ApiConfiguration;\n  model: CohereTextEmbeddingModelType;\n  inputType?:\n    | \"search_document\"\n    | \"search_query\"\n    | \"classification\"\n    | \"clustering\";\n  truncate?: \"NONE\" | \"START\" | \"END\";\n}\n\n/**\n * Create a text embedding model that calls the Cohere Co.Embed API.\n *\n * @see https://docs.cohere.com/reference/embed\n *\n * @example\n * const embeddings = await embedMany(\n *   new CohereTextEmbeddingModel({ model: \"embed-english-light-v2.0\" }),\n *   [\n *     \"At first, Nox didn't know what to do with the pup.\",\n *     \"He keenly observed and absorbed everything around him, from the birds in the sky to the trees in the forest.\",\n *   ]\n * );\n */\nexport class CohereTextEmbeddingModel\n  extends AbstractModel<CohereTextEmbeddingModelSettings>\n  implements\n    EmbeddingModel<string, CohereTextEmbeddingModelSettings>,\n    FullTokenizer\n{\n  constructor(settings: CohereTextEmbeddingModelSettings) {\n    super({ settings });\n\n    this.contextWindowSize =\n      COHERE_TEXT_EMBEDDING_MODELS[this.modelName].contextWindowSize;\n\n    this.tokenizer = new CohereTokenizer({\n      api: this.settings.api,\n      model: this.settings.model,\n    });\n\n    this.dimensions = COHERE_TEXT_EMBEDDING_MODELS[this.modelName].dimensions;\n  }\n\n  readonly provider = \"cohere\" as const;\n  get modelName() {\n    return this.settings.model;\n  }\n\n  readonly maxValuesPerCall = 96;\n  readonly isParallelizable = true;\n  readonly dimensions: number;\n\n  readonly contextWindowSize: number;\n  private readonly tokenizer: CohereTokenizer;\n\n  async tokenize(text: string) {\n    return this.tokenizer.tokenize(text);\n  }\n\n  async tokenizeWithTexts(text: string) {\n    return this.tokenizer.tokenizeWithTexts(text);\n  }\n\n  async detokenize(tokens: number[]) {\n    return this.tokenizer.detokenize(tokens);\n  }\n\n  async callAPI(\n    texts: Array<string>,\n    callOptions: FunctionCallOptions\n  ): Promise<CohereTextEmbeddingResponse> {\n    if (texts.length > this.maxValuesPerCall) {\n      throw new Error(\n        `The Cohere embedding API only supports ${this.maxValuesPerCall} texts per API call.`\n      );\n    }\n\n    const api = this.settings.api ?? new CohereApiConfiguration();\n    const abortSignal = callOptions.run?.abortSignal;\n\n    return callWithRetryAndThrottle({\n      retry: api.retry,\n      throttle: api.throttle,\n      call: async () =>\n        postJsonToApi({\n          url: api.assembleUrl(`/embed`),\n          headers: api.headers({\n            functionType: callOptions.functionType,\n            functionId: callOptions.functionId,\n            run: callOptions.run,\n            callId: callOptions.callId,\n          }),\n          body: {\n            model: this.settings.model,\n            texts,\n            input_type: this.settings.inputType,\n            truncate: this.settings.truncate,\n          },\n          failedResponseHandler: failedCohereCallResponseHandler,\n          successfulResponseHandler: createJsonResponseHandler(\n            zodSchema(cohereTextEmbeddingResponseSchema)\n          ),\n          abortSignal,\n        }),\n    });\n  }\n\n  get settingsForEvent(): Partial<CohereTextEmbeddingModelSettings> {\n    return {\n      truncate: this.settings.truncate,\n    };\n  }\n\n  async doEmbedValues(texts: string[], options: FunctionCallOptions) {\n    const rawResponse = await this.callAPI(texts, options);\n    return {\n      rawResponse,\n      embeddings: rawResponse.embeddings,\n    };\n  }\n\n  withSettings(additionalSettings: Partial<CohereTextEmbeddingModelSettings>) {\n    return new CohereTextEmbeddingModel(\n      Object.assign({}, this.settings, additionalSettings)\n    ) as this;\n  }\n}\n\nconst cohereTextEmbeddingResponseSchema = z.object({\n  id: z.string(),\n  texts: z.array(z.string()),\n  embeddings: z.array(z.array(z.number())),\n  meta: z.object({\n    api_version: z.object({\n      version: z.string(),\n    }),\n  }),\n});\n\nexport type CohereTextEmbeddingResponse = z.infer<\n  typeof cohereTextEmbeddingResponseSchema\n>;\n","import { z } from \"zod\";\nimport { FunctionCallOptions } from \"../../core/FunctionOptions\";\nimport { ApiConfiguration } from \"../../core/api/ApiConfiguration\";\nimport { callWithRetryAndThrottle } from \"../../core/api/callWithRetryAndThrottle\";\nimport {\n  createJsonResponseHandler,\n  postJsonToApi,\n} from \"../../core/api/postToApi\";\nimport { zodSchema } from \"../../core/schema/ZodSchema\";\nimport { FullTokenizer } from \"../../model-function/tokenize-text/Tokenizer\";\nimport { CohereApiConfiguration } from \"./CohereApiConfiguration\";\nimport { failedCohereCallResponseHandler } from \"./CohereError\";\nimport { CohereTextEmbeddingModelType } from \"./CohereTextEmbeddingModel\";\nimport { CohereTextGenerationModelType } from \"./CohereTextGenerationModel\";\n\nexport type CohereTokenizerModelType =\n  | CohereTextGenerationModelType\n  | CohereTextEmbeddingModelType;\n\nexport interface CohereTokenizerSettings {\n  api?: ApiConfiguration;\n  model: CohereTokenizerModelType;\n}\n\n/**\n * Tokenizer for the Cohere models. It uses the Co.Tokenize and Co.Detokenize APIs.\n *\n * @see https://docs.cohere.com/reference/tokenize\n * @see https://docs.cohere.com/reference/detokenize\n *\n * @example\n * const tokenizer = new CohereTokenizer({ model: \"command\" });\n *\n * const text = \"At first, Nox didn't know what to do with the pup.\";\n *\n * const tokenCount = await countTokens(tokenizer, text);\n * const tokens = await tokenizer.tokenize(text);\n * const tokensAndTokenTexts = await tokenizer.tokenizeWithTexts(text);\n * const reconstructedText = await tokenizer.detokenize(tokens);\n */\nexport class CohereTokenizer implements FullTokenizer {\n  readonly settings: CohereTokenizerSettings;\n\n  constructor(settings: CohereTokenizerSettings) {\n    this.settings = settings;\n  }\n\n  async callTokenizeAPI(\n    text: string,\n    callOptions?: FunctionCallOptions\n  ): Promise<CohereTokenizationResponse> {\n    const api = this.settings.api ?? new CohereApiConfiguration();\n    const abortSignal = callOptions?.run?.abortSignal;\n\n    return callWithRetryAndThrottle({\n      retry: api.retry,\n      throttle: api.throttle,\n      call: async () =>\n        postJsonToApi({\n          url: api.assembleUrl(`/tokenize`),\n          headers: api.headers({\n            functionType: \"tokenize\",\n            functionId: callOptions?.functionId,\n            run: callOptions?.run,\n            callId: \"\",\n          }),\n          body: {\n            model: this.settings.model,\n            text,\n          },\n          failedResponseHandler: failedCohereCallResponseHandler,\n          successfulResponseHandler: createJsonResponseHandler(\n            zodSchema(cohereTokenizationResponseSchema)\n          ),\n          abortSignal,\n        }),\n    });\n  }\n\n  async callDeTokenizeAPI(\n    tokens: number[],\n    callOptions?: FunctionCallOptions\n  ): Promise<CohereDetokenizationResponse> {\n    const api = this.settings.api ?? new CohereApiConfiguration();\n    const abortSignal = callOptions?.run?.abortSignal;\n\n    return callWithRetryAndThrottle({\n      retry: api.retry,\n      throttle: api.throttle,\n      call: async () =>\n        postJsonToApi({\n          url: api.assembleUrl(`/detokenize`),\n          headers: api.headers({\n            functionType: \"detokenize\",\n            functionId: callOptions?.functionId,\n            run: callOptions?.run,\n            callId: \"\",\n          }),\n          body: {\n            model: this.settings.model,\n            tokens,\n          },\n          failedResponseHandler: failedCohereCallResponseHandler,\n          successfulResponseHandler: createJsonResponseHandler(\n            zodSchema(cohereDetokenizationResponseSchema)\n          ),\n          abortSignal,\n        }),\n    });\n  }\n\n  async tokenize(text: string) {\n    return (await this.tokenizeWithTexts(text)).tokens;\n  }\n\n  async tokenizeWithTexts(text: string) {\n    const response = await this.callTokenizeAPI(text);\n\n    return {\n      tokens: response.tokens,\n      tokenTexts: response.token_strings,\n    };\n  }\n\n  async detokenize(tokens: number[]) {\n    const response = await this.callDeTokenizeAPI(tokens);\n\n    return response.text;\n  }\n}\n\nconst cohereDetokenizationResponseSchema = z.object({\n  text: z.string(),\n  meta: z.object({\n    api_version: z.object({\n      version: z.string(),\n    }),\n  }),\n});\n\nexport type CohereDetokenizationResponse = z.infer<\n  typeof cohereDetokenizationResponseSchema\n>;\n\nconst cohereTokenizationResponseSchema = z.object({\n  tokens: z.array(z.number()),\n  token_strings: z.array(z.string()),\n  meta: z.object({\n    api_version: z.object({\n      version: z.string(),\n    }),\n  }),\n});\n\nexport type CohereTokenizationResponse = z.infer<\n  typeof cohereTokenizationResponseSchema\n>;\n","import { z } from \"zod\";\nimport { FunctionCallOptions } from \"../../core/FunctionOptions\";\nimport { ApiConfiguration } from \"../../core/api/ApiConfiguration\";\nimport { callWithRetryAndThrottle } from \"../../core/api/callWithRetryAndThrottle\";\nimport {\n  ResponseHandler,\n  createJsonResponseHandler,\n  postJsonToApi,\n} from \"../../core/api/postToApi\";\nimport { zodSchema } from \"../../core/schema/ZodSchema\";\nimport { validateTypes } from \"../../core/schema/validateTypes\";\nimport { AbstractModel } from \"../../model-function/AbstractModel\";\nimport { PromptTemplateTextStreamingModel } from \"../../model-function/generate-text/PromptTemplateTextStreamingModel\";\nimport {\n  TextGenerationModelSettings,\n  TextStreamingBaseModel,\n  textGenerationModelProperties,\n} from \"../../model-function/generate-text/TextGenerationModel\";\nimport { TextGenerationPromptTemplate } from \"../../model-function/generate-text/TextGenerationPromptTemplate\";\nimport { TextGenerationFinishReason } from \"../../model-function/generate-text/TextGenerationResult\";\nimport {\n  chat,\n  instruction,\n  text,\n} from \"../../model-function/generate-text/prompt-template/TextPromptTemplate\";\nimport { countTokens } from \"../../model-function/tokenize-text/countTokens\";\nimport { createJsonStreamResponseHandler } from \"../../util/streaming/createJsonStreamResponseHandler\";\nimport { CohereApiConfiguration } from \"./CohereApiConfiguration\";\nimport { failedCohereCallResponseHandler } from \"./CohereError\";\nimport { CohereTokenizer } from \"./CohereTokenizer\";\n\nexport const COHERE_TEXT_GENERATION_MODELS = {\n  command: {\n    contextWindowSize: 4096,\n  },\n  \"command-light\": {\n    contextWindowSize: 4096,\n  },\n};\n\nexport type CohereTextGenerationModelType =\n  keyof typeof COHERE_TEXT_GENERATION_MODELS;\n\nexport interface CohereTextGenerationModelSettings\n  extends TextGenerationModelSettings {\n  api?: ApiConfiguration;\n\n  model: CohereTextGenerationModelType;\n\n  temperature?: number;\n  k?: number;\n  p?: number;\n  frequencyPenalty?: number;\n  presencePenalty?: number;\n  returnLikelihoods?: \"GENERATION\" | \"ALL\" | \"NONE\";\n  logitBias?: Record<string, number>;\n  truncate?: \"NONE\" | \"START\" | \"END\";\n\n  cohereStopSequences?: string[]; // renamed because of conflict with stopSequences\n}\n\n/**\n * Create a text generation model that calls the Cohere Co.Generate API.\n *\n * @see https://docs.cohere.com/reference/generate\n *\n * @example\n * const model = new CohereTextGenerationModel({\n *   model: \"command\",\n *   temperature: 0.7,\n *   maxGenerationTokens: 500,\n * });\n *\n * const text = await generateText(\n *    model,\n *   \"Write a short story about a robot learning to love:\\n\\n\"\n * );\n */\nexport class CohereTextGenerationModel\n  extends AbstractModel<CohereTextGenerationModelSettings>\n  implements TextStreamingBaseModel<string, CohereTextGenerationModelSettings>\n{\n  constructor(settings: CohereTextGenerationModelSettings) {\n    super({ settings });\n\n    this.contextWindowSize =\n      COHERE_TEXT_GENERATION_MODELS[this.settings.model].contextWindowSize;\n\n    this.tokenizer = new CohereTokenizer({\n      api: this.settings.api,\n      model: this.settings.model,\n    });\n  }\n\n  readonly provider = \"cohere\" as const;\n  get modelName() {\n    return this.settings.model;\n  }\n\n  readonly contextWindowSize: number;\n  readonly tokenizer: CohereTokenizer;\n\n  async countPromptTokens(input: string) {\n    return countTokens(this.tokenizer, input);\n  }\n\n  async callAPI<RESPONSE>(\n    prompt: string,\n    callOptions: FunctionCallOptions,\n    options: {\n      responseFormat: CohereTextGenerationResponseFormatType<RESPONSE>;\n    }\n  ): Promise<RESPONSE> {\n    const api = this.settings.api ?? new CohereApiConfiguration();\n    const responseFormat = options.responseFormat;\n    const abortSignal = callOptions.run?.abortSignal;\n\n    return callWithRetryAndThrottle({\n      retry: api.retry,\n      throttle: api.throttle,\n      call: async () =>\n        postJsonToApi({\n          url: api.assembleUrl(`/generate`),\n          headers: api.headers({\n            functionType: callOptions.functionType,\n            functionId: callOptions.functionId,\n            run: callOptions.run,\n            callId: callOptions.callId,\n          }),\n          body: {\n            stream: responseFormat.stream,\n            model: this.settings.model,\n            prompt,\n            num_generations: this.settings.numberOfGenerations,\n            max_tokens: this.settings.maxGenerationTokens,\n            temperature: this.settings.temperature,\n            k: this.settings.k,\n            p: this.settings.p,\n            frequency_penalty: this.settings.frequencyPenalty,\n            presence_penalty: this.settings.presencePenalty,\n            end_sequences: this.settings.stopSequences,\n            stop_sequences: this.settings.cohereStopSequences,\n            return_likelihoods: this.settings.returnLikelihoods,\n            logit_bias: this.settings.logitBias,\n            truncate: this.settings.truncate,\n          },\n          failedResponseHandler: failedCohereCallResponseHandler,\n          successfulResponseHandler: responseFormat.handler,\n          abortSignal,\n        }),\n    });\n  }\n\n  get settingsForEvent(): Partial<CohereTextGenerationModelSettings> {\n    const eventSettingProperties: Array<string> = [\n      ...textGenerationModelProperties,\n\n      \"temperature\",\n      \"k\",\n      \"p\",\n      \"frequencyPenalty\",\n      \"presencePenalty\",\n      \"returnLikelihoods\",\n      \"logitBias\",\n      \"truncate\",\n      \"cohereStopSequences\",\n    ] satisfies (keyof CohereTextGenerationModelSettings)[];\n\n    return Object.fromEntries(\n      Object.entries(this.settings).filter(([key]) =>\n        eventSettingProperties.includes(key)\n      )\n    );\n  }\n\n  async doGenerateTexts(prompt: string, options: FunctionCallOptions) {\n    return this.processTextGenerationResponse(\n      await this.callAPI(prompt, options, {\n        responseFormat: CohereTextGenerationResponseFormat.json,\n      })\n    );\n  }\n\n  restoreGeneratedTexts(rawResponse: unknown) {\n    return this.processTextGenerationResponse(\n      validateTypes({\n        value: rawResponse,\n        schema: zodSchema(cohereTextGenerationResponseSchema),\n      })\n    );\n  }\n\n  processTextGenerationResponse(rawResponse: CohereTextGenerationResponse) {\n    return {\n      rawResponse,\n      textGenerationResults: rawResponse.generations.map((generation) => ({\n        text: generation.text,\n        finishReason: this.translateFinishReason(generation.finish_reason),\n      })),\n    };\n  }\n\n  private translateFinishReason(\n    finishReason: string | null | undefined\n  ): TextGenerationFinishReason {\n    switch (finishReason) {\n      case \"COMPLETE\":\n        return \"stop\";\n      case \"MAX_TOKENS\":\n        return \"length\";\n      case \"ERROR_TOXIC\":\n        return \"content-filter\";\n      case \"ERROR\":\n        return \"error\";\n      default:\n        return \"unknown\";\n    }\n  }\n\n  doStreamText(prompt: string, options: FunctionCallOptions) {\n    return this.callAPI(prompt, options, {\n      responseFormat: CohereTextGenerationResponseFormat.deltaIterable,\n    });\n  }\n\n  extractTextDelta(delta: unknown) {\n    const chunk = delta as CohereTextStreamChunk;\n    return chunk.is_finished === true ? \"\" : chunk.text;\n  }\n\n  withJsonOutput(): this {\n    return this;\n  }\n\n  withTextPrompt() {\n    return this.withPromptTemplate(text());\n  }\n\n  withInstructionPrompt() {\n    return this.withPromptTemplate(instruction());\n  }\n\n  withChatPrompt(options?: { user?: string; assistant?: string }) {\n    return this.withPromptTemplate(chat(options));\n  }\n\n  withPromptTemplate<INPUT_PROMPT>(\n    promptTemplate: TextGenerationPromptTemplate<INPUT_PROMPT, string>\n  ): PromptTemplateTextStreamingModel<\n    INPUT_PROMPT,\n    string,\n    CohereTextGenerationModelSettings,\n    this\n  > {\n    return new PromptTemplateTextStreamingModel({\n      model: this.withSettings({\n        stopSequences: [\n          ...(this.settings.stopSequences ?? []),\n          ...promptTemplate.stopSequences,\n        ],\n      }),\n      promptTemplate,\n    });\n  }\n\n  withSettings(additionalSettings: Partial<CohereTextGenerationModelSettings>) {\n    return new CohereTextGenerationModel(\n      Object.assign({}, this.settings, additionalSettings)\n    ) as this;\n  }\n}\n\nconst cohereTextGenerationResponseSchema = z.object({\n  id: z.string(),\n  generations: z.array(\n    z.object({\n      id: z.string(),\n      text: z.string(),\n      finish_reason: z.string().optional(),\n    })\n  ),\n  prompt: z.string(),\n  meta: z\n    .object({\n      api_version: z.object({\n        version: z.string(),\n      }),\n    })\n    .optional(),\n});\n\nexport type CohereTextGenerationResponse = z.infer<\n  typeof cohereTextGenerationResponseSchema\n>;\n\nconst cohereTextStreamChunkSchema = z.discriminatedUnion(\"is_finished\", [\n  z.object({\n    text: z.string(),\n    is_finished: z.literal(false),\n  }),\n  z.object({\n    is_finished: z.literal(true),\n    finish_reason: z.string(),\n    response: cohereTextGenerationResponseSchema,\n  }),\n]);\n\nexport type CohereTextStreamChunk = z.infer<typeof cohereTextStreamChunkSchema>;\n\nexport type CohereTextGenerationResponseFormatType<T> = {\n  stream: boolean;\n  handler: ResponseHandler<T>;\n};\n\nexport const CohereTextGenerationResponseFormat = {\n  /**\n   * Returns the response as a JSON object.\n   */\n  json: {\n    stream: false,\n    handler: createJsonResponseHandler(\n      zodSchema(cohereTextGenerationResponseSchema)\n    ),\n  },\n\n  /**\n   * Returns an async iterable over the full deltas (all choices, including full current state at time of event)\n   * of the response stream.\n   */\n  deltaIterable: {\n    stream: true,\n    handler: createJsonStreamResponseHandler(\n      zodSchema(cohereTextStreamChunkSchema)\n    ),\n  },\n};\n","import { Schema } from \"../../core/schema/Schema\";\nimport { parseJSON } from \"../../core/schema/parseJSON\";\n\nexport function parseJsonStream<T>({\n  schema,\n  stream,\n  process,\n  onDone,\n}: {\n  schema: Schema<T>;\n  stream: ReadableStream<Uint8Array>;\n  process: (event: T) => void;\n  onDone?: () => void;\n}) {\n  function processLine(line: string) {\n    process(parseJSON({ text: line, schema }));\n  }\n\n  return (async () => {\n    try {\n      const reader = new ReadableStreamDefaultReader(stream);\n      const utf8Decoder = new TextDecoder(\"utf-8\");\n\n      let unprocessedText = \"\";\n\n      // eslint-disable-next-line no-constant-condition\n      while (true) {\n        const { value: chunk, done } = await reader.read();\n\n        if (done) {\n          break;\n        }\n\n        unprocessedText += utf8Decoder.decode(chunk, { stream: true });\n\n        const processableLines = unprocessedText.split(\"\\n\");\n\n        unprocessedText = processableLines.pop() ?? \"\";\n\n        processableLines.forEach(processLine);\n      }\n\n      // processing remaining text:\n      if (unprocessedText) {\n        processLine(unprocessedText);\n      }\n    } finally {\n      onDone?.();\n    }\n  })();\n}\n","import { Schema } from \"../../core/schema/Schema\";\nimport { Delta } from \"../../model-function/Delta\";\nimport { AsyncQueue } from \"../AsyncQueue\";\nimport { parseJsonStream } from \"./parseJsonStream\";\n\nexport async function parseJsonStreamAsAsyncIterable<T>({\n  stream,\n  schema,\n}: {\n  stream: ReadableStream<Uint8Array>;\n  schema: Schema<T>;\n}): Promise<AsyncIterable<Delta<T>>> {\n  const queue = new AsyncQueue<Delta<T>>();\n\n  // process the stream asynchonously (no 'await' on purpose):\n  parseJsonStream({\n    stream,\n    schema,\n    process(event) {\n      queue.push({ type: \"delta\", deltaValue: event });\n    },\n    onDone() {\n      queue.close();\n    },\n  });\n\n  return queue;\n}\n","import { Schema } from \"../../core/schema/Schema\";\nimport { parseJsonStreamAsAsyncIterable } from \"./parseJsonStreamAsAsyncIterable\";\n\nexport const createJsonStreamResponseHandler =\n  <T>(schema: Schema<T>) =>\n  ({ response }: { response: Response }) =>\n    parseJsonStreamAsAsyncIterable({\n      stream: response.body!,\n      schema,\n    });\n","import {\n  BaseUrlApiConfigurationWithDefaults,\n  PartialBaseUrlPartsApiConfigurationOptions,\n} from \"../../core/api/BaseUrlApiConfiguration\";\nimport { loadApiKey } from \"../../core/api/loadApiKey\";\n\n/**\n * Creates an API configuration for ElevenLabs API.\n * It calls the API at https://api.elevenlabs.io/v1 and uses the `ELEVENLABS_API_KEY` env variable by default.\n */\nexport class ElevenLabsApiConfiguration extends BaseUrlApiConfigurationWithDefaults {\n  constructor(\n    settings: PartialBaseUrlPartsApiConfigurationOptions & {\n      apiKey?: string;\n    } = {}\n  ) {\n    super({\n      ...settings,\n      headers: {\n        \"xi-api-key\": loadApiKey({\n          apiKey: settings.apiKey,\n          environmentVariableName: \"ELEVENLABS_API_KEY\",\n          description: \"ElevenLabs\",\n        }),\n      },\n      baseUrlDefaults: {\n        protocol: \"https\",\n        host: \"api.elevenlabs.io\",\n        port: \"443\",\n        path: \"/v1\",\n      },\n    });\n  }\n\n  get apiKey() {\n    return this.fixedHeadersValue[\"xi-api-key\"];\n  }\n}\n","import { PartialBaseUrlPartsApiConfigurationOptions } from \"../../core/api/BaseUrlApiConfiguration\";\nimport { ElevenLabsApiConfiguration } from \"./ElevenLabsApiConfiguration\";\nimport {\n  ElevenLabsSpeechModel,\n  ElevenLabsSpeechModelSettings,\n} from \"./ElevenLabsSpeechModel\";\n\n/**\n * Creates an API configuration for the ElevenLabs API.\n * It calls the API at https://api.elevenlabs.io/v1 and uses the `ELEVENLABS_API_KEY` env variable by default.\n */\nexport function Api(\n  settings: PartialBaseUrlPartsApiConfigurationOptions & {\n    apiKey?: string;\n  }\n) {\n  return new ElevenLabsApiConfiguration(settings);\n}\n\n/**\n * Synthesize speech using the ElevenLabs Text to Speech API.\n *\n * Both regular text-to-speech and full duplex text-to-speech streaming are supported.\n *\n * @see https://docs.elevenlabs.io/api-reference/text-to-speech\n * @see https://docs.elevenlabs.io/api-reference/text-to-speech-websockets\n *\n * @returns A new instance of {@link ElevenLabsSpeechModel}.\n */\nexport function SpeechGenerator(settings: ElevenLabsSpeechModelSettings) {\n  return new ElevenLabsSpeechModel(settings);\n}\n","import { z } from \"zod\";\nimport { FunctionCallOptions } from \"../../core/FunctionOptions\";\nimport { ApiConfiguration } from \"../../core/api/ApiConfiguration\";\nimport { callWithRetryAndThrottle } from \"../../core/api/callWithRetryAndThrottle\";\nimport {\n  createAudioMpegResponseHandler,\n  createTextErrorResponseHandler,\n  postJsonToApi,\n} from \"../../core/api/postToApi\";\nimport { zodSchema } from \"../../core/schema/ZodSchema\";\nimport { safeParseJSON } from \"../../core/schema/parseJSON\";\nimport { AbstractModel } from \"../../model-function/AbstractModel\";\nimport { Delta } from \"../../model-function/Delta\";\nimport {\n  SpeechGenerationModelSettings,\n  StreamingSpeechGenerationModel,\n} from \"../../model-function/generate-speech/SpeechGenerationModel\";\nimport { AsyncQueue } from \"../../util/AsyncQueue\";\nimport { createSimpleWebSocket } from \"../../util/SimpleWebSocket\";\nimport { base64ToUint8Array } from \"../../util/format/UInt8Utils\";\nimport { ElevenLabsApiConfiguration } from \"./ElevenLabsApiConfiguration\";\n\nconst elevenLabsModels = [\n  \"eleven_multilingual_v2\",\n  \"eleven_multilingual_v1\",\n  \"eleven_monolingual_v1\",\n  \"eleven_turbo_v2\",\n] as const;\n\nconst defaultModel = \"eleven_monolingual_v1\";\n\nexport interface ElevenLabsSpeechModelSettings\n  extends SpeechGenerationModelSettings {\n  api?: ApiConfiguration & {\n    apiKey: string;\n  };\n\n  voice: string;\n\n  model?:\n    | (typeof elevenLabsModels)[number]\n    // string & {} is used to enable auto-completion of literals\n    // while also allowing strings:\n    // eslint-disable-next-line @typescript-eslint/ban-types\n    | (string & {});\n\n  optimizeStreamingLatency?: 0 | 1 | 2 | 3 | 4;\n  outputFormat?:\n    | \"mp3_44100\"\n    | \"pcm_16000\"\n    | \"pcm_22050\"\n    | \"pcm_24000\"\n    | \"pcm_44100\";\n\n  voiceSettings?: {\n    stability: number;\n    similarityBoost: number;\n    style?: number;\n    useSpeakerBoost?: boolean;\n  };\n\n  generationConfig?: {\n    chunkLengthSchedule: number[];\n  };\n}\n\n/**\n * Synthesize speech using the ElevenLabs Text to Speech API.\n *\n * Both regular text-to-speech and full duplex text-to-speech streaming are supported.\n *\n * @see https://docs.elevenlabs.io/api-reference/text-to-speech\n * @see https://docs.elevenlabs.io/api-reference/text-to-speech-websockets\n */\nexport class ElevenLabsSpeechModel\n  extends AbstractModel<ElevenLabsSpeechModelSettings>\n  implements StreamingSpeechGenerationModel<ElevenLabsSpeechModelSettings>\n{\n  constructor(settings: ElevenLabsSpeechModelSettings) {\n    super({ settings });\n  }\n\n  readonly provider = \"elevenlabs\";\n\n  get modelName() {\n    return this.settings.voice;\n  }\n\n  private async callAPI(\n    text: string,\n    callOptions: FunctionCallOptions\n  ): Promise<Uint8Array> {\n    const api = this.settings.api ?? new ElevenLabsApiConfiguration();\n    const abortSignal = callOptions?.run?.abortSignal;\n\n    return callWithRetryAndThrottle({\n      retry: api.retry,\n      throttle: api.throttle,\n      call: async () =>\n        postJsonToApi({\n          url: api.assembleUrl(\n            `/text-to-speech/${this.settings.voice}${assembleQuery({\n              optimize_streaming_latency:\n                this.settings.optimizeStreamingLatency,\n              output_format: this.settings.outputFormat,\n            })}`\n          ),\n          headers: api.headers({\n            functionType: callOptions.functionType,\n            functionId: callOptions.functionId,\n            run: callOptions.run,\n            callId: callOptions.callId,\n          }),\n          body: {\n            text,\n            model_id: this.settings.model ?? defaultModel,\n            voice_settings: toApiVoiceSettings(this.settings.voiceSettings),\n          },\n          failedResponseHandler: createTextErrorResponseHandler(),\n          successfulResponseHandler: createAudioMpegResponseHandler(),\n          abortSignal,\n        }),\n    });\n  }\n\n  get settingsForEvent(): Partial<ElevenLabsSpeechModelSettings> {\n    return {\n      model: this.settings.model,\n      voice: this.settings.voice,\n      voiceSettings: this.settings.voiceSettings,\n    };\n  }\n\n  doGenerateSpeechStandard(text: string, options: FunctionCallOptions) {\n    return this.callAPI(text, options);\n  }\n\n  async doGenerateSpeechStreamDuplex(\n    textStream: AsyncIterable<string>\n    // options?: FunctionOptions | undefined\n  ): Promise<AsyncIterable<Delta<Uint8Array>>> {\n    const queue = new AsyncQueue<Delta<Uint8Array>>();\n\n    const model = this.settings.model ?? defaultModel;\n    const socket = await createSimpleWebSocket(\n      `wss://api.elevenlabs.io/v1/text-to-speech/${\n        this.settings.voice\n      }/stream-input${assembleQuery({\n        model_id: model,\n        optimize_streaming_latency: this.settings.optimizeStreamingLatency,\n        output_format: this.settings.outputFormat,\n      })}`\n    );\n\n    socket.onopen = async () => {\n      const api = this.settings.api ?? new ElevenLabsApiConfiguration();\n\n      // send begin-of-stream (BOS) message:\n      socket.send(\n        JSON.stringify({\n          // The JS WebSocket API does not support authorization headers, so we send the API key in the BOS message.\n          // See https://stackoverflow.com/questions/4361173/http-headers-in-websockets-client-api\n          xi_api_key: api.apiKey,\n          text: \" \", // first message\n          voice_settings: toApiVoiceSettings(this.settings.voiceSettings),\n          generation_config: toGenerationConfig(this.settings.generationConfig),\n        })\n      );\n\n      // send text in chunks:\n      let textBuffer = \"\";\n      for await (const textDelta of textStream) {\n        textBuffer += textDelta;\n\n        // using \". \" as separator: sending in full sentences improves the quality\n        // of the audio output significantly.\n        const separator = textBuffer.lastIndexOf(\". \");\n\n        if (separator === -1) {\n          continue;\n        }\n\n        const textToProcess = textBuffer.slice(0, separator);\n        textBuffer = textBuffer.slice(separator + 1);\n\n        socket.send(\n          JSON.stringify({\n            text: textToProcess,\n            try_trigger_generation: true,\n          })\n        );\n      }\n\n      // send remaining text:\n      if (textBuffer.length > 0) {\n        socket.send(\n          JSON.stringify({\n            text: `${textBuffer} `, // append space\n            try_trigger_generation: true,\n          })\n        );\n      }\n\n      // send end-of-stream (EOS) message:\n      socket.send(JSON.stringify({ text: \"\" }));\n    };\n\n    socket.onmessage = (event) => {\n      const parseResult = safeParseJSON({\n        text: event.data,\n        schema: zodSchema(streamingResponseSchema),\n      });\n\n      if (!parseResult.success) {\n        queue.push({ type: \"error\", error: parseResult.error });\n        return;\n      }\n\n      const response = parseResult.value;\n\n      if (\"error\" in response) {\n        queue.push({ type: \"error\", error: response });\n        return;\n      }\n\n      if (!response.isFinal) {\n        queue.push({\n          type: \"delta\",\n          deltaValue: base64ToUint8Array(response.audio),\n        });\n      }\n    };\n\n    socket.onerror = (error) => {\n      queue.push({ type: \"error\", error });\n    };\n\n    socket.onclose = () => {\n      queue.close();\n    };\n\n    return queue;\n  }\n\n  withSettings(additionalSettings: Partial<ElevenLabsSpeechModelSettings>) {\n    return new ElevenLabsSpeechModel({\n      ...this.settings,\n      ...additionalSettings,\n    }) as this;\n  }\n}\n\nconst streamingResponseSchema = z.union([\n  z.object({\n    audio: z.string(),\n    isFinal: z.literal(false).nullable(),\n    normalizedAlignment: z\n      .object({\n        chars: z.array(z.string()),\n        charStartTimesMs: z.array(z.number()),\n        charDurationsMs: z.array(z.number()),\n      })\n      .nullable(),\n  }),\n  z.object({\n    isFinal: z.literal(true),\n  }),\n  z.object({\n    message: z.string(),\n    error: z.string(),\n    code: z.number(),\n  }),\n]);\n\nfunction assembleQuery(parameters: Record<string, unknown | undefined>) {\n  let query = \"\";\n  let hasQuestionMark = false;\n\n  for (const [key, value] of Object.entries(parameters)) {\n    if (value == null) {\n      continue;\n    }\n\n    if (!hasQuestionMark) {\n      query += \"?\";\n      hasQuestionMark = true;\n    } else {\n      query += \"&\";\n    }\n\n    query += `${key}=${value}`;\n  }\n\n  return query;\n}\n\nfunction toApiVoiceSettings(\n  voiceSettings?: ElevenLabsSpeechModelSettings[\"voiceSettings\"]\n) {\n  return voiceSettings != null\n    ? {\n        stability: voiceSettings.stability,\n        similarity_boost: voiceSettings.similarityBoost,\n        style: voiceSettings.style,\n        use_speaker_boost: voiceSettings.useSpeakerBoost,\n      }\n    : undefined;\n}\n\nfunction toGenerationConfig(\n  generationConfig?: ElevenLabsSpeechModelSettings[\"generationConfig\"]\n) {\n  return generationConfig != null\n    ? { chunk_length_schedule: generationConfig.chunkLengthSchedule }\n    : undefined;\n}\n","import * as Runtime from \"./detectRuntime\";\n\nexport interface SimpleWebSocket {\n  send(data: string): void;\n  onmessage: ((event: MessageEvent) => void) | null;\n  onopen: ((event: Event) => void) | null;\n  onclose: ((event: CloseEvent) => void) | null;\n  onerror: ((event: Event) => void) | null;\n  close(code?: number, reason?: string): void;\n}\n\n/**\n * Creates a simplified websocket connection. This function works in both Node.js and browser.\n */\nexport async function createSimpleWebSocket(\n  url: string\n): Promise<SimpleWebSocket> {\n  switch (Runtime.detectRuntime()) {\n    case \"vercel-edge\":\n    case \"cloudflare-workers\":\n    case \"browser\": {\n      return new WebSocket(url) as SimpleWebSocket;\n    }\n\n    case \"node\": {\n      // Use ws library (for Node.js).\n      // Note: we try both import and require to support both ESM and CJS.\n\n      // eslint-disable-next-line @typescript-eslint/no-explicit-any\n      let WebSocket: any;\n\n      try {\n        WebSocket = (await import(\"ws\")).default;\n      } catch (error) {\n        try {\n          // eslint-disable-next-line @typescript-eslint/no-var-requires\n          WebSocket = require(\"ws\");\n        } catch (error) {\n          throw new Error(`Failed to load 'ws' module dynamically.`);\n        }\n      }\n\n      return new WebSocket(url) as SimpleWebSocket;\n    }\n\n    default: {\n      throw new Error(\"Unknown runtime\");\n    }\n  }\n}\n","import {\n  BaseUrlApiConfigurationWithDefaults,\n  PartialBaseUrlPartsApiConfigurationOptions,\n} from \"../../core/api/BaseUrlApiConfiguration\";\nimport { loadApiKey } from \"../../core/api/loadApiKey\";\n\n/**\n * Creates an API configuration for the HuggingFace API.\n * It calls the API at https://api-inference.huggingface.co/models and uses the `HUGGINGFACE_API_KEY` env variable by default.\n */\nexport class HuggingFaceApiConfiguration extends BaseUrlApiConfigurationWithDefaults {\n  constructor(\n    settings: PartialBaseUrlPartsApiConfigurationOptions & {\n      apiKey?: string;\n    } = {}\n  ) {\n    super({\n      ...settings,\n      headers: {\n        Authorization: `Bearer ${loadApiKey({\n          apiKey: settings.apiKey,\n          environmentVariableName: \"HUGGINGFACE_API_KEY\",\n          description: \"Hugging Face\",\n        })}`,\n      },\n      baseUrlDefaults: {\n        protocol: \"https\",\n        host: \"api-inference.huggingface.co\",\n        port: \"443\",\n        path: \"/models\",\n      },\n    });\n  }\n}\n","import { z } from \"zod\";\nimport { createJsonErrorResponseHandler } from \"../../core/api/postToApi\";\nimport { zodSchema } from \"../../core/schema/ZodSchema\";\n\nconst huggingFaceErrorDataSchema = z.object({\n  error: z.array(z.string()).or(z.string()),\n});\n\nexport type HuggingFaceErrorData = z.infer<typeof huggingFaceErrorDataSchema>;\n\nexport const failedHuggingFaceCallResponseHandler =\n  createJsonErrorResponseHandler({\n    errorSchema: zodSchema(huggingFaceErrorDataSchema),\n    errorToMessage: (data) =>\n      typeof data.error === \"string\" ? data.error : data.error.join(\"\\n\\n\"),\n  });\n","import { PartialBaseUrlPartsApiConfigurationOptions } from \"../../core/api/BaseUrlApiConfiguration\";\nimport { HuggingFaceApiConfiguration } from \"./HuggingFaceApiConfiguration\";\nimport {\n  HuggingFaceTextEmbeddingModel,\n  HuggingFaceTextEmbeddingModelSettings,\n} from \"./HuggingFaceTextEmbeddingModel\";\nimport {\n  HuggingFaceTextGenerationModel,\n  HuggingFaceTextGenerationModelSettings,\n} from \"./HuggingFaceTextGenerationModel\";\n\n/**\n * Creates an API configuration for the HuggingFace API.\n * It calls the API at https://api-inference.huggingface.co/models and uses the `HUGGINGFACE_API_KEY` env variable by default.\n */\nexport function Api(\n  settings: PartialBaseUrlPartsApiConfigurationOptions & {\n    apiKey?: string;\n  }\n) {\n  return new HuggingFaceApiConfiguration(settings);\n}\n\n/**\n * Create a text generation model that calls a Hugging Face Inference API Text Generation Task.\n *\n * @see https://huggingface.co/docs/api-inference/detailed_parameters#text-generation-task\n *\n * @example\n * const model = huggingface.TextGenerator({\n *   model: \"tiiuae/falcon-7b\",\n *   temperature: 0.7,\n *   maxGenerationTokens: 500,\n *   retry: retryWithExponentialBackoff({ maxTries: 5 }),\n * });\n *\n * const text = await generateText(\n *   model,\n *   \"Write a short story about a robot learning to love:\\n\\n\"\n * );\n *\n * @returns A new instance of {@link HuggingFaceTextGenerationModel}.\n */\nexport function TextGenerator(\n  settings: HuggingFaceTextGenerationModelSettings\n) {\n  return new HuggingFaceTextGenerationModel(settings);\n}\n\n/**\n * Create a text embedding model that calls a Hugging Face Inference API Feature Extraction Task.\n *\n * @see https://huggingface.co/docs/api-inference/detailed_parameters#feature-extraction-task\n *\n * @example\n * const model = huggingface.TextEmbedder({\n *   model: \"intfloat/e5-base-v2\",\n *   maxTexstsPerCall: 5,\n *   retry: retryWithExponentialBackoff({ maxTries: 5 }),\n * });\n *\n * const embeddings = await embedMany(\n *   model,\n *   [\n *     \"At first, Nox didn't know what to do with the pup.\",\n *     \"He keenly observed and absorbed everything around him, from the birds in the sky to the trees in the forest.\",\n *   ]\n * );\n *\n * @returns A new instance of {@link HuggingFaceTextEmbeddingModel}.\n */\nexport function TextEmbedder(settings: HuggingFaceTextEmbeddingModelSettings) {\n  return new HuggingFaceTextEmbeddingModel(settings);\n}\n","import { z } from \"zod\";\nimport { FunctionCallOptions } from \"../../core/FunctionOptions\";\nimport { ApiConfiguration } from \"../../core/api/ApiConfiguration\";\nimport { callWithRetryAndThrottle } from \"../../core/api/callWithRetryAndThrottle\";\nimport {\n  createJsonResponseHandler,\n  postJsonToApi,\n} from \"../../core/api/postToApi\";\nimport { zodSchema } from \"../../core/schema/ZodSchema\";\nimport { AbstractModel } from \"../../model-function/AbstractModel\";\nimport {\n  EmbeddingModel,\n  EmbeddingModelSettings,\n} from \"../../model-function/embed/EmbeddingModel\";\nimport { HuggingFaceApiConfiguration } from \"./HuggingFaceApiConfiguration\";\nimport { failedHuggingFaceCallResponseHandler } from \"./HuggingFaceError\";\n\nexport interface HuggingFaceTextEmbeddingModelSettings\n  extends EmbeddingModelSettings {\n  api?: ApiConfiguration;\n\n  model: string;\n\n  maxValuesPerCall?: number;\n  dimensions?: number;\n\n  options?: {\n    useCache?: boolean;\n    waitForModel?: boolean;\n  };\n}\n\n/**\n * Create a text embedding model that calls a Hugging Face Inference API Feature Extraction Task.\n *\n * @see https://huggingface.co/docs/api-inference/detailed_parameters#feature-extraction-task\n *\n * @example\n * const model = new HuggingFaceTextGenerationModel({\n *   model: \"intfloat/e5-base-v2\",\n *   maxTexstsPerCall: 5,\n *   retry: retryWithExponentialBackoff({ maxTries: 5 }),\n * });\n *\n * const embeddings = await embedMany(\n *   model,\n *   [\n *     \"At first, Nox didn't know what to do with the pup.\",\n *     \"He keenly observed and absorbed everything around him, from the birds in the sky to the trees in the forest.\",\n *   ]\n * );\n */\nexport class HuggingFaceTextEmbeddingModel\n  extends AbstractModel<HuggingFaceTextEmbeddingModelSettings>\n  implements EmbeddingModel<string, HuggingFaceTextEmbeddingModelSettings>\n{\n  constructor(settings: HuggingFaceTextEmbeddingModelSettings) {\n    super({ settings });\n\n    // There is no limit documented in the HuggingFace API. Use 1024 as a reasonable default.\n    this.maxValuesPerCall = settings.maxValuesPerCall ?? 1024;\n    this.dimensions = settings.dimensions;\n  }\n\n  readonly provider = \"huggingface\";\n  get modelName() {\n    return this.settings.model;\n  }\n\n  readonly maxValuesPerCall;\n  readonly isParallelizable = true;\n\n  readonly contextWindowSize = undefined;\n  readonly dimensions;\n\n  readonly tokenizer = undefined;\n\n  async callAPI(\n    texts: Array<string>,\n    callOptions: FunctionCallOptions\n  ): Promise<HuggingFaceTextEmbeddingResponse> {\n    if (texts.length > this.maxValuesPerCall) {\n      throw new Error(\n        `The HuggingFace feature extraction API is configured to only support ${this.maxValuesPerCall} texts per API call.`\n      );\n    }\n\n    const api = this.settings.api ?? new HuggingFaceApiConfiguration();\n    const abortSignal = callOptions?.run?.abortSignal;\n\n    return callWithRetryAndThrottle({\n      retry: api.retry,\n      throttle: api.throttle,\n      call: async () =>\n        postJsonToApi({\n          url: api.assembleUrl(`/${this.settings.model}`),\n          headers: api.headers({\n            functionType: callOptions.functionType,\n            functionId: callOptions.functionId,\n            run: callOptions.run,\n            callId: callOptions.callId,\n          }),\n          body: {\n            inputs: texts,\n            options: {\n              use_cache: this.settings.options?.useCache ?? true,\n              wait_for_model: this.settings.options?.waitForModel ?? true,\n            },\n          },\n          failedResponseHandler: failedHuggingFaceCallResponseHandler,\n          successfulResponseHandler: createJsonResponseHandler(\n            zodSchema(huggingFaceTextEmbeddingResponseSchema)\n          ),\n          abortSignal,\n        }),\n    });\n  }\n\n  get settingsForEvent(): Partial<HuggingFaceTextEmbeddingModelSettings> {\n    return {\n      dimensions: this.settings.dimensions,\n      options: this.settings.options,\n    };\n  }\n\n  readonly countPromptTokens = undefined;\n\n  async doEmbedValues(texts: string[], options: FunctionCallOptions) {\n    const rawResponse = await this.callAPI(texts, options);\n\n    return {\n      rawResponse,\n      embeddings: rawResponse,\n    };\n  }\n\n  withSettings(\n    additionalSettings: Partial<HuggingFaceTextEmbeddingModelSettings>\n  ) {\n    return new HuggingFaceTextEmbeddingModel(\n      Object.assign({}, this.settings, additionalSettings)\n    ) as this;\n  }\n}\n\nconst huggingFaceTextEmbeddingResponseSchema = z.array(z.array(z.number()));\n\nexport type HuggingFaceTextEmbeddingResponse = z.infer<\n  typeof huggingFaceTextEmbeddingResponseSchema\n>;\n","import { z } from \"zod\";\nimport { FunctionCallOptions } from \"../../core/FunctionOptions\";\nimport { ApiConfiguration } from \"../../core/api/ApiConfiguration\";\nimport { callWithRetryAndThrottle } from \"../../core/api/callWithRetryAndThrottle\";\nimport {\n  createJsonResponseHandler,\n  postJsonToApi,\n} from \"../../core/api/postToApi\";\nimport { zodSchema } from \"../../core/schema/ZodSchema\";\nimport { validateTypes } from \"../../core/schema/validateTypes\";\nimport { AbstractModel } from \"../../model-function/AbstractModel\";\nimport { PromptTemplateTextGenerationModel } from \"../../model-function/generate-text/PromptTemplateTextGenerationModel\";\nimport {\n  TextGenerationModel,\n  TextGenerationModelSettings,\n  textGenerationModelProperties,\n} from \"../../model-function/generate-text/TextGenerationModel\";\nimport { TextGenerationPromptTemplate } from \"../../model-function/generate-text/TextGenerationPromptTemplate\";\nimport { HuggingFaceApiConfiguration } from \"./HuggingFaceApiConfiguration\";\nimport { failedHuggingFaceCallResponseHandler } from \"./HuggingFaceError\";\n\nexport interface HuggingFaceTextGenerationModelSettings\n  extends TextGenerationModelSettings {\n  api?: ApiConfiguration;\n\n  model: string;\n\n  topK?: number;\n  topP?: number;\n  temperature?: number;\n  repetitionPenalty?: number;\n  maxTime?: number;\n  doSample?: boolean;\n}\n\n/**\n * Create a text generation model that calls a Hugging Face Inference API Text Generation Task.\n *\n * @see https://huggingface.co/docs/api-inference/detailed_parameters#text-generation-task\n *\n * @example\n * const model = new HuggingFaceTextGenerationModel({\n *   model: \"tiiuae/falcon-7b\",\n *   temperature: 0.7,\n *   maxGenerationTokens: 500,\n *   retry: retryWithExponentialBackoff({ maxTries: 5 }),\n * });\n *\n * const text = await generateText(\n *   model,\n *   \"Write a short story about a robot learning to love:\\n\\n\"\n * );\n */\nexport class HuggingFaceTextGenerationModel\n  extends AbstractModel<HuggingFaceTextGenerationModelSettings>\n  implements\n    TextGenerationModel<string, HuggingFaceTextGenerationModelSettings>\n{\n  constructor(settings: HuggingFaceTextGenerationModelSettings) {\n    super({ settings });\n  }\n\n  readonly provider = \"huggingface\";\n  get modelName() {\n    return this.settings.model;\n  }\n\n  readonly contextWindowSize = undefined;\n  readonly tokenizer = undefined;\n  readonly countPromptTokens = undefined;\n\n  async callAPI(\n    prompt: string,\n    callOptions: FunctionCallOptions\n  ): Promise<HuggingFaceTextGenerationResponse> {\n    const api = this.settings.api ?? new HuggingFaceApiConfiguration();\n    const abortSignal = callOptions?.run?.abortSignal;\n\n    return callWithRetryAndThrottle({\n      retry: api.retry,\n      throttle: api.throttle,\n      call: async () =>\n        postJsonToApi({\n          url: api.assembleUrl(`/${this.settings.model}`),\n          headers: api.headers({\n            functionType: callOptions.functionType,\n            functionId: callOptions.functionId,\n            run: callOptions.run,\n            callId: callOptions.callId,\n          }),\n          body: {\n            inputs: prompt,\n            top_k: this.settings.topK,\n            top_p: this.settings.topP,\n            temperature: this.settings.temperature,\n            repetition_penalty: this.settings.repetitionPenalty,\n            max_new_tokens: this.settings.maxGenerationTokens,\n            max_time: this.settings.maxTime,\n            num_return_sequences: this.settings.numberOfGenerations,\n            do_sample: this.settings.doSample,\n            options: {\n              use_cache: true,\n              wait_for_model: true,\n            },\n          },\n          failedResponseHandler: failedHuggingFaceCallResponseHandler,\n          successfulResponseHandler: createJsonResponseHandler(\n            zodSchema(huggingFaceTextGenerationResponseSchema)\n          ),\n          abortSignal,\n        }),\n    });\n  }\n\n  get settingsForEvent(): Partial<HuggingFaceTextGenerationModelSettings> {\n    const eventSettingProperties: Array<string> = [\n      ...textGenerationModelProperties,\n\n      \"topK\",\n      \"topP\",\n      \"temperature\",\n      \"repetitionPenalty\",\n      \"maxTime\",\n      \"doSample\",\n    ] satisfies (keyof HuggingFaceTextGenerationModelSettings)[];\n\n    return Object.fromEntries(\n      Object.entries(this.settings).filter(([key]) =>\n        eventSettingProperties.includes(key)\n      )\n    );\n  }\n\n  async doGenerateTexts(prompt: string, options: FunctionCallOptions) {\n    return this.processTextGenerationResponse(\n      await this.callAPI(prompt, options)\n    );\n  }\n\n  restoreGeneratedTexts(rawResponse: unknown) {\n    return this.processTextGenerationResponse(\n      validateTypes({\n        value: rawResponse,\n        schema: zodSchema(huggingFaceTextGenerationResponseSchema),\n      })\n    );\n  }\n\n  processTextGenerationResponse(\n    rawResponse: HuggingFaceTextGenerationResponse\n  ) {\n    return {\n      rawResponse,\n      textGenerationResults: rawResponse.map((response) => ({\n        text: response.generated_text,\n        finishReason: \"unknown\" as const,\n      })),\n    };\n  }\n\n  withJsonOutput(): this {\n    return this;\n  }\n\n  withPromptTemplate<INPUT_PROMPT>(\n    promptTemplate: TextGenerationPromptTemplate<INPUT_PROMPT, string>\n  ): PromptTemplateTextGenerationModel<\n    INPUT_PROMPT,\n    string,\n    HuggingFaceTextGenerationModelSettings,\n    this\n  > {\n    return new PromptTemplateTextGenerationModel({\n      model: this, // stop tokens are not supported by this model\n      promptTemplate,\n    });\n  }\n\n  withSettings(\n    additionalSettings: Partial<HuggingFaceTextGenerationModelSettings>\n  ) {\n    return new HuggingFaceTextGenerationModel(\n      Object.assign({}, this.settings, additionalSettings)\n    ) as this;\n  }\n}\n\nconst huggingFaceTextGenerationResponseSchema = z.array(\n  z.object({\n    generated_text: z.string(),\n  })\n);\n\nexport type HuggingFaceTextGenerationResponse = z.infer<\n  typeof huggingFaceTextGenerationResponseSchema\n>;\n","import {\n  BaseUrlApiConfigurationWithDefaults,\n  PartialBaseUrlPartsApiConfigurationOptions,\n} from \"../../core/api/BaseUrlApiConfiguration\";\n\n/**\n * Creates an API configuration for the Llama.cpp server.\n * It calls the API at http://127.0.0.1:8080 by default.\n */\nexport class LlamaCppApiConfiguration extends BaseUrlApiConfigurationWithDefaults {\n  constructor(settings: PartialBaseUrlPartsApiConfigurationOptions = {}) {\n    super({\n      ...settings,\n      baseUrlDefaults: {\n        protocol: \"http\",\n        host: \"127.0.0.1\",\n        port: \"8080\",\n        path: \"\",\n      },\n    });\n  }\n}\n","import { z } from \"zod\";\nimport { FunctionCallOptions } from \"../../core/FunctionOptions\";\nimport { ApiConfiguration } from \"../../core/api/ApiConfiguration\";\nimport { callWithRetryAndThrottle } from \"../../core/api/callWithRetryAndThrottle\";\nimport {\n  ResponseHandler,\n  createJsonResponseHandler,\n  postJsonToApi,\n} from \"../../core/api/postToApi\";\nimport { JsonSchemaProducer } from \"../../core/schema/JsonSchemaProducer\";\nimport { Schema } from \"../../core/schema/Schema\";\nimport { zodSchema } from \"../../core/schema/ZodSchema\";\nimport { parseJSON } from \"../../core/schema/parseJSON\";\nimport { validateTypes } from \"../../core/schema/validateTypes\";\nimport { AbstractModel } from \"../../model-function/AbstractModel\";\nimport { Delta } from \"../../model-function/Delta\";\nimport {\n  FlexibleObjectFromTextPromptTemplate,\n  ObjectFromTextPromptTemplate,\n} from \"../../model-function/generate-object/ObjectFromTextPromptTemplate\";\nimport { ObjectFromTextStreamingModel } from \"../../model-function/generate-object/ObjectFromTextStreamingModel\";\nimport { PromptTemplateTextStreamingModel } from \"../../model-function/generate-text/PromptTemplateTextStreamingModel\";\nimport {\n  TextGenerationModelSettings,\n  TextStreamingBaseModel,\n  TextStreamingModel,\n  textGenerationModelProperties,\n} from \"../../model-function/generate-text/TextGenerationModel\";\nimport { TextGenerationPromptTemplate } from \"../../model-function/generate-text/TextGenerationPromptTemplate\";\nimport { ChatPrompt } from \"../../model-function/generate-text/prompt-template/ChatPrompt\";\nimport { InstructionPrompt } from \"../../model-function/generate-text/prompt-template/InstructionPrompt\";\nimport { TextGenerationPromptTemplateProvider } from \"../../model-function/generate-text/prompt-template/PromptTemplateProvider\";\nimport { AsyncQueue } from \"../../util/AsyncQueue\";\nimport { parseEventSourceStream } from \"../../util/streaming/parseEventSourceStream\";\nimport { LlamaCppApiConfiguration } from \"./LlamaCppApiConfiguration\";\nimport { failedLlamaCppCallResponseHandler } from \"./LlamaCppError\";\nimport { Text } from \"./LlamaCppPrompt\";\nimport { LlamaCppTokenizer } from \"./LlamaCppTokenizer\";\nimport { convertJsonSchemaToGBNF } from \"./convertJsonSchemaToGBNF\";\n\nexport interface LlamaCppCompletionModelSettings<\n  CONTEXT_WINDOW_SIZE extends number | undefined,\n> extends TextGenerationModelSettings {\n  api?: ApiConfiguration;\n\n  /**\n   * Specify the context window size of the model that you have loaded in your\n   * Llama.cpp server.\n   */\n  contextWindowSize?: CONTEXT_WINDOW_SIZE;\n\n  /**\n   * Adjust the randomness of the generated text (default: 0.8).\n   */\n  temperature?: number;\n\n  /**\n   * Limit the next token selection to the K most probable tokens (default: 40).\n   */\n  topK?: number;\n\n  /**\n   * Limit the next token selection to a subset of tokens with a cumulative probability above a threshold P (default: 0.95).\n   */\n  topP?: number;\n\n  /**\n   * The minimum probability for a token to be considered, relative to the probability of the most likely token (default: 0.05).\n   */\n  minP?: number;\n\n  /**\n   * Specify the number of tokens from the prompt to retain when the context size is exceeded\n   * and tokens need to be discarded. By default, this value is set to 0 (meaning no tokens\n   * are kept). Use -1 to retain all tokens from the prompt.\n   */\n  nKeep?: number;\n\n  /**\n   * Enable tail free sampling with parameter z (default: 1.0, 1.0 = disabled).\n   */\n  tfsZ?: number;\n\n  /**\n   * Enable locally typical sampling with parameter p (default: 1.0, 1.0 = disabled).\n   */\n  typicalP?: number;\n\n  /**\n   * Control the repetition of token sequences in the generated text (default: 1.1).\n   */\n  repeatPenalty?: number;\n\n  /**\n   * Last n tokens to consider for penalizing repetition (default: 64, 0 = disabled, -1 = ctx-size).\n   */\n  repeatLastN?: number;\n\n  /**\n   * Penalize newline tokens when applying the repeat penalty (default: true).\n   */\n  penalizeNl?: boolean;\n\n  /**\n   * Repeat alpha presence penalty (default: 0.0, 0.0 = disabled).\n   */\n  presencePenalty?: number;\n\n  /**\n   * Repeat alpha frequency penalty (default: 0.0, 0.0 = disabled).\n   */\n  frequencyPenalty?: number;\n\n  /**\n   * This will replace the prompt for the purpose of the penalty evaluation.\n   * Can be either null, a string or an array of numbers representing tokens\n   * (default: null = use the original prompt).\n   */\n  penaltyPrompt?: string | number[];\n\n  /**\n   * Enable Mirostat sampling, controlling perplexity during text generation\n   * (default: 0, 0 = disabled, 1 = Mirostat, 2 = Mirostat 2.0).\n   */\n  mirostat?: number;\n\n  /**\n   * Set the Mirostat target entropy, parameter tau (default: 5.0).\n   */\n  mirostatTau?: number;\n\n  /**\n   * Set the Mirostat learning rate, parameter eta (default: 0.1).\n   */\n  mirostatEta?: number;\n\n  /**\n   * Set grammar for grammar-based sampling (default: no grammar)\n   *\n   * @see https://github.com/ggerganov/llama.cpp/blob/master/grammars/README.md\n   */\n  grammar?: string;\n\n  /**\n   * Set the random number generator (RNG) seed\n   * (default: -1, -1 = random seed).\n   */\n  seed?: number;\n\n  /**\n   * Ignore end of stream token and continue generating (default: false).\n   */\n  ignoreEos?: boolean;\n\n  /**\n   * Modify the likelihood of a token appearing in the generated text completion.\n   * For example, use \"logit_bias\": [[15043,1.0]] to increase the likelihood of the token\n   * 'Hello', or \"logit_bias\": [[15043,-1.0]] to decrease its likelihood.\n   * Setting the value to false, \"logit_bias\": [[15043,false]] ensures that the token Hello is\n   * never produced (default: []).\n   */\n  logitBias?: Array<[number, number | false]>;\n\n  /**\n   * If greater than 0, the response also contains the probabilities of top N tokens\n   * for each generated token (default: 0)\n   */\n  nProbs?: number;\n\n  /**\n   * Save the prompt and generation for avoid reprocess entire prompt if a part of this isn't change (default: false)\n   */\n  cachePrompt?: boolean;\n\n  /**\n   * Assign the completion task to an specific slot.\n   * If is -1 the task will be assigned to a Idle slot (default: -1)\n   */\n  slotId?: number;\n\n  /**\n   * Prompt template provider that is used when calling `.withTextPrompt()`, `withInstructionPrompt()` or `withChatPrompt()`.\n   */\n  promptTemplate?: TextGenerationPromptTemplateProvider<LlamaCppCompletionPrompt>;\n}\n\nexport interface LlamaCppCompletionPrompt {\n  /**\n   * Text prompt. Images can be included through references such as `[img-ID]`, e.g. `[img-1]`.\n   */\n  text: string;\n\n  /**\n   * Maps image id to image base data.\n   */\n  images?: Record<number, string>;\n}\n\nexport class LlamaCppCompletionModel<\n    CONTEXT_WINDOW_SIZE extends number | undefined,\n  >\n  extends AbstractModel<LlamaCppCompletionModelSettings<CONTEXT_WINDOW_SIZE>>\n  implements\n    TextStreamingBaseModel<\n      LlamaCppCompletionPrompt,\n      LlamaCppCompletionModelSettings<CONTEXT_WINDOW_SIZE>\n    >\n{\n  constructor(\n    settings: LlamaCppCompletionModelSettings<CONTEXT_WINDOW_SIZE> = {}\n  ) {\n    super({ settings });\n    this.tokenizer = new LlamaCppTokenizer(this.settings.api);\n  }\n\n  readonly provider = \"llamacpp\";\n  get modelName() {\n    return null;\n  }\n\n  get contextWindowSize(): CONTEXT_WINDOW_SIZE {\n    return this.settings.contextWindowSize as CONTEXT_WINDOW_SIZE;\n  }\n\n  readonly tokenizer: LlamaCppTokenizer;\n\n  async callAPI<RESPONSE>(\n    prompt: LlamaCppCompletionPrompt,\n    callOptions: FunctionCallOptions,\n    options: {\n      responseFormat: LlamaCppCompletionResponseFormatType<RESPONSE>;\n    }\n  ): Promise<RESPONSE> {\n    const api = this.settings.api ?? new LlamaCppApiConfiguration();\n    const responseFormat = options.responseFormat;\n    const abortSignal = callOptions.run?.abortSignal;\n\n    return callWithRetryAndThrottle({\n      retry: api.retry,\n      throttle: api.throttle,\n      call: async () =>\n        postJsonToApi({\n          url: api.assembleUrl(`/completion`),\n          headers: api.headers({\n            functionType: callOptions.functionType,\n            functionId: callOptions.functionId,\n            run: callOptions.run,\n            callId: callOptions.callId,\n          }),\n          body: {\n            stream: responseFormat.stream,\n            prompt: prompt.text,\n            image_data:\n              prompt.images != null\n                ? Object.entries(prompt.images).map(([id, data]) => ({\n                    id: +id,\n                    data,\n                  }))\n                : undefined,\n            temperature: this.settings.temperature,\n            top_k: this.settings.topK,\n            top_p: this.settings.topP,\n            min_p: this.settings.minP,\n            n_predict: this.settings.maxGenerationTokens,\n            n_keep: this.settings.nKeep,\n            stop: this.settings.stopSequences,\n            tfs_z: this.settings.tfsZ,\n            typical_p: this.settings.typicalP,\n            repeat_penalty: this.settings.repeatPenalty,\n            repeat_last_n: this.settings.repeatLastN,\n            penalize_nl: this.settings.penalizeNl,\n            presence_penalty: this.settings.presencePenalty,\n            frequency_penalty: this.settings.frequencyPenalty,\n            penalty_prompt: this.settings.penaltyPrompt,\n            mirostat: this.settings.mirostat,\n            mirostat_tau: this.settings.mirostatTau,\n            mirostat_eta: this.settings.mirostatEta,\n            grammar: this.settings.grammar,\n            seed: this.settings.seed,\n            ignore_eos: this.settings.ignoreEos,\n            logit_bias: this.settings.logitBias,\n            n_probs: this.settings.nProbs,\n            cache_prompt: this.settings.cachePrompt,\n            slot_id: this.settings.slotId,\n          },\n          failedResponseHandler: failedLlamaCppCallResponseHandler,\n          successfulResponseHandler: responseFormat.handler,\n          abortSignal,\n        }),\n    });\n  }\n\n  get settingsForEvent(): Partial<\n    LlamaCppCompletionModelSettings<CONTEXT_WINDOW_SIZE>\n  > {\n    const eventSettingProperties: Array<string> = [\n      ...textGenerationModelProperties,\n\n      \"contextWindowSize\",\n      \"temperature\",\n      \"topK\",\n      \"topP\",\n      \"minP\",\n      \"nKeep\",\n      \"tfsZ\",\n      \"typicalP\",\n      \"repeatPenalty\",\n      \"repeatLastN\",\n      \"penalizeNl\",\n      \"presencePenalty\",\n      \"frequencyPenalty\",\n      \"penaltyPrompt\",\n      \"mirostat\",\n      \"mirostatTau\",\n      \"mirostatEta\",\n      \"grammar\",\n      \"seed\",\n      \"ignoreEos\",\n      \"logitBias\",\n      \"nProbs\",\n      \"cachePrompt\",\n      \"slotId\",\n    ] satisfies (keyof LlamaCppCompletionModelSettings<CONTEXT_WINDOW_SIZE>)[];\n\n    return Object.fromEntries(\n      Object.entries(this.settings).filter(([key]) =>\n        eventSettingProperties.includes(key)\n      )\n    );\n  }\n\n  async countPromptTokens(prompt: LlamaCppCompletionPrompt): Promise<number> {\n    const tokens = await this.tokenizer.tokenize(prompt.text);\n    return tokens.length;\n  }\n\n  async doGenerateTexts(\n    prompt: LlamaCppCompletionPrompt,\n    options: FunctionCallOptions\n  ) {\n    return this.processTextGenerationResponse(\n      await this.callAPI(prompt, options, {\n        responseFormat: LlamaCppCompletionResponseFormat.json,\n      })\n    );\n  }\n\n  restoreGeneratedTexts(rawResponse: unknown) {\n    return this.processTextGenerationResponse(\n      validateTypes({\n        value: rawResponse,\n        schema: zodSchema(llamaCppTextGenerationResponseSchema),\n      })\n    );\n  }\n\n  processTextGenerationResponse(rawResponse: LlamaCppTextGenerationResponse) {\n    return {\n      rawResponse,\n      textGenerationResults: [\n        {\n          text: rawResponse.content,\n          finishReason:\n            rawResponse.stopped_eos || rawResponse.stopped_word\n              ? (\"stop\" as const)\n              : rawResponse.stopped_limit\n                ? (\"length\" as const)\n                : (\"unknown\" as const),\n        },\n      ],\n      usage: {\n        promptTokens: rawResponse.tokens_evaluated,\n        completionTokens: rawResponse.tokens_predicted,\n        totalTokens:\n          rawResponse.tokens_evaluated + rawResponse.tokens_predicted,\n      },\n    };\n  }\n\n  doStreamText(prompt: LlamaCppCompletionPrompt, options: FunctionCallOptions) {\n    return this.callAPI(prompt, options, {\n      responseFormat: LlamaCppCompletionResponseFormat.deltaIterable,\n    });\n  }\n\n  extractTextDelta(delta: unknown) {\n    return (delta as LlamaCppTextStreamChunk).content;\n  }\n\n  asObjectGenerationModel<INPUT_PROMPT, LlamaCppPrompt>(\n    promptTemplate:\n      | ObjectFromTextPromptTemplate<INPUT_PROMPT, LlamaCppPrompt>\n      | FlexibleObjectFromTextPromptTemplate<INPUT_PROMPT, unknown>\n  ) {\n    return \"adaptModel\" in promptTemplate\n      ? new ObjectFromTextStreamingModel({\n          model: promptTemplate.adaptModel(this),\n          template: promptTemplate,\n        })\n      : new ObjectFromTextStreamingModel({\n          model: this as TextStreamingModel<LlamaCppPrompt>,\n          template: promptTemplate,\n        });\n  }\n\n  withJsonOutput(schema: Schema<unknown> & JsonSchemaProducer): this {\n    // don't override the grammar if it's already set (to allow user to override)\n    if (this.settings.grammar != null) {\n      return this;\n    }\n\n    const grammar = convertJsonSchemaToGBNF(schema.getJsonSchema());\n\n    return this.withSettings({\n      grammar: grammar,\n    });\n  }\n\n  private get promptTemplateProvider(): TextGenerationPromptTemplateProvider<LlamaCppCompletionPrompt> {\n    return this.settings.promptTemplate ?? Text;\n  }\n\n  withTextPrompt(): PromptTemplateTextStreamingModel<\n    string,\n    LlamaCppCompletionPrompt,\n    LlamaCppCompletionModelSettings<CONTEXT_WINDOW_SIZE>,\n    this\n  > {\n    return this.withPromptTemplate(this.promptTemplateProvider.text());\n  }\n\n  withInstructionPrompt(): PromptTemplateTextStreamingModel<\n    InstructionPrompt,\n    LlamaCppCompletionPrompt,\n    LlamaCppCompletionModelSettings<CONTEXT_WINDOW_SIZE>,\n    this\n  > {\n    return this.withPromptTemplate(this.promptTemplateProvider.instruction());\n  }\n\n  withChatPrompt(): PromptTemplateTextStreamingModel<\n    ChatPrompt,\n    LlamaCppCompletionPrompt,\n    LlamaCppCompletionModelSettings<CONTEXT_WINDOW_SIZE>,\n    this\n  > {\n    return this.withPromptTemplate(this.promptTemplateProvider.chat());\n  }\n\n  /**\n   * Maps the prompt for the full Llama.cpp prompt template (incl. image support).\n   */\n  withPromptTemplate<INPUT_PROMPT>(\n    promptTemplate: TextGenerationPromptTemplate<\n      INPUT_PROMPT,\n      LlamaCppCompletionPrompt\n    >\n  ): PromptTemplateTextStreamingModel<\n    INPUT_PROMPT,\n    LlamaCppCompletionPrompt,\n    LlamaCppCompletionModelSettings<CONTEXT_WINDOW_SIZE>,\n    this\n  > {\n    return new PromptTemplateTextStreamingModel({\n      model: this.withSettings({\n        stopSequences: [\n          ...(this.settings.stopSequences ?? []),\n          ...promptTemplate.stopSequences,\n        ],\n      }),\n      promptTemplate,\n    });\n  }\n\n  withSettings(\n    additionalSettings: Partial<\n      LlamaCppCompletionModelSettings<CONTEXT_WINDOW_SIZE>\n    >\n  ) {\n    return new LlamaCppCompletionModel(\n      Object.assign({}, this.settings, additionalSettings)\n    ) as this;\n  }\n}\n\nconst llamaCppTextGenerationResponseSchema = z.object({\n  content: z.string(),\n  stop: z.literal(true),\n  generation_settings: z.object({\n    frequency_penalty: z.number(),\n    ignore_eos: z.boolean(),\n    logit_bias: z.array(z.number()),\n    mirostat: z.number(),\n    mirostat_eta: z.number(),\n    mirostat_tau: z.number(),\n    model: z.string(),\n    n_ctx: z.number(),\n    n_keep: z.number(),\n    n_predict: z.number(),\n    n_probs: z.number(),\n    penalize_nl: z.boolean(),\n    presence_penalty: z.number(),\n    repeat_last_n: z.number(),\n    repeat_penalty: z.number(),\n    seed: z.number(),\n    stop: z.array(z.string()),\n    stream: z.boolean(),\n    temperature: z.number().optional(), // optional for backwards compatibility\n    tfs_z: z.number(),\n    top_k: z.number(),\n    top_p: z.number(),\n    typical_p: z.number(),\n  }),\n  model: z.string(),\n  prompt: z.string(),\n  stopped_eos: z.boolean(),\n  stopped_limit: z.boolean(),\n  stopped_word: z.boolean(),\n  stopping_word: z.string(),\n  timings: z.object({\n    predicted_ms: z.number(),\n    predicted_n: z.number(),\n    predicted_per_second: z.number().nullable(),\n    predicted_per_token_ms: z.number().nullable(),\n    prompt_ms: z.number().nullable().optional(),\n    prompt_n: z.number(),\n    prompt_per_second: z.number().nullable(),\n    prompt_per_token_ms: z.number().nullable(),\n  }),\n  tokens_cached: z.number(),\n  tokens_evaluated: z.number(),\n  tokens_predicted: z.number(),\n  truncated: z.boolean(),\n});\n\nexport type LlamaCppTextGenerationResponse = z.infer<\n  typeof llamaCppTextGenerationResponseSchema\n>;\n\nconst llamaCppTextStreamChunkSchema = z.discriminatedUnion(\"stop\", [\n  z.object({\n    content: z.string(),\n    stop: z.literal(false),\n  }),\n  llamaCppTextGenerationResponseSchema,\n]);\n\nexport type LlamaCppTextStreamChunk = z.infer<\n  typeof llamaCppTextStreamChunkSchema\n>;\n\nasync function createLlamaCppFullDeltaIterableQueue(\n  stream: ReadableStream<Uint8Array>\n): Promise<AsyncIterable<Delta<LlamaCppTextStreamChunk>>> {\n  const queue = new AsyncQueue<Delta<LlamaCppTextStreamChunk>>();\n\n  // process the stream asynchonously (no 'await' on purpose):\n  parseEventSourceStream({ stream })\n    .then(async (events) => {\n      try {\n        for await (const event of events) {\n          const data = event.data;\n\n          const eventData = parseJSON({\n            text: data,\n            schema: zodSchema(llamaCppTextStreamChunkSchema),\n          });\n\n          queue.push({ type: \"delta\", deltaValue: eventData });\n\n          if (eventData.stop) {\n            queue.close();\n          }\n        }\n      } catch (error) {\n        queue.push({ type: \"error\", error });\n        queue.close();\n      }\n    })\n    .catch((error) => {\n      queue.push({ type: \"error\", error });\n      queue.close();\n    });\n\n  return queue;\n}\n\nexport type LlamaCppCompletionResponseFormatType<T> = {\n  stream: boolean;\n  handler: ResponseHandler<T>;\n};\n\nexport const LlamaCppCompletionResponseFormat = {\n  /**\n   * Returns the response as a JSON object.\n   */\n  json: {\n    stream: false,\n    handler: createJsonResponseHandler(\n      zodSchema(llamaCppTextGenerationResponseSchema)\n    ),\n  } satisfies LlamaCppCompletionResponseFormatType<LlamaCppTextGenerationResponse>,\n\n  /**\n   * Returns an async iterable over the full deltas (all choices, including full current state at time of event)\n   * of the response stream.\n   */\n  deltaIterable: {\n    stream: true,\n    handler: async ({ response }: { response: Response }) =>\n      createLlamaCppFullDeltaIterableQueue(response.body!),\n  } satisfies LlamaCppCompletionResponseFormatType<\n    AsyncIterable<Delta<LlamaCppTextStreamChunk>>\n  >,\n};\n","export async function* convertReadableStreamToAsyncIterable<T>(\n  stream: ReadableStream<T>\n): AsyncIterable<T> {\n  const reader = stream.getReader();\n  try {\n    while (true) {\n      const { done, value } = await reader.read();\n      if (done) {\n        return; // This will close the generator\n      }\n      yield value;\n    }\n  } finally {\n    reader.releaseLock();\n  }\n}\n","import {\n  EventSourceParser,\n  ParsedEvent,\n  createParser,\n} from \"eventsource-parser\";\n\n/**\n * A TransformStream that ingests a stream of strings and produces a stream of ParsedEvents.\n *\n * @example\n * ```\n * const eventStream =\n *   response.body\n *     .pipeThrough(new TextDecoderStream())\n *     .pipeThrough(new EventSourceParserStream())\n * ```\n */\n// Copied from https://github.com/rexxars/eventsource-parser/blob/main/src/stream.ts to avoid issues with the commonjs build.\nexport class EventSourceParserStream extends TransformStream<\n  string,\n  ParsedEvent\n> {\n  constructor() {\n    let parser!: EventSourceParser;\n\n    super({\n      start(controller) {\n        parser = createParser((event) => {\n          if (event.type === \"event\") {\n            controller.enqueue(event);\n          }\n        });\n      },\n      transform(chunk) {\n        parser.feed(chunk);\n      },\n    });\n  }\n}\n","import { ParsedEvent } from \"eventsource-parser\";\nimport { convertReadableStreamToAsyncIterable } from \"./convertReadableStreamToAsyncIterable\";\nimport { EventSourceParserStream } from \"./EventSourceParserStream\";\n\nexport async function parseEventSourceStream({\n  stream,\n}: {\n  stream: ReadableStream<Uint8Array>;\n}): Promise<AsyncIterable<ParsedEvent>> {\n  const eventStream = stream\n    .pipeThrough(new TextDecoderStream())\n    .pipeThrough(new EventSourceParserStream());\n\n  return convertReadableStreamToAsyncIterable(eventStream);\n}\n","import { z } from \"zod\";\nimport { createJsonErrorResponseHandler } from \"../../core/api/postToApi\";\nimport { zodSchema } from \"../../core/schema/ZodSchema\";\n\nconst llamaCppErrorDataSchema = z.object({\n  error: z.string(),\n});\n\nexport type LlamaCppErrorData = z.infer<typeof llamaCppErrorDataSchema>;\n\nexport const failedLlamaCppCallResponseHandler = createJsonErrorResponseHandler(\n  {\n    errorSchema: zodSchema(llamaCppErrorDataSchema),\n    errorToMessage: (error) => error.error,\n  }\n);\n","import { TextGenerationPromptTemplate } from \"../../model-function/generate-text/TextGenerationPromptTemplate\";\nimport * as alpacaPrompt from \"../../model-function/generate-text/prompt-template/AlpacaPromptTemplate\";\nimport * as chatMlPrompt from \"../../model-function/generate-text/prompt-template/ChatMLPromptTemplate\";\nimport * as llama2Prompt from \"../../model-function/generate-text/prompt-template/Llama2PromptTemplate\";\nimport * as mistralPrompt from \"../../model-function/generate-text/prompt-template/MistralInstructPromptTemplate\";\nimport * as neuralChatPrompt from \"../../model-function/generate-text/prompt-template/NeuralChatPromptTemplate\";\nimport { TextGenerationPromptTemplateProvider } from \"../../model-function/generate-text/prompt-template/PromptTemplateProvider\";\nimport * as synthiaPrompt from \"../../model-function/generate-text/prompt-template/SynthiaPromptTemplate\";\nimport * as textPrompt from \"../../model-function/generate-text/prompt-template/TextPromptTemplate\";\nimport * as vicunaPrompt from \"../../model-function/generate-text/prompt-template/VicunaPromptTemplate\";\nimport * as LlamaCppBakLLaVA1Prompt from \"./LlamaCppBakLLaVA1PromptTemplate\";\nimport { LlamaCppCompletionPrompt } from \"./LlamaCppCompletionModel\";\n\nexport function asLlamaCppPromptTemplate<SOURCE_PROMPT>(\n  promptTemplate: TextGenerationPromptTemplate<SOURCE_PROMPT, string>\n): TextGenerationPromptTemplate<SOURCE_PROMPT, LlamaCppCompletionPrompt> {\n  return {\n    format: (prompt) => ({\n      text: promptTemplate.format(prompt),\n    }),\n    stopSequences: promptTemplate.stopSequences,\n  };\n}\n\nexport function asLlamaCppTextPromptTemplateProvider(\n  promptTemplateProvider: TextGenerationPromptTemplateProvider<string>\n): TextGenerationPromptTemplateProvider<LlamaCppCompletionPrompt> {\n  return {\n    text: () => asLlamaCppPromptTemplate(promptTemplateProvider.text()),\n\n    instruction: () =>\n      asLlamaCppPromptTemplate(promptTemplateProvider.instruction()),\n\n    chat: () => asLlamaCppPromptTemplate(promptTemplateProvider.chat()),\n  };\n}\n\nexport const Text = asLlamaCppTextPromptTemplateProvider(textPrompt);\n\n/**\n * Formats text, instruction or chat prompts as a Mistral instruct prompt.\n *\n * Note that Mistral does not support system prompts. We emulate them.\n *\n * Text prompt:\n * ```\n * <s>[INST] { instruction } [/INST]\n * ```\n *\n * Instruction prompt when system prompt is set:\n * ```\n * <s>[INST] ${ system prompt } [/INST] </s>[INST] ${instruction} [/INST] ${ response prefix }\n * ```\n *\n * Instruction prompt template when there is no system prompt:\n * ```\n * <s>[INST] ${ instruction } [/INST] ${ response prefix }\n * ```\n *\n * Chat prompt when system prompt is set:\n * ```\n * <s>[INST] ${ system prompt } [/INST] </s> [INST] ${ user msg 1 } [/INST] ${ model response 1 } [INST] ${ user msg 2 } [/INST] ${ model response 2 } [INST] ${ user msg 3 } [/INST]\n * ```\n *\n * Chat prompt when there is no system prompt:\n * ```\n * <s>[INST] ${ user msg 1 } [/INST] ${ model response 1 } </s>[INST] ${ user msg 2 } [/INST] ${ model response 2 } [INST] ${ user msg 3 } [/INST]\n * ```\n *\n * @see https://docs.mistral.ai/models/#chat-template\n */\nexport const Mistral = asLlamaCppTextPromptTemplateProvider(mistralPrompt);\n\nexport const ChatML = asLlamaCppTextPromptTemplateProvider(chatMlPrompt);\nexport const Llama2 = asLlamaCppTextPromptTemplateProvider(llama2Prompt);\nexport const NeuralChat =\n  asLlamaCppTextPromptTemplateProvider(neuralChatPrompt);\nexport const Alpaca = asLlamaCppTextPromptTemplateProvider(alpacaPrompt);\nexport const Synthia = asLlamaCppTextPromptTemplateProvider(synthiaPrompt);\nexport const Vicuna = asLlamaCppTextPromptTemplateProvider(vicunaPrompt);\nexport const BakLLaVA1 = LlamaCppBakLLaVA1Prompt;\n","import { TextGenerationPromptTemplate } from \"../../model-function/generate-text/TextGenerationPromptTemplate\";\nimport { ChatPrompt } from \"../../model-function/generate-text/prompt-template/ChatPrompt\";\nimport { validateContentIsString } from \"../../model-function/generate-text/prompt-template/ContentPart\";\nimport { InstructionPrompt } from \"../../model-function/generate-text/prompt-template/InstructionPrompt\";\nimport { InvalidPromptError } from \"../../model-function/generate-text/prompt-template/InvalidPromptError\";\nimport { text as vicunaText } from \"../../model-function/generate-text/prompt-template/TextPromptTemplate\";\nimport { convertDataContentToBase64String } from \"../../util/format/DataContent\";\nimport { LlamaCppCompletionPrompt } from \"./LlamaCppCompletionModel\";\n\n// default Vicuna 1 system message\nconst DEFAULT_SYSTEM_MESSAGE =\n  \"A chat between a curious user and an artificial intelligence assistant. \" +\n  \"The assistant gives helpful, detailed, and polite answers to the user's questions.\";\n\n/**\n * Text prompt.\n */\nexport function text(): TextGenerationPromptTemplate<\n  string,\n  LlamaCppCompletionPrompt\n> {\n  const delegate = vicunaText();\n  return {\n    stopSequences: [],\n    format(prompt) {\n      return { text: delegate.format(prompt) };\n    },\n  };\n}\n\n/**\n * BakLLaVA 1 uses a Vicuna 1 prompt. This mapping combines it with the LlamaCpp prompt.\n *\n * @see https://github.com/SkunkworksAI/BakLLaVA\n */\nexport function instruction(): TextGenerationPromptTemplate<\n  InstructionPrompt,\n  LlamaCppCompletionPrompt\n> {\n  return {\n    format(prompt) {\n      let text = \"\";\n\n      text += `${prompt.system ?? DEFAULT_SYSTEM_MESSAGE}\\n\\n`;\n\n      text += `USER: `;\n\n      const images: Record<string, string> = {};\n\n      if (typeof prompt.instruction === \"string\") {\n        text += `${prompt.instruction}\\n`;\n      } else {\n        // construct text and image mapping:\n        let imageCounter = 1;\n        for (const content of prompt.instruction) {\n          switch (content.type) {\n            case \"text\": {\n              text += content.text;\n              break;\n            }\n            case \"image\": {\n              text += `[img-${imageCounter}]`;\n              images[imageCounter.toString()] =\n                convertDataContentToBase64String(content.image);\n              imageCounter++;\n              break;\n            }\n          }\n\n          text += `${content}\\n`;\n        }\n      }\n\n      text += `\\nASSISTANT: `;\n\n      return { text, images };\n    },\n    stopSequences: [`\\nUSER:`],\n  };\n}\n\nexport function chat(): TextGenerationPromptTemplate<\n  ChatPrompt,\n  LlamaCppCompletionPrompt\n> {\n  return {\n    format(prompt) {\n      let text = \"\";\n\n      text += `${prompt.system ?? DEFAULT_SYSTEM_MESSAGE}\\n\\n`;\n\n      // construct text and image mapping:\n      let imageCounter = 1;\n      const images: Record<string, string> = {};\n\n      for (const { role, content } of prompt.messages) {\n        switch (role) {\n          case \"user\": {\n            text += `USER: `;\n\n            if (typeof content === \"string\") {\n              text += content;\n              break;\n            }\n\n            for (const part of content) {\n              switch (part.type) {\n                case \"text\": {\n                  text += part.text;\n                  break;\n                }\n                case \"image\": {\n                  text += `[img-${imageCounter}]`;\n                  images[imageCounter.toString()] =\n                    convertDataContentToBase64String(part.image);\n                  imageCounter++;\n                  break;\n                }\n              }\n            }\n\n            break;\n          }\n          case \"assistant\": {\n            text += `ASSISTANT: ${validateContentIsString(content, prompt)}`;\n            break;\n          }\n          case \"tool\": {\n            throw new InvalidPromptError(\n              \"Tool messages are not supported.\",\n              prompt\n            );\n          }\n          default: {\n            const _exhaustiveCheck: never = role;\n            throw new Error(`Unsupported role: ${_exhaustiveCheck}`);\n          }\n        }\n\n        text += `\\n\\n`;\n      }\n\n      text += `ASSISTANT: `;\n\n      return { text, images };\n    },\n    stopSequences: [`\\nUSER:`],\n  };\n}\n","import { z } from \"zod\";\nimport { FunctionCallOptions } from \"../../core/FunctionOptions\";\nimport { ApiConfiguration } from \"../../core/api/ApiConfiguration\";\nimport { callWithRetryAndThrottle } from \"../../core/api/callWithRetryAndThrottle\";\nimport {\n  createJsonResponseHandler,\n  postJsonToApi,\n} from \"../../core/api/postToApi\";\nimport { zodSchema } from \"../../core/schema/ZodSchema\";\nimport { BasicTokenizer } from \"../../model-function/tokenize-text/Tokenizer\";\nimport { LlamaCppApiConfiguration } from \"./LlamaCppApiConfiguration\";\nimport { failedLlamaCppCallResponseHandler } from \"./LlamaCppError\";\n\n/**\n * Tokenizer for LlamaCpp.\n\n * @example\n * const tokenizer = new LlamaCppTokenizer();\n *\n * const text = \"At first, Nox didn't know what to do with the pup.\";\n *\n * const tokenCount = await countTokens(tokenizer, text);\n * const tokens = await tokenizer.tokenize(text);\n * const tokensAndTokenTexts = await tokenizer.tokenizeWithTexts(text);\n * const reconstructedText = await tokenizer.detokenize(tokens);\n */\nexport class LlamaCppTokenizer implements BasicTokenizer {\n  readonly api: ApiConfiguration;\n\n  constructor(api: ApiConfiguration = new LlamaCppApiConfiguration()) {\n    this.api = api;\n  }\n\n  async callTokenizeAPI(\n    text: string,\n    callOptions?: FunctionCallOptions\n  ): Promise<LlamaCppTokenizationResponse> {\n    const api = this.api;\n    const abortSignal = callOptions?.run?.abortSignal;\n\n    return callWithRetryAndThrottle({\n      retry: api.retry,\n      throttle: api.throttle,\n      call: async () =>\n        postJsonToApi({\n          url: api.assembleUrl(`/tokenize`),\n          headers: api.headers({\n            functionType: \"tokenize\",\n            functionId: callOptions?.functionId,\n            run: callOptions?.run,\n            callId: \"\",\n          }),\n          body: {\n            content: text,\n          },\n          failedResponseHandler: failedLlamaCppCallResponseHandler,\n          successfulResponseHandler: createJsonResponseHandler(\n            zodSchema(llamaCppTokenizationResponseSchema)\n          ),\n          abortSignal,\n        }),\n    });\n  }\n\n  async tokenize(text: string) {\n    const response = await this.callTokenizeAPI(text);\n    return response.tokens;\n  }\n}\n\nconst llamaCppTokenizationResponseSchema = z.object({\n  tokens: z.array(z.number()),\n});\n\nexport type LlamaCppTokenizationResponse = z.infer<\n  typeof llamaCppTokenizationResponseSchema\n>;\n","/* eslint-disable @typescript-eslint/no-explicit-any */\n\n/**\n * Convert JSON Schema to a GBNF grammar.\n *\n * This is a modified version of\n * https://github.com/ggerganov/llama.cpp/blob/master/examples/server/public/json-schema-to-grammar.mjs\n */\nexport function convertJsonSchemaToGBNF(schema: unknown): string {\n  const rules = new RuleMap();\n\n  rules.add(\"space\", SPACE_RULE);\n\n  visit(schema, undefined, rules);\n\n  return rules.toGBNF();\n}\nconst SPACE_RULE = '\" \"?';\n\nconst PRIMITIVE_RULES = {\n  boolean: '(\"true\" | \"false\") space',\n  number:\n    '(\"-\"? ([0-9] | [1-9] [0-9]*)) (\".\" [0-9]+)? ([eE] [-+]? [0-9]+)? space',\n  integer: '(\"-\"? ([0-9] | [1-9] [0-9]*)) space',\n  string: ` \"\\\\\"\" ( [^\"\\\\\\\\] | \"\\\\\\\\\" ([\"\\\\\\\\/bfnrt] | \"u\" [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F]) )* \"\\\\\"\" space`,\n  null: '\"null\" space',\n} as Record<string, string>;\n\nclass RuleMap {\n  readonly rules = new Map<string, string>();\n\n  add(name: string, rule: string): string {\n    const escapedName = this.escapeRuleName(name, rule);\n    this.rules.set(escapedName, rule);\n    return escapedName;\n  }\n\n  /**\n   * Replace invalid characters in rule name with hyphens.\n   * Disambiguate the name if it already exists.\n   */\n  private escapeRuleName(name: string, rule: string) {\n    const baseName = name.replace(/[^\\dA-Za-z-]+/g, \"-\");\n\n    if (!this.rules.has(baseName) || this.rules.get(baseName) === rule) {\n      return baseName;\n    }\n\n    let i = 0;\n    while (this.rules.has(`${baseName}${i}`)) {\n      if (this.rules.get(`${baseName}${i}`) === rule) {\n        return `${baseName}${i}`;\n      }\n\n      i++;\n    }\n\n    return `${baseName}${i}`;\n  }\n\n  toGBNF() {\n    return Array.from(this.rules)\n      .map(([name, rule]) => `${name} ::= ${rule}`)\n      .join(\"\\n\");\n  }\n}\n\nconst GRAMMAR_LITERAL_ESCAPES = {\n  \"\\r\": \"\\\\r\",\n  \"\\n\": \"\\\\n\",\n  '\"': '\\\\\"',\n} as Record<string, string>;\n\nfunction formatLiteral(literal: string) {\n  const escaped = JSON.stringify(literal).replace(\n    /[\\n\\r\"]/g,\n    (m) => GRAMMAR_LITERAL_ESCAPES[m]\n  );\n\n  return `\"${escaped}\"`;\n}\n\nfunction visit(schema: any, name: string | undefined, rules: RuleMap): string {\n  const schemaType = schema.type;\n  const ruleName = name || \"root\";\n\n  if (schema.oneOf || schema.anyOf) {\n    const rule = (schema.oneOf || schema.anyOf)\n      .map((altSchema: any, i: number) =>\n        visit(altSchema, `${name}${name ? \"-\" : \"\"}${i}`, rules)\n      )\n      .join(\" | \");\n\n    return rules.add(ruleName, rule);\n  } else if (\"const\" in schema) {\n    return rules.add(ruleName, formatLiteral(schema.const));\n  } else if (\"enum\" in schema) {\n    const rule = schema.enum.map(formatLiteral).join(\" | \");\n    return rules.add(ruleName, rule);\n  } else if (schemaType === \"object\" && \"properties\" in schema) {\n    const propPairs = Object.entries(schema.properties);\n\n    let rule = '\"{\" space';\n    propPairs.forEach(([propName, propSchema], i) => {\n      const propRuleName = visit(\n        propSchema,\n        `${name ?? \"\"}${name ? \"-\" : \"\"}${propName}`,\n        rules\n      );\n      if (i > 0) {\n        rule += ' \",\" space';\n      }\n      rule += ` ${formatLiteral(propName)} space \":\" space ${propRuleName}`;\n    });\n    rule += ' \"}\" space';\n\n    return rules.add(ruleName, rule);\n  } else if (schemaType === \"array\" && \"items\" in schema) {\n    const itemRuleName = visit(\n      schema.items,\n      `${name ?? \"\"}${name ? \"-\" : \"\"}item`,\n      rules\n    );\n    const rule = `\"[\" space (${itemRuleName} (\",\" space ${itemRuleName})*)? \"]\" space`;\n    return rules.add(ruleName, rule);\n  } else {\n    if (!PRIMITIVE_RULES[schemaType]) {\n      throw new Error(`Unrecognized schema: ${JSON.stringify(schema)}`);\n    }\n    return rules.add(\n      ruleName === \"root\" ? \"root\" : schemaType,\n      PRIMITIVE_RULES[schemaType]\n    );\n  }\n}\n","import { ApiConfiguration } from \"../../core/api/ApiConfiguration\";\nimport { PartialBaseUrlPartsApiConfigurationOptions } from \"../../core/api/BaseUrlApiConfiguration\";\nimport { LlamaCppApiConfiguration } from \"./LlamaCppApiConfiguration\";\nimport {\n  LlamaCppCompletionModel,\n  LlamaCppCompletionModelSettings,\n} from \"./LlamaCppCompletionModel\";\nimport {\n  LlamaCppTextEmbeddingModel,\n  LlamaCppTextEmbeddingModelSettings,\n} from \"./LlamaCppTextEmbeddingModel\";\nimport { LlamaCppTokenizer } from \"./LlamaCppTokenizer\";\n\n/**\n * Creates an API configuration for the Llama.cpp server.\n * It calls the API at http://127.0.0.1:8080 by default.\n */\nexport function Api(settings: PartialBaseUrlPartsApiConfigurationOptions) {\n  return new LlamaCppApiConfiguration(settings);\n}\n\nexport function CompletionTextGenerator<CONTEXT_WINDOW_SIZE extends number>(\n  settings: LlamaCppCompletionModelSettings<CONTEXT_WINDOW_SIZE> = {}\n) {\n  return new LlamaCppCompletionModel<CONTEXT_WINDOW_SIZE>(settings);\n}\n\nexport function TextEmbedder(\n  settings: LlamaCppTextEmbeddingModelSettings = {}\n) {\n  return new LlamaCppTextEmbeddingModel(settings);\n}\n\nexport function Tokenizer(\n  api: ApiConfiguration = new LlamaCppApiConfiguration()\n) {\n  return new LlamaCppTokenizer(api);\n}\n\n/**\n * GBNF grammars. You can use them in the `grammar` option of the `TextGenerator` model.\n */\nexport * as grammar from \"./LlamaCppGrammars\";\n\nexport * as prompt from \"./LlamaCppPrompt\";\n","import { z } from \"zod\";\nimport { FunctionCallOptions } from \"../../core/FunctionOptions\";\nimport { ApiConfiguration } from \"../../core/api/ApiConfiguration\";\nimport { callWithRetryAndThrottle } from \"../../core/api/callWithRetryAndThrottle\";\nimport {\n  createJsonResponseHandler,\n  postJsonToApi,\n} from \"../../core/api/postToApi\";\nimport { zodSchema } from \"../../core/schema/ZodSchema\";\nimport { AbstractModel } from \"../../model-function/AbstractModel\";\nimport {\n  EmbeddingModel,\n  EmbeddingModelSettings,\n} from \"../../model-function/embed/EmbeddingModel\";\nimport { LlamaCppApiConfiguration } from \"./LlamaCppApiConfiguration\";\nimport { failedLlamaCppCallResponseHandler } from \"./LlamaCppError\";\nimport { LlamaCppTokenizer } from \"./LlamaCppTokenizer\";\n\nexport interface LlamaCppTextEmbeddingModelSettings\n  extends EmbeddingModelSettings {\n  api?: ApiConfiguration;\n  dimensions?: number;\n  isParallelizable?: boolean;\n}\n\nexport class LlamaCppTextEmbeddingModel\n  extends AbstractModel<LlamaCppTextEmbeddingModelSettings>\n  implements EmbeddingModel<string, LlamaCppTextEmbeddingModelSettings>\n{\n  constructor(settings: LlamaCppTextEmbeddingModelSettings = {}) {\n    super({ settings });\n\n    this.tokenizer = new LlamaCppTokenizer(this.settings.api);\n  }\n\n  readonly provider = \"llamacpp\" as const;\n  get modelName() {\n    return null;\n  }\n\n  readonly maxValuesPerCall = 1;\n  get isParallelizable() {\n    return this.settings.isParallelizable ?? false;\n  }\n\n  readonly contextWindowSize = undefined;\n  get dimensions() {\n    return this.settings.dimensions;\n  }\n\n  private readonly tokenizer: LlamaCppTokenizer;\n\n  async tokenize(text: string) {\n    return this.tokenizer.tokenize(text);\n  }\n\n  async callAPI(\n    texts: Array<string>,\n    callOptions: FunctionCallOptions\n  ): Promise<LlamaCppTextEmbeddingResponse> {\n    if (texts.length > this.maxValuesPerCall) {\n      throw new Error(\n        `The Llama.cpp embedding API only supports ${this.maxValuesPerCall} texts per API call.`\n      );\n    }\n\n    const api = this.settings.api ?? new LlamaCppApiConfiguration();\n    const abortSignal = callOptions.run?.abortSignal;\n\n    return callWithRetryAndThrottle({\n      retry: this.settings.api?.retry,\n      throttle: this.settings.api?.throttle,\n      call: async () =>\n        postJsonToApi({\n          url: api.assembleUrl(`/embedding`),\n          headers: api.headers({\n            functionType: callOptions.functionType,\n            functionId: callOptions.functionId,\n            run: callOptions.run,\n            callId: callOptions.callId,\n          }),\n          body: { content: texts[0] },\n          failedResponseHandler: failedLlamaCppCallResponseHandler,\n          successfulResponseHandler: createJsonResponseHandler(\n            zodSchema(llamaCppTextEmbeddingResponseSchema)\n          ),\n          abortSignal,\n        }),\n    });\n  }\n\n  get settingsForEvent(): Partial<LlamaCppTextEmbeddingModelSettings> {\n    return {\n      dimensions: this.settings.dimensions,\n    };\n  }\n\n  async doEmbedValues(texts: string[], options: FunctionCallOptions) {\n    const rawResponse = await this.callAPI(texts, options);\n\n    return {\n      rawResponse,\n      embeddings: [rawResponse.embedding],\n    };\n  }\n\n  withSettings(\n    additionalSettings: Partial<LlamaCppTextEmbeddingModelSettings>\n  ) {\n    return new LlamaCppTextEmbeddingModel(\n      Object.assign({}, this.settings, additionalSettings)\n    ) as this;\n  }\n}\n\nconst llamaCppTextEmbeddingResponseSchema = z.object({\n  embedding: z.array(z.number()),\n});\n\nexport type LlamaCppTextEmbeddingResponse = z.infer<\n  typeof llamaCppTextEmbeddingResponseSchema\n>;\n","/**\n * GBNF grammar for JSON.\n *\n * @see https://github.com/ggerganov/llama.cpp/blob/master/grammars/json.gbnf\n */\nexport const json: string = `\nroot   ::= object\nvalue  ::= object | array | string | number | (\"true\" | \"false\" | \"null\") ws\n\nobject ::=\n  \"{\" ws (\n            string \":\" ws value\n    (\",\" ws string \":\" ws value)*\n  )? \"}\" ws\n\narray  ::=\n  \"[\" ws (\n            value\n    (\",\" ws value)*\n  )? \"]\" ws\n\nstring ::=\n  \"\\\\\"\" (\n    [^\"\\\\\\\\] |\n    \"\\\\\\\\\" ([\"\\\\\\\\/bfnrt] | \"u\" [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F]) # escapes\n  )* \"\\\\\"\" ws\n\nnumber ::= (\"-\"? ([0-9] | [1-9] [0-9]*)) (\".\" [0-9]+)? ([eE] [-+]? [0-9]+)? ws\n\n# Optional space: by convention, applied in this grammar after literal chars when allowed\nws ::= ([ \\t\\n] ws)?\n`;\n\n/**\n * GBNF grammar for JSON array outputs. Restricts whitespace at the end of the array.\n *\n * @see https://github.com/ggerganov/llama.cpp/blob/master/grammars/json_arr.gbnf\n */\nexport const jsonArray: string = `\nroot   ::= arr\nvalue  ::= object | array | string | number | (\"true\" | \"false\" | \"null\") ws\n\narr  ::=\n  \"[\\n\" ws (\n            value\n    (\",\\n\" ws value)*\n  )? \"]\"\n\nobject ::=\n  \"{\" ws (\n            string \":\" ws value\n    (\",\" ws string \":\" ws value)*\n  )? \"}\" ws\n\narray  ::=\n  \"[\" ws (\n            value\n    (\",\" ws value)*\n  )? \"]\" ws\n\nstring ::=\n  \"\\\\\"\" (\n    [^\"\\\\\\\\] |\n    \"\\\\\\\\\" ([\"\\\\\\\\/bfnrt] | \"u\" [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F]) # escapes\n  )* \"\\\\\"\" ws\n\nnumber ::= (\"-\"? ([0-9] | [1-9] [0-9]*)) (\".\" [0-9]+)? ([eE] [-+]? [0-9]+)? ws\n\n# Optional space: by convention, applied in this grammar after literal chars when allowed\nws ::= ([ \\t\\n] ws)?\n`;\n\n/**\n * GBNF grammar for list outputs. List items are separated by newlines and start with `- `.\n *\n * @see https://github.com/ggerganov/llama.cpp/blob/master/grammars/list.gbnf\n */\nexport const list: string = `\nroot ::= item+\n\n# Excludes various line break characters\nitem ::= \"- \" [^\\r\\n\\x0b\\x0c\\x85\\u2028\\u2029]+ \"\\n\"\n`;\n\nexport { convertJsonSchemaToGBNF as fromJsonSchema } from \"./convertJsonSchemaToGBNF\";\n","import {\n  BaseUrlApiConfigurationWithDefaults,\n  PartialBaseUrlPartsApiConfigurationOptions,\n} from \"../../core/api/BaseUrlApiConfiguration\";\nimport { loadApiKey } from \"../../core/api/loadApiKey\";\n\n/**\n * Creates an API configuration for the LMNT API.\n * It calls the API at https://api.lmnt.com/v1 and uses the `LMNT_API_KEY` env variable by default.\n */\nexport class LmntApiConfiguration extends BaseUrlApiConfigurationWithDefaults {\n  constructor(\n    settings: PartialBaseUrlPartsApiConfigurationOptions & {\n      apiKey?: string;\n    } = {}\n  ) {\n    super({\n      ...settings,\n      headers: {\n        \"X-API-Key\": loadApiKey({\n          apiKey: settings.apiKey,\n          environmentVariableName: \"LMNT_API_KEY\",\n          description: \"LMNT\",\n        }),\n      },\n      baseUrlDefaults: {\n        protocol: \"https\",\n        host: \"api.lmnt.com\",\n        port: \"443\",\n        path: \"/v1\",\n      },\n    });\n  }\n}\n","import { LmntSpeechModel, LmntSpeechModelSettings } from \"./LmntSpeechModel\";\nimport { PartialBaseUrlPartsApiConfigurationOptions } from \"../../core/api/BaseUrlApiConfiguration\";\nimport { LmntApiConfiguration } from \"./LmntApiConfiguration\";\n\n/**\n * Creates an API configuration for the LMNT API.\n * It calls the API at https://api.lmnt.com/v1 and uses the `LMNT_API_KEY` env variable by default.\n */\nexport function Api(\n  settings: PartialBaseUrlPartsApiConfigurationOptions & {\n    apiKey?: string;\n  }\n) {\n  return new LmntApiConfiguration(settings);\n}\n\n/**\n * Synthesize speech using the LMNT API.\n *\n * @see https://docs.lmnt.com/api-reference/speech/synthesize-speech-1\n *\n * @returns A new instance of {@link LmntSpeechModel}.\n */\nexport function SpeechGenerator(settings: LmntSpeechModelSettings) {\n  return new LmntSpeechModel(settings);\n}\n","import { z } from \"zod\";\nimport { FunctionCallOptions } from \"../../core/FunctionOptions\";\nimport { ApiConfiguration } from \"../../core/api/ApiConfiguration\";\nimport { callWithRetryAndThrottle } from \"../../core/api/callWithRetryAndThrottle\";\nimport {\n  createJsonResponseHandler,\n  createTextErrorResponseHandler,\n  postToApi,\n} from \"../../core/api/postToApi\";\nimport { zodSchema } from \"../../core/schema/ZodSchema\";\nimport { AbstractModel } from \"../../model-function/AbstractModel\";\nimport {\n  SpeechGenerationModel,\n  SpeechGenerationModelSettings,\n} from \"../../model-function/generate-speech/SpeechGenerationModel\";\nimport { base64ToUint8Array } from \"../../util/format/UInt8Utils\";\nimport { LmntApiConfiguration } from \"./LmntApiConfiguration\";\n\nexport interface LmntSpeechModelSettings extends SpeechGenerationModelSettings {\n  api?: ApiConfiguration;\n\n  /**\n   * The voice id of the voice to use for synthesis.\n   */\n  voice: string;\n\n  /**\n   * The talking speed of the generated speech. A Floating point value between 0.25 (slow) and 2.0 (fast); defaults to 1.0\n   */\n  speed?: number;\n\n  /**\n   * Seed used to specify a different take; defaults to random\n   */\n  seed?: number;\n\n  /**\n   * Produce speech of this length in seconds; maximum 300.0 (5 minutes)\n   */\n  length?: number;\n}\n\n/**\n * Synthesize speech using the LMNT API.\n *\n * @see https://docs.lmnt.com/api-reference/speech/synthesize-speech-1\n */\nexport class LmntSpeechModel\n  extends AbstractModel<LmntSpeechModelSettings>\n  implements SpeechGenerationModel<LmntSpeechModelSettings>\n{\n  constructor(settings: LmntSpeechModelSettings) {\n    super({ settings });\n  }\n\n  readonly provider = \"lmnt\";\n\n  get modelName() {\n    return this.settings.voice;\n  }\n\n  private async callAPI(\n    text: string,\n    callOptions: FunctionCallOptions\n  ): Promise<LmntSpeechResponse> {\n    const api = this.settings.api ?? new LmntApiConfiguration();\n    const abortSignal = callOptions.run?.abortSignal;\n\n    return callWithRetryAndThrottle({\n      retry: api.retry,\n      throttle: api.throttle,\n      call: async () => {\n        const formData = new FormData();\n        formData.append(\"text\", text);\n        formData.append(\"voice\", this.settings.voice);\n        formData.append(\"format\", \"mp3\");\n        formData.append(\"return_durations\", \"true\");\n\n        if (this.settings.speed != null) {\n          formData.append(\"speed\", this.settings.speed.toString());\n        }\n        if (this.settings.seed != null) {\n          formData.append(\"seed\", this.settings.seed.toString());\n        }\n        if (this.settings.length != null) {\n          formData.append(\"length\", this.settings.length.toString());\n        }\n\n        return postToApi({\n          url: api.assembleUrl(`/ai/speech`),\n          headers: api.headers({\n            functionType: callOptions.functionType,\n            functionId: callOptions.functionId,\n            run: callOptions.run,\n            callId: callOptions.callId,\n          }),\n          body: {\n            content: formData,\n            values: {\n              text,\n              voice: this.settings.voice,\n              speed: this.settings.speed,\n              seed: this.settings.seed,\n              length: this.settings.length,\n            },\n          },\n          failedResponseHandler: createTextErrorResponseHandler(),\n          successfulResponseHandler: createJsonResponseHandler(\n            zodSchema(lmntSpeechResponseSchema)\n          ),\n          abortSignal,\n        });\n      },\n    });\n  }\n\n  get settingsForEvent(): Partial<LmntSpeechModelSettings> {\n    return {\n      voice: this.settings.voice,\n      speed: this.settings.speed,\n      seed: this.settings.seed,\n      length: this.settings.length,\n    };\n  }\n\n  async doGenerateSpeechStandard(text: string, options: FunctionCallOptions) {\n    const response = await this.callAPI(text, options);\n    return base64ToUint8Array(response.audio);\n  }\n\n  withSettings(additionalSettings: Partial<LmntSpeechModelSettings>) {\n    return new LmntSpeechModel({\n      ...this.settings,\n      ...additionalSettings,\n    }) as this;\n  }\n}\n\nconst lmntSpeechResponseSchema = z.object({\n  audio: z.string(),\n  durations: z.array(\n    z.object({\n      duration: z.number(),\n      start: z.number(),\n      text: z.string(),\n    })\n  ),\n  seed: z.number(),\n});\n\nexport type LmntSpeechResponse = z.infer<typeof lmntSpeechResponseSchema>;\n","import {\n  BaseUrlApiConfigurationWithDefaults,\n  PartialBaseUrlPartsApiConfigurationOptions,\n} from \"../../core/api/BaseUrlApiConfiguration\";\nimport { loadApiKey } from \"../../core/api/loadApiKey\";\n\n/**\n * Creates an API configuration for the Mistral API.\n * It calls the API at https://api.mistral.ai/v1 and uses the `MISTRAL_API_KEY` env variable by default.\n */\nexport class MistralApiConfiguration extends BaseUrlApiConfigurationWithDefaults {\n  constructor(\n    settings: PartialBaseUrlPartsApiConfigurationOptions & {\n      apiKey?: string;\n    } = {}\n  ) {\n    super({\n      ...settings,\n      headers: {\n        Authorization: `Bearer ${loadApiKey({\n          apiKey: settings.apiKey,\n          environmentVariableName: \"MISTRAL_API_KEY\",\n          description: \"Mistral\",\n        })}`,\n      },\n      baseUrlDefaults: {\n        protocol: \"https\",\n        host: \"api.mistral.ai\",\n        port: \"443\",\n        path: \"/v1\",\n      },\n    });\n  }\n}\n","import { z } from \"zod\";\nimport { FunctionCallOptions } from \"../../core/FunctionOptions\";\nimport { ApiConfiguration } from \"../../core/api/ApiConfiguration\";\nimport { callWithRetryAndThrottle } from \"../../core/api/callWithRetryAndThrottle\";\nimport {\n  ResponseHandler,\n  createJsonResponseHandler,\n  postJsonToApi,\n} from \"../../core/api/postToApi\";\nimport { zodSchema } from \"../../core/schema/ZodSchema\";\nimport { validateTypes } from \"../../core/schema/validateTypes\";\nimport { AbstractModel } from \"../../model-function/AbstractModel\";\nimport { PromptTemplateTextStreamingModel } from \"../../model-function/generate-text/PromptTemplateTextStreamingModel\";\nimport {\n  TextGenerationModelSettings,\n  TextStreamingBaseModel,\n  textGenerationModelProperties,\n} from \"../../model-function/generate-text/TextGenerationModel\";\nimport { TextGenerationPromptTemplate } from \"../../model-function/generate-text/TextGenerationPromptTemplate\";\nimport { TextGenerationFinishReason } from \"../../model-function/generate-text/TextGenerationResult\";\nimport { createEventSourceResponseHandler } from \"../../util/streaming/createEventSourceResponseHandler\";\nimport { MistralApiConfiguration } from \"./MistralApiConfiguration\";\nimport { chat, instruction, text } from \"./MistralChatPromptTemplate\";\nimport { failedMistralCallResponseHandler } from \"./MistralError\";\n\nexport type MistralChatMessage = {\n  role: \"system\" | \"user\" | \"assistant\";\n  content: string;\n};\n\nexport type MistralChatPrompt = Array<MistralChatMessage>;\n\nexport interface MistralChatModelSettings extends TextGenerationModelSettings {\n  api?: ApiConfiguration;\n\n  model: \"mistral-tiny\" | \"mistral-small\" | \"mistral-medium\";\n\n  /**\n   * What sampling temperature to use, between 0.0 and 2.0.\n   * Higher values like 0.8 will make the output more random,\n   * while lower values like 0.2 will make it more focused and deterministic.\n   *\n   * We generally recommend altering this or top_p but not both.\n   *\n   * Default: 0.7\n   */\n  temperature?: number | null;\n\n  /**\n   * Nucleus sampling, where the model considers the results of the tokens\n   * with top_p probability mass. So 0.1 means only the tokens comprising\n   * the top 10% probability mass are considered.\n   *\n   * We generally recommend altering this or temperature but not both.\n   *\n   * Default: 1\n   */\n  topP?: number;\n\n  /**\n   * Whether to inject a safety prompt before all conversations.\n   *\n   * Default: false\n   */\n  safeMode?: boolean;\n\n  /**\n   * The seed to use for random sampling. If set, different calls will\n   * generate deterministic results.\n   */\n  randomSeed?: number | null;\n}\n\nexport class MistralChatModel\n  extends AbstractModel<MistralChatModelSettings>\n  implements\n    TextStreamingBaseModel<MistralChatPrompt, MistralChatModelSettings>\n{\n  constructor(settings: MistralChatModelSettings) {\n    super({ settings });\n  }\n\n  readonly provider = \"mistral\";\n  get modelName() {\n    return this.settings.model;\n  }\n\n  readonly contextWindowSize = undefined;\n  readonly tokenizer = undefined;\n  readonly countPromptTokens = undefined;\n\n  async callAPI<RESULT>(\n    prompt: MistralChatPrompt,\n    callOptions: FunctionCallOptions,\n    options: {\n      responseFormat: MistralChatResponseFormatType<RESULT>;\n    }\n  ) {\n    const api = this.settings.api ?? new MistralApiConfiguration();\n    const abortSignal = callOptions.run?.abortSignal;\n    const stream = options.responseFormat.stream;\n    const successfulResponseHandler = options.responseFormat.handler;\n\n    return callWithRetryAndThrottle({\n      retry: api.retry,\n      throttle: api.throttle,\n      call: async () =>\n        postJsonToApi({\n          url: api.assembleUrl(`/chat/completions`),\n          headers: api.headers({\n            functionType: callOptions.functionType,\n            functionId: callOptions.functionId,\n            run: callOptions.run,\n            callId: callOptions.callId,\n          }),\n          body: {\n            stream,\n            messages: prompt,\n            model: this.settings.model,\n            temperature: this.settings.temperature,\n            top_p: this.settings.topP,\n            max_tokens: this.settings.maxGenerationTokens,\n            safe_mode: this.settings.safeMode,\n            random_seed: this.settings.randomSeed,\n          },\n          failedResponseHandler: failedMistralCallResponseHandler,\n          successfulResponseHandler,\n          abortSignal,\n        }),\n    });\n  }\n\n  get settingsForEvent(): Partial<MistralChatModelSettings> {\n    const eventSettingProperties: Array<string> = [\n      ...textGenerationModelProperties,\n\n      \"temperature\",\n      \"topP\",\n      \"safeMode\",\n      \"randomSeed\",\n    ] satisfies (keyof MistralChatModelSettings)[];\n\n    return Object.fromEntries(\n      Object.entries(this.settings).filter(([key]) =>\n        eventSettingProperties.includes(key)\n      )\n    );\n  }\n\n  async doGenerateTexts(\n    prompt: MistralChatPrompt,\n    options: FunctionCallOptions\n  ) {\n    return this.processTextGenerationResponse(\n      await this.callAPI(prompt, options, {\n        responseFormat: MistralChatResponseFormat.json,\n      })\n    );\n  }\n\n  restoreGeneratedTexts(rawResponse: unknown) {\n    return this.processTextGenerationResponse(\n      validateTypes({\n        value: rawResponse,\n        schema: zodSchema(mistralChatResponseSchema),\n      })\n    );\n  }\n\n  processTextGenerationResponse(rawResponse: MistralChatResponse) {\n    return {\n      rawResponse,\n      textGenerationResults: rawResponse.choices.map((choice) => ({\n        text: choice.message.content,\n        finishReason: this.translateFinishReason(choice.finish_reason),\n      })),\n    };\n  }\n\n  private translateFinishReason(\n    finishReason: string | null | undefined\n  ): TextGenerationFinishReason {\n    switch (finishReason) {\n      case \"stop\":\n        return \"stop\";\n      case \"length\":\n      case \"model_length\":\n        return \"length\";\n      default:\n        return \"unknown\";\n    }\n  }\n\n  doStreamText(prompt: MistralChatPrompt, options: FunctionCallOptions) {\n    return this.callAPI(prompt, options, {\n      responseFormat: MistralChatResponseFormat.textDeltaIterable,\n    });\n  }\n\n  extractTextDelta(delta: unknown) {\n    const chunk = delta as MistralChatStreamChunk;\n    return chunk.choices[0].delta.content ?? undefined;\n  }\n\n  withTextPrompt() {\n    return this.withPromptTemplate(text());\n  }\n\n  withInstructionPrompt() {\n    return this.withPromptTemplate(instruction());\n  }\n\n  withChatPrompt() {\n    return this.withPromptTemplate(chat());\n  }\n\n  withJsonOutput(): this {\n    return this;\n  }\n\n  withPromptTemplate<INPUT_PROMPT>(\n    promptTemplate: TextGenerationPromptTemplate<\n      INPUT_PROMPT,\n      MistralChatPrompt\n    >\n  ): PromptTemplateTextStreamingModel<\n    INPUT_PROMPT,\n    MistralChatPrompt,\n    MistralChatModelSettings,\n    this\n  > {\n    return new PromptTemplateTextStreamingModel({\n      model: this, // stop tokens are not supported by this model\n      promptTemplate,\n    });\n  }\n\n  withSettings(additionalSettings: Partial<MistralChatModelSettings>) {\n    return new MistralChatModel(\n      Object.assign({}, this.settings, additionalSettings)\n    ) as this;\n  }\n}\n\nconst mistralChatResponseSchema = z.object({\n  id: z.string(),\n  object: z.string(),\n  created: z.number(),\n  model: z.string(),\n  choices: z.array(\n    z.object({\n      index: z.number(),\n      message: z.object({\n        role: z.enum([\"user\", \"assistant\"]),\n        content: z.string(),\n      }),\n      finish_reason: z.enum([\"stop\", \"length\", \"model_length\"]),\n    })\n  ),\n  usage: z.object({\n    prompt_tokens: z.number(),\n    completion_tokens: z.number(),\n    total_tokens: z.number(),\n  }),\n});\n\nexport type MistralChatResponse = z.infer<typeof mistralChatResponseSchema>;\n\nconst mistralChatStreamChunkSchema = z.object({\n  id: z.string(),\n  object: z.string().optional(),\n  created: z.number().optional(),\n  model: z.string(),\n  choices: z.array(\n    z.object({\n      index: z.number(),\n      delta: z.object({\n        role: z.enum([\"assistant\", \"user\"]).optional().nullable(),\n        content: z.string().nullable().optional(),\n      }),\n      finish_reason: z\n        .enum([\"stop\", \"length\", \"model_length\"])\n        .nullable()\n        .optional(),\n    })\n  ),\n});\n\nexport type MistralChatStreamChunk = z.infer<\n  typeof mistralChatStreamChunkSchema\n>;\n\nexport type MistralChatResponseFormatType<T> = {\n  stream: boolean;\n  handler: ResponseHandler<T>;\n};\n\nexport const MistralChatResponseFormat = {\n  /**\n   * Returns the response as a JSON object.\n   */\n  json: {\n    stream: false,\n    handler: createJsonResponseHandler(zodSchema(mistralChatResponseSchema)),\n  },\n\n  /**\n   * Returns an async iterable over the text deltas (only the tex different of the first choice).\n   */\n  textDeltaIterable: {\n    stream: true,\n    handler: createEventSourceResponseHandler(\n      zodSchema(mistralChatStreamChunkSchema)\n    ),\n  },\n};\n","import { safeParseJSON } from \"../../core/schema/parseJSON\";\nimport { Schema } from \"../../core/schema/Schema\";\nimport { Delta } from \"../../model-function/Delta\";\nimport { AsyncQueue } from \"../AsyncQueue\";\nimport { parseEventSourceStream } from \"./parseEventSourceStream\";\n\nexport async function parseEventSourceStreamAsAsyncIterable<T>({\n  stream,\n  schema,\n}: {\n  stream: ReadableStream<Uint8Array>;\n  schema: Schema<T>;\n}): Promise<AsyncIterable<Delta<T>>> {\n  const queue = new AsyncQueue<Delta<T>>();\n\n  // process the stream asynchonously (no 'await' on purpose):\n  parseEventSourceStream({ stream })\n    .then(async (events) => {\n      try {\n        for await (const event of events) {\n          const data = event.data;\n\n          if (data === \"[DONE]\") {\n            queue.close();\n            return;\n          }\n\n          const parseResult = safeParseJSON({\n            text: data,\n            schema,\n          });\n\n          if (!parseResult.success) {\n            queue.push({\n              type: \"error\",\n              error: parseResult.error,\n            });\n\n            // Note: the queue is not closed on purpose. Some providers might add additional\n            // chunks that are not parsable, and ModelFusion should be resilient to that.\n            continue;\n          }\n\n          const completionChunk = parseResult.value;\n\n          queue.push({\n            type: \"delta\",\n            deltaValue: completionChunk,\n          });\n        }\n      } catch (error) {\n        queue.push({ type: \"error\", error });\n        queue.close();\n        return;\n      }\n    })\n    .catch((error) => {\n      queue.push({ type: \"error\", error });\n      queue.close();\n      return;\n    });\n\n  return queue;\n}\n","import { Schema } from \"../../core/schema/Schema\";\nimport { parseEventSourceStreamAsAsyncIterable } from \"./parseEventSourceStreamAsAsyncIterable\";\n\nexport const createEventSourceResponseHandler =\n  <T>(schema: Schema<T>) =>\n  ({ response }: { response: Response }) =>\n    parseEventSourceStreamAsAsyncIterable({\n      stream: response.body!,\n      schema,\n    });\n","import { TextGenerationPromptTemplate } from \"../../model-function/generate-text/TextGenerationPromptTemplate\";\nimport { ChatPrompt } from \"../../model-function/generate-text/prompt-template/ChatPrompt\";\nimport { validateContentIsString } from \"../../model-function/generate-text/prompt-template/ContentPart\";\nimport { InstructionPrompt } from \"../../model-function/generate-text/prompt-template/InstructionPrompt\";\nimport { InvalidPromptError } from \"../../model-function/generate-text/prompt-template/InvalidPromptError\";\nimport { MistralChatPrompt } from \"./MistralChatModel\";\n\n/**\n * Formats a text prompt as a Mistral prompt.\n */\nexport function text(): TextGenerationPromptTemplate<\n  string,\n  MistralChatPrompt\n> {\n  return {\n    format: (prompt) => [{ role: \"user\", content: prompt }],\n    stopSequences: [],\n  };\n}\n\n/**\n * Formats an instruction prompt as a Mistral prompt.\n */\nexport function instruction(): TextGenerationPromptTemplate<\n  InstructionPrompt,\n  MistralChatPrompt\n> {\n  return {\n    format(prompt) {\n      const messages: MistralChatPrompt = [];\n\n      if (prompt.system != null) {\n        messages.push({ role: \"system\", content: prompt.system });\n      }\n\n      const instruction = validateContentIsString(prompt.instruction, prompt);\n      messages.push({ role: \"user\", content: instruction });\n\n      return messages;\n    },\n    stopSequences: [],\n  };\n}\n\n/**\n * Formats a chat prompt as a Mistral prompt.\n */\nexport function chat(): TextGenerationPromptTemplate<\n  ChatPrompt,\n  MistralChatPrompt\n> {\n  return {\n    format(prompt) {\n      const messages: MistralChatPrompt = [];\n\n      if (prompt.system != null) {\n        messages.push({ role: \"system\", content: prompt.system });\n      }\n\n      for (const { role, content } of prompt.messages) {\n        switch (role) {\n          case \"user\": {\n            const textContent = validateContentIsString(content, prompt);\n            messages.push({ role: \"user\", content: textContent });\n            break;\n          }\n          case \"assistant\": {\n            messages.push({\n              role: \"assistant\",\n              content: validateContentIsString(content, prompt),\n            });\n            break;\n          }\n          case \"tool\": {\n            throw new InvalidPromptError(\n              \"Tool messages are not supported.\",\n              prompt\n            );\n          }\n          default: {\n            const _exhaustiveCheck: never = role;\n            throw new Error(`Unsupported role: ${_exhaustiveCheck}`);\n          }\n        }\n      }\n\n      return messages;\n    },\n    stopSequences: [],\n  };\n}\n","import { z } from \"zod\";\nimport { ApiCallError } from \"../../core/api/ApiCallError\";\nimport {\n  ResponseHandler,\n  createJsonErrorResponseHandler,\n} from \"../../core/api/postToApi\";\nimport { zodSchema } from \"../../core/schema/ZodSchema\";\n\nconst mistralErrorDataSchema = z.object({\n  object: z.literal(\"error\"),\n  message: z.string(),\n  type: z.string(),\n  param: z.string().nullable(),\n  code: z.string(),\n});\n\nexport type MistralErrorData = z.infer<typeof mistralErrorDataSchema>;\n\nexport const failedMistralCallResponseHandler: ResponseHandler<ApiCallError> =\n  createJsonErrorResponseHandler({\n    errorSchema: zodSchema(mistralErrorDataSchema),\n    errorToMessage: (error) => error.message,\n  });\n","import { PartialBaseUrlPartsApiConfigurationOptions } from \"../../core/api/BaseUrlApiConfiguration\";\nimport { MistralApiConfiguration } from \"./MistralApiConfiguration\";\nimport { MistralChatModel, MistralChatModelSettings } from \"./MistralChatModel\";\nimport {\n  MistralTextEmbeddingModel,\n  MistralTextEmbeddingModelSettings,\n} from \"./MistralTextEmbeddingModel\";\n\n/**\n * Creates an API configuration for the Mistral API.\n * It calls the API at https://api.mistral.ai/v1 and uses the `MISTRAL_API_KEY` env variable by default.\n */\nexport function Api(\n  settings: PartialBaseUrlPartsApiConfigurationOptions & {\n    apiKey?: string;\n  }\n) {\n  return new MistralApiConfiguration(settings);\n}\n\nexport function ChatTextGenerator(settings: MistralChatModelSettings) {\n  return new MistralChatModel(settings);\n}\n\nexport function TextEmbedder(settings: MistralTextEmbeddingModelSettings) {\n  return new MistralTextEmbeddingModel(settings);\n}\n\nexport {\n  MistralChatMessage as ChatMessage,\n  MistralChatPrompt as ChatPrompt,\n} from \"./MistralChatModel\";\n","import { z } from \"zod\";\nimport { FunctionCallOptions } from \"../../core/FunctionOptions\";\nimport { ApiConfiguration } from \"../../core/api/ApiConfiguration\";\nimport { callWithRetryAndThrottle } from \"../../core/api/callWithRetryAndThrottle\";\nimport {\n  createJsonResponseHandler,\n  postJsonToApi,\n} from \"../../core/api/postToApi\";\nimport { zodSchema } from \"../../core/schema/ZodSchema\";\nimport { AbstractModel } from \"../../model-function/AbstractModel\";\nimport {\n  EmbeddingModel,\n  EmbeddingModelSettings,\n} from \"../../model-function/embed/EmbeddingModel\";\nimport { MistralApiConfiguration } from \"./MistralApiConfiguration\";\nimport { failedMistralCallResponseHandler } from \"./MistralError\";\n\nexport interface MistralTextEmbeddingModelSettings\n  extends EmbeddingModelSettings {\n  api?: ApiConfiguration;\n\n  /**\n   * The ID of the model to use for this request.\n   */\n  model: \"mistral-embed\";\n\n  /**\n   * The format of the output data.\n   *\n   * Default: \"float\"\n   */\n  encodingFormat?: \"float\";\n}\n\nexport class MistralTextEmbeddingModel\n  extends AbstractModel<MistralTextEmbeddingModelSettings>\n  implements EmbeddingModel<string, MistralTextEmbeddingModelSettings>\n{\n  constructor(settings: MistralTextEmbeddingModelSettings) {\n    super({ settings });\n  }\n\n  readonly provider = \"mistral\" as const;\n  get modelName() {\n    return this.settings.model;\n  }\n\n  readonly maxValuesPerCall = 32;\n\n  /**\n   * Parallel calls are technically possible, but I have been hitting rate limits and disabled\n   * them for now.\n   */\n  readonly isParallelizable = false;\n\n  readonly dimensions = 1024;\n\n  async callAPI(\n    texts: Array<string>,\n    callOptions: FunctionCallOptions\n  ): Promise<MistralTextEmbeddingResponse> {\n    if (texts.length > this.maxValuesPerCall) {\n      throw new Error(\n        `The Mistral embedding API only supports ${this.maxValuesPerCall} texts per API call.`\n      );\n    }\n\n    const api = this.settings.api ?? new MistralApiConfiguration();\n    const abortSignal = callOptions.run?.abortSignal;\n    const model = this.settings.model;\n    const encodingFormat = this.settings.encodingFormat ?? \"float\";\n\n    return callWithRetryAndThrottle({\n      retry: this.settings.api?.retry,\n      throttle: this.settings.api?.throttle,\n      call: async () =>\n        postJsonToApi({\n          url: api.assembleUrl(`/embeddings`),\n          headers: api.headers({\n            functionType: callOptions.functionType,\n            functionId: callOptions.functionId,\n            run: callOptions.run,\n            callId: callOptions.callId,\n          }),\n          body: {\n            model,\n            input: texts,\n            encoding_format: encodingFormat,\n          },\n          failedResponseHandler: failedMistralCallResponseHandler,\n          successfulResponseHandler: createJsonResponseHandler(\n            zodSchema(MistralTextEmbeddingResponseSchema)\n          ),\n          abortSignal,\n        }),\n    });\n  }\n\n  get settingsForEvent(): Partial<MistralTextEmbeddingModelSettings> {\n    return {\n      encodingFormat: this.settings.encodingFormat,\n    };\n  }\n\n  async doEmbedValues(texts: string[], options: FunctionCallOptions) {\n    const rawResponse = await this.callAPI(texts, options);\n\n    return {\n      rawResponse,\n      embeddings: rawResponse.data.map((entry) => entry.embedding),\n    };\n  }\n\n  withSettings(additionalSettings: Partial<MistralTextEmbeddingModelSettings>) {\n    return new MistralTextEmbeddingModel(\n      Object.assign({}, this.settings, additionalSettings)\n    ) as this;\n  }\n}\n\nconst MistralTextEmbeddingResponseSchema = z.object({\n  id: z.string(),\n  object: z.string(),\n  data: z.array(\n    z.object({\n      object: z.string(),\n      embedding: z.array(z.number()),\n      index: z.number(),\n    })\n  ),\n  model: z.string(),\n  usage: z.object({\n    prompt_tokens: z.number(),\n    total_tokens: z.number(),\n  }),\n});\n\nexport type MistralTextEmbeddingResponse = z.infer<\n  typeof MistralTextEmbeddingResponseSchema\n>;\n","import {\n  BaseUrlApiConfigurationWithDefaults,\n  PartialBaseUrlPartsApiConfigurationOptions,\n} from \"../../core/api/BaseUrlApiConfiguration\";\n\n/**\n * Creates an API configuration for the Ollama API.\n * It calls the API at http://127.0.0.1:11434 by default.\n */\nexport class OllamaApiConfiguration extends BaseUrlApiConfigurationWithDefaults {\n  constructor(settings: PartialBaseUrlPartsApiConfigurationOptions = {}) {\n    super({\n      ...settings,\n      baseUrlDefaults: {\n        protocol: \"http\",\n        host: \"127.0.0.1\",\n        port: \"11434\",\n        path: \"\",\n      },\n    });\n  }\n}\n","import { z } from \"zod\";\nimport { FunctionCallOptions } from \"../../core/FunctionOptions\";\nimport { ApiCallError } from \"../../core/api/ApiCallError\";\nimport { ApiConfiguration } from \"../../core/api/ApiConfiguration\";\nimport { callWithRetryAndThrottle } from \"../../core/api/callWithRetryAndThrottle\";\nimport { ResponseHandler, postJsonToApi } from \"../../core/api/postToApi\";\nimport { zodSchema } from \"../../core/schema/ZodSchema\";\nimport { safeParseJSON } from \"../../core/schema/parseJSON\";\nimport { validateTypes } from \"../../core/schema/validateTypes\";\nimport { AbstractModel } from \"../../model-function/AbstractModel\";\nimport {\n  FlexibleObjectFromTextPromptTemplate,\n  ObjectFromTextPromptTemplate,\n} from \"../../model-function/generate-object/ObjectFromTextPromptTemplate\";\nimport { ObjectFromTextStreamingModel } from \"../../model-function/generate-object/ObjectFromTextStreamingModel\";\nimport { PromptTemplateTextStreamingModel } from \"../../model-function/generate-text/PromptTemplateTextStreamingModel\";\nimport {\n  TextStreamingBaseModel,\n  TextStreamingModel,\n  textGenerationModelProperties,\n} from \"../../model-function/generate-text/TextGenerationModel\";\nimport { TextGenerationPromptTemplate } from \"../../model-function/generate-text/TextGenerationPromptTemplate\";\nimport { TextGenerationToolCallModel } from \"../../tool/generate-tool-call/TextGenerationToolCallModel\";\nimport { ToolCallPromptTemplate } from \"../../tool/generate-tool-call/ToolCallPromptTemplate\";\nimport { TextGenerationToolCallsModel } from \"../../tool/generate-tool-calls/TextGenerationToolCallsModel\";\nimport { ToolCallsPromptTemplate } from \"../../tool/generate-tool-calls/ToolCallsPromptTemplate\";\nimport { createJsonStreamResponseHandler } from \"../../util/streaming/createJsonStreamResponseHandler\";\nimport { OllamaApiConfiguration } from \"./OllamaApiConfiguration\";\nimport { chat, instruction, text } from \"./OllamaChatPromptTemplate\";\nimport { failedOllamaCallResponseHandler } from \"./OllamaError\";\nimport { OllamaTextGenerationSettings } from \"./OllamaTextGenerationSettings\";\n\nexport type OllamaChatMessage = {\n  role: \"system\" | \"user\" | \"assistant\";\n  content: string;\n\n  /**\n   Images. Supports base64-encoded `png` and `jpeg` images up to 100MB in size.\n   */\n  images?: Array<string>;\n};\n\nexport type OllamaChatPrompt = Array<OllamaChatMessage>;\n\nexport interface OllamaChatModelSettings extends OllamaTextGenerationSettings {\n  api?: ApiConfiguration;\n}\n\n/**\n * Text generation model that uses the Ollama chat API.\n */\nexport class OllamaChatModel\n  extends AbstractModel<OllamaChatModelSettings>\n  implements TextStreamingBaseModel<OllamaChatPrompt, OllamaChatModelSettings>\n{\n  constructor(settings: OllamaChatModelSettings) {\n    super({ settings });\n  }\n\n  readonly provider = \"ollama\";\n  get modelName() {\n    return this.settings.model;\n  }\n\n  readonly tokenizer = undefined;\n  readonly countPromptTokens = undefined;\n  readonly contextWindowSize = undefined;\n\n  async callAPI<RESPONSE>(\n    prompt: OllamaChatPrompt,\n    callOptions: FunctionCallOptions,\n    options: {\n      responseFormat: OllamaChatResponseFormatType<RESPONSE>;\n    }\n  ): Promise<RESPONSE> {\n    const { responseFormat } = options;\n    const api = this.settings.api ?? new OllamaApiConfiguration();\n    const abortSignal = callOptions.run?.abortSignal;\n\n    return callWithRetryAndThrottle({\n      retry: api.retry,\n      throttle: api.throttle,\n      call: async () =>\n        postJsonToApi({\n          url: api.assembleUrl(`/api/chat`),\n          headers: api.headers({\n            functionType: callOptions.functionType,\n            functionId: callOptions.functionId,\n            run: callOptions.run,\n            callId: callOptions.callId,\n          }),\n          body: {\n            stream: responseFormat.stream,\n            model: this.settings.model,\n            messages: prompt,\n            format: this.settings.format,\n            options: {\n              mirostat: this.settings.mirostat,\n              mirostat_eta: this.settings.mirostatEta,\n              mirostat_tau: this.settings.mirostatTau,\n              num_gpu: this.settings.numGpu,\n              num_gqa: this.settings.numGqa,\n              num_predict: this.settings.maxGenerationTokens,\n              num_threads: this.settings.numThreads,\n              repeat_last_n: this.settings.repeatLastN,\n              repeat_penalty: this.settings.repeatPenalty,\n              seed: this.settings.seed,\n              stop: this.settings.stopSequences,\n              temperature: this.settings.temperature,\n              tfs_z: this.settings.tfsZ,\n              top_k: this.settings.topK,\n              top_p: this.settings.topP,\n            },\n            template: this.settings.template,\n          },\n          failedResponseHandler: failedOllamaCallResponseHandler,\n          successfulResponseHandler: responseFormat.handler,\n          abortSignal,\n        }),\n    });\n  }\n\n  get settingsForEvent(): Partial<OllamaChatModelSettings> {\n    const eventSettingProperties: Array<string> = [\n      ...textGenerationModelProperties,\n\n      \"temperature\",\n      \"mirostat\",\n      \"mirostatEta\",\n      \"mirostatTau\",\n      \"numGqa\",\n      \"numGpu\",\n      \"numThreads\",\n      \"repeatLastN\",\n      \"repeatPenalty\",\n      \"seed\",\n      \"tfsZ\",\n      \"topK\",\n      \"topP\",\n      \"template\",\n      \"format\",\n    ] satisfies (keyof OllamaChatModelSettings)[];\n\n    return Object.fromEntries(\n      Object.entries(this.settings).filter(([key]) =>\n        eventSettingProperties.includes(key)\n      )\n    );\n  }\n\n  async doGenerateTexts(\n    prompt: OllamaChatPrompt,\n    options: FunctionCallOptions\n  ) {\n    return this.processTextGenerationResponse(\n      await this.callAPI(prompt, options, {\n        responseFormat: OllamaChatResponseFormat.json,\n      })\n    );\n  }\n\n  restoreGeneratedTexts(rawResponse: unknown) {\n    return this.processTextGenerationResponse(\n      validateTypes({\n        value: rawResponse,\n        schema: zodSchema(ollamaChatResponseSchema),\n      })\n    );\n  }\n\n  private processTextGenerationResponse(rawResponse: OllamaChatResponse) {\n    return {\n      rawResponse,\n      textGenerationResults: [\n        {\n          text: rawResponse.message.content,\n          finishReason: \"unknown\" as const,\n        },\n      ],\n    };\n  }\n\n  doStreamText(prompt: OllamaChatPrompt, options: FunctionCallOptions) {\n    return this.callAPI(prompt, options, {\n      responseFormat: OllamaChatResponseFormat.deltaIterable,\n    });\n  }\n\n  extractTextDelta(delta: unknown) {\n    const chunk = delta as OllamaChatStreamChunk;\n    return chunk.done === true ? undefined : chunk.message.content;\n  }\n\n  asToolCallGenerationModel<INPUT_PROMPT>(\n    promptTemplate: ToolCallPromptTemplate<INPUT_PROMPT, OllamaChatPrompt>\n  ) {\n    return new TextGenerationToolCallModel({\n      model: this,\n      template: promptTemplate,\n    });\n  }\n\n  asToolCallsOrTextGenerationModel<INPUT_PROMPT>(\n    promptTemplate: ToolCallsPromptTemplate<INPUT_PROMPT, OllamaChatPrompt>\n  ) {\n    return new TextGenerationToolCallsModel({\n      model: this,\n      template: promptTemplate,\n    });\n  }\n\n  asObjectGenerationModel<INPUT_PROMPT, OllamaChatPrompt>(\n    promptTemplate:\n      | ObjectFromTextPromptTemplate<INPUT_PROMPT, OllamaChatPrompt>\n      | FlexibleObjectFromTextPromptTemplate<INPUT_PROMPT, unknown>\n  ) {\n    return \"adaptModel\" in promptTemplate\n      ? new ObjectFromTextStreamingModel({\n          model: promptTemplate.adaptModel(this),\n          template: promptTemplate,\n        })\n      : new ObjectFromTextStreamingModel({\n          model: this as TextStreamingModel<OllamaChatPrompt>,\n          template: promptTemplate,\n        });\n  }\n\n  withTextPrompt() {\n    return this.withPromptTemplate(text());\n  }\n\n  withInstructionPrompt() {\n    return this.withPromptTemplate(instruction());\n  }\n\n  withChatPrompt() {\n    return this.withPromptTemplate(chat());\n  }\n\n  withPromptTemplate<INPUT_PROMPT>(\n    promptTemplate: TextGenerationPromptTemplate<INPUT_PROMPT, OllamaChatPrompt>\n  ): PromptTemplateTextStreamingModel<\n    INPUT_PROMPT,\n    OllamaChatPrompt,\n    OllamaChatModelSettings,\n    this\n  > {\n    return new PromptTemplateTextStreamingModel({\n      model: this.withSettings({\n        stopSequences: [\n          ...(this.settings.stopSequences ?? []),\n          ...promptTemplate.stopSequences,\n        ],\n      }),\n      promptTemplate,\n    });\n  }\n\n  withJsonOutput() {\n    return this.withSettings({ format: \"json\" });\n  }\n\n  withSettings(additionalSettings: Partial<OllamaChatModelSettings>) {\n    return new OllamaChatModel(\n      Object.assign({}, this.settings, additionalSettings)\n    ) as this;\n  }\n}\n\nconst ollamaChatResponseSchema = z.object({\n  model: z.string(),\n  created_at: z.string(),\n  done: z.literal(true),\n  message: z.object({\n    role: z.string(),\n    content: z.string(),\n  }),\n  total_duration: z.number(),\n  load_duration: z.number().optional(),\n  prompt_eval_count: z.number().optional(),\n  prompt_eval_duration: z.number().optional(),\n  eval_count: z.number(),\n  eval_duration: z.number(),\n});\n\nexport type OllamaChatResponse = z.infer<typeof ollamaChatResponseSchema>;\n\nconst ollamaChatStreamChunkSchema = z.discriminatedUnion(\"done\", [\n  z.object({\n    done: z.literal(false),\n    model: z.string(),\n    created_at: z.string(),\n    message: z.object({\n      role: z.string(),\n      content: z.string(),\n    }),\n  }),\n  z.object({\n    done: z.literal(true),\n    model: z.string(),\n    created_at: z.string(),\n    total_duration: z.number(),\n    load_duration: z.number().optional(),\n    prompt_eval_count: z.number().optional(),\n    prompt_eval_duration: z.number().optional(),\n    eval_count: z.number(),\n    eval_duration: z.number(),\n  }),\n]);\n\nexport type OllamaChatStreamChunk = z.infer<typeof ollamaChatStreamChunkSchema>;\n\nexport type OllamaChatResponseFormatType<T> = {\n  stream: boolean;\n  handler: ResponseHandler<T>;\n};\n\nexport const OllamaChatResponseFormat = {\n  /**\n   * Returns the response as a JSON object.\n   */\n  json: {\n    stream: false,\n    handler: (async ({ response, url, requestBodyValues }) => {\n      const responseBody = await response.text();\n\n      const parsedResult = safeParseJSON({\n        text: responseBody,\n        schema: zodSchema(\n          z.union([\n            ollamaChatResponseSchema,\n            z.object({\n              done: z.literal(false),\n              model: z.string(),\n              created_at: z.string(),\n            }),\n          ])\n        ),\n      });\n\n      if (!parsedResult.success) {\n        throw new ApiCallError({\n          message: \"Invalid JSON response\",\n          cause: parsedResult.error,\n          statusCode: response.status,\n          responseBody,\n          url,\n          requestBodyValues,\n        });\n      }\n\n      if (parsedResult.value.done === false) {\n        throw new ApiCallError({\n          message: \"Incomplete Ollama response received\",\n          statusCode: response.status,\n          responseBody,\n          url,\n          requestBodyValues,\n          isRetryable: true,\n        });\n      }\n\n      return parsedResult.value;\n    }) satisfies ResponseHandler<OllamaChatResponse>,\n  } satisfies OllamaChatResponseFormatType<OllamaChatResponse>,\n\n  /**\n   * Returns an async iterable over the full deltas (all choices, including full current state at time of event)\n   * of the response stream.\n   */\n  deltaIterable: {\n    stream: true,\n    handler: createJsonStreamResponseHandler(\n      zodSchema(ollamaChatStreamChunkSchema)\n    ),\n  },\n};\n","import { TextGenerationPromptTemplate } from \"../../model-function/generate-text/TextGenerationPromptTemplate\";\nimport {\n  ChatPrompt,\n  UserContent,\n} from \"../../model-function/generate-text/prompt-template/ChatPrompt\";\nimport { validateContentIsString } from \"../../model-function/generate-text/prompt-template/ContentPart\";\nimport { InstructionPrompt } from \"../../model-function/generate-text/prompt-template/InstructionPrompt\";\nimport { InvalidPromptError } from \"../../model-function/generate-text/prompt-template/InvalidPromptError\";\nimport { convertDataContentToBase64String } from \"../../util/format/DataContent\";\nimport { OllamaChatPrompt } from \"./OllamaChatModel\";\n\n/**\n * OllamaChatPrompt identity chat format.\n */\nexport function identity(): TextGenerationPromptTemplate<\n  OllamaChatPrompt,\n  OllamaChatPrompt\n> {\n  return { format: (prompt) => prompt, stopSequences: [] };\n}\n\n/**\n * Formats a text prompt as an Ollama chat prompt.\n */\nexport function text(): TextGenerationPromptTemplate<string, OllamaChatPrompt> {\n  return {\n    format: (prompt) => [{ role: \"user\", content: prompt }],\n    stopSequences: [],\n  };\n}\n\n/**\n * Formats an instruction prompt as an Ollama chat prompt.\n */\nexport function instruction(): TextGenerationPromptTemplate<\n  InstructionPrompt,\n  OllamaChatPrompt\n> {\n  return {\n    format(prompt) {\n      const messages: OllamaChatPrompt = [];\n\n      if (prompt.system != null) {\n        messages.push({\n          role: \"system\",\n          content: prompt.system,\n        });\n      }\n\n      messages.push({\n        role: \"user\",\n        ...extractUserContent(prompt.instruction),\n      });\n\n      return messages;\n    },\n    stopSequences: [],\n  };\n}\n\n/**\n * Formats a chat prompt as an Ollama chat prompt.\n */\nexport function chat(): TextGenerationPromptTemplate<\n  ChatPrompt,\n  OllamaChatPrompt\n> {\n  return {\n    format(prompt) {\n      const messages: OllamaChatPrompt = [];\n\n      if (prompt.system != null) {\n        messages.push({ role: \"system\", content: prompt.system });\n      }\n\n      for (const { role, content } of prompt.messages) {\n        switch (role) {\n          case \"user\": {\n            messages.push({\n              role: \"user\",\n              ...extractUserContent(content),\n            });\n            break;\n          }\n\n          case \"assistant\": {\n            messages.push({\n              role: \"assistant\",\n              content: validateContentIsString(content, prompt),\n            });\n            break;\n          }\n\n          case \"tool\": {\n            throw new InvalidPromptError(\n              \"Tool messages are not supported.\",\n              prompt\n            );\n          }\n\n          default: {\n            const _exhaustiveCheck: never = role;\n            throw new Error(`Unsupported role: ${_exhaustiveCheck}`);\n          }\n        }\n      }\n\n      return messages;\n    },\n    stopSequences: [],\n  };\n}\n\nfunction extractUserContent(input: UserContent) {\n  if (typeof input === \"string\") {\n    return { content: input, images: undefined };\n  }\n\n  const images: string[] = [];\n  let content = \"\";\n\n  for (const part of input) {\n    if (part.type === \"text\") {\n      content += part.text;\n    } else {\n      images.push(convertDataContentToBase64String(part.image));\n    }\n  }\n\n  return { content, images };\n}\n","import { z } from \"zod\";\nimport { ApiCallError } from \"../../core/api/ApiCallError\";\nimport {\n  ResponseHandler,\n  createJsonErrorResponseHandler,\n} from \"../../core/api/postToApi\";\nimport { zodSchema } from \"../../core/schema/ZodSchema\";\n\nconst ollamaErrorDataSchema = z.object({\n  error: z.string(),\n});\n\nexport type OllamaErrorData = z.infer<typeof ollamaErrorDataSchema>;\n\nexport const failedOllamaCallResponseHandler: ResponseHandler<ApiCallError> =\n  createJsonErrorResponseHandler({\n    errorSchema: zodSchema(ollamaErrorDataSchema),\n    errorToMessage: (error) => error.error,\n  });\n","import { z } from \"zod\";\nimport { FunctionCallOptions } from \"../../core/FunctionOptions\";\nimport { ApiCallError } from \"../../core/api/ApiCallError\";\nimport { ApiConfiguration } from \"../../core/api/ApiConfiguration\";\nimport { callWithRetryAndThrottle } from \"../../core/api/callWithRetryAndThrottle\";\nimport { ResponseHandler, postJsonToApi } from \"../../core/api/postToApi\";\nimport { zodSchema } from \"../../core/schema/ZodSchema\";\nimport { safeParseJSON } from \"../../core/schema/parseJSON\";\nimport { validateTypes } from \"../../core/schema/validateTypes\";\nimport { AbstractModel } from \"../../model-function/AbstractModel\";\nimport {\n  FlexibleObjectFromTextPromptTemplate,\n  ObjectFromTextPromptTemplate,\n} from \"../../model-function/generate-object/ObjectFromTextPromptTemplate\";\nimport { ObjectFromTextStreamingModel } from \"../../model-function/generate-object/ObjectFromTextStreamingModel\";\nimport { PromptTemplateTextStreamingModel } from \"../../model-function/generate-text/PromptTemplateTextStreamingModel\";\nimport {\n  TextStreamingBaseModel,\n  TextStreamingModel,\n  textGenerationModelProperties,\n} from \"../../model-function/generate-text/TextGenerationModel\";\nimport { TextGenerationPromptTemplate } from \"../../model-function/generate-text/TextGenerationPromptTemplate\";\nimport { ChatPrompt } from \"../../model-function/generate-text/prompt-template/ChatPrompt\";\nimport { InstructionPrompt } from \"../../model-function/generate-text/prompt-template/InstructionPrompt\";\nimport { TextGenerationPromptTemplateProvider } from \"../../model-function/generate-text/prompt-template/PromptTemplateProvider\";\nimport { TextGenerationToolCallModel } from \"../../tool/generate-tool-call/TextGenerationToolCallModel\";\nimport { ToolCallPromptTemplate } from \"../../tool/generate-tool-call/ToolCallPromptTemplate\";\nimport { TextGenerationToolCallsModel } from \"../../tool/generate-tool-calls/TextGenerationToolCallsModel\";\nimport { ToolCallsPromptTemplate } from \"../../tool/generate-tool-calls/ToolCallsPromptTemplate\";\nimport { createJsonStreamResponseHandler } from \"../../util/streaming/createJsonStreamResponseHandler\";\nimport { OllamaApiConfiguration } from \"./OllamaApiConfiguration\";\nimport { Text } from \"./OllamaCompletionPrompt\";\nimport { failedOllamaCallResponseHandler } from \"./OllamaError\";\nimport { OllamaTextGenerationSettings } from \"./OllamaTextGenerationSettings\";\n\nexport interface OllamaCompletionPrompt {\n  /**\n   * Text prompt.\n   */\n  prompt: string;\n\n  /**\n   Images. Supports base64-encoded `png` and `jpeg` images up to 100MB in size.\n   */\n  images?: Array<string>;\n}\n\n/**\n * Text generation model that uses the Ollama completion API.\n *\n * @see https://github.com/jmorganca/ollama/blob/main/docs/api.md#generate-a-completion\n */\nexport interface OllamaCompletionModelSettings<\n  CONTEXT_WINDOW_SIZE extends number | undefined,\n> extends OllamaTextGenerationSettings {\n  api?: ApiConfiguration;\n\n  /**\n   * Specify the context window size of the model that you have loaded in your\n   * Ollama server. (Default: 2048)\n   */\n  contextWindowSize?: CONTEXT_WINDOW_SIZE;\n\n  /**\n   * When set to true, no formatting will be applied to the prompt and no context\n   * will be returned.\n   */\n  raw?: boolean;\n\n  system?: string;\n  context?: number[];\n\n  /**\n   * Prompt template provider that is used when calling `.withTextPrompt()`, `withInstructionPrompt()` or `withChatPrompt()`.\n   */\n  promptTemplate?: TextGenerationPromptTemplateProvider<OllamaCompletionPrompt>;\n}\n\nexport class OllamaCompletionModel<\n    CONTEXT_WINDOW_SIZE extends number | undefined,\n  >\n  extends AbstractModel<OllamaCompletionModelSettings<CONTEXT_WINDOW_SIZE>>\n  implements\n    TextStreamingBaseModel<\n      OllamaCompletionPrompt,\n      OllamaCompletionModelSettings<CONTEXT_WINDOW_SIZE>\n    >\n{\n  constructor(settings: OllamaCompletionModelSettings<CONTEXT_WINDOW_SIZE>) {\n    super({ settings });\n  }\n\n  readonly provider = \"ollama\";\n  get modelName() {\n    return this.settings.model;\n  }\n\n  readonly tokenizer = undefined;\n  readonly countPromptTokens = undefined;\n\n  get contextWindowSize(): CONTEXT_WINDOW_SIZE {\n    return this.settings.contextWindowSize as CONTEXT_WINDOW_SIZE;\n  }\n\n  async callAPI<RESPONSE>(\n    prompt: OllamaCompletionPrompt,\n    callOptions: FunctionCallOptions,\n    options: {\n      responseFormat: OllamaCompletionResponseFormatType<RESPONSE>;\n    }\n  ): Promise<RESPONSE> {\n    const { responseFormat } = options;\n    const api = this.settings.api ?? new OllamaApiConfiguration();\n    const abortSignal = callOptions.run?.abortSignal;\n\n    return callWithRetryAndThrottle({\n      retry: api.retry,\n      throttle: api.throttle,\n      call: async () =>\n        postJsonToApi({\n          url: api.assembleUrl(`/api/generate`),\n          headers: api.headers({\n            functionType: callOptions.functionType,\n            functionId: callOptions.functionId,\n            run: callOptions.run,\n            callId: callOptions.callId,\n          }),\n          body: {\n            stream: responseFormat.stream,\n            model: this.settings.model,\n            prompt: prompt.prompt,\n            images: prompt.images,\n            format: this.settings.format,\n            options: {\n              mirostat: this.settings.mirostat,\n              mirostat_eta: this.settings.mirostatEta,\n              mirostat_tau: this.settings.mirostatTau,\n              num_ctx: this.settings.contextWindowSize,\n              num_gpu: this.settings.numGpu,\n              num_gqa: this.settings.numGqa,\n              num_predict: this.settings.maxGenerationTokens,\n              num_threads: this.settings.numThreads,\n              repeat_last_n: this.settings.repeatLastN,\n              repeat_penalty: this.settings.repeatPenalty,\n              seed: this.settings.seed,\n              stop: this.settings.stopSequences,\n              temperature: this.settings.temperature,\n              tfs_z: this.settings.tfsZ,\n              top_k: this.settings.topK,\n              top_p: this.settings.topP,\n            },\n            system: this.settings.system,\n            template: this.settings.template,\n            context: this.settings.context,\n            raw: this.settings.raw,\n          },\n          failedResponseHandler: failedOllamaCallResponseHandler,\n          successfulResponseHandler: responseFormat.handler,\n          abortSignal,\n        }),\n    });\n  }\n\n  get settingsForEvent(): Partial<\n    OllamaCompletionModelSettings<CONTEXT_WINDOW_SIZE>\n  > {\n    const eventSettingProperties: Array<string> = [\n      ...textGenerationModelProperties,\n\n      \"contextWindowSize\",\n      \"temperature\",\n      \"mirostat\",\n      \"mirostatEta\",\n      \"mirostatTau\",\n      \"numGqa\",\n      \"numGpu\",\n      \"numThreads\",\n      \"repeatLastN\",\n      \"repeatPenalty\",\n      \"seed\",\n      \"tfsZ\",\n      \"topK\",\n      \"topP\",\n      \"system\",\n      \"template\",\n      \"context\",\n      \"format\",\n      \"raw\",\n    ] satisfies (keyof OllamaCompletionModelSettings<CONTEXT_WINDOW_SIZE>)[];\n\n    return Object.fromEntries(\n      Object.entries(this.settings).filter(([key]) =>\n        eventSettingProperties.includes(key)\n      )\n    );\n  }\n\n  async doGenerateTexts(\n    prompt: OllamaCompletionPrompt,\n    options: FunctionCallOptions\n  ) {\n    return this.processTextGenerationResponse(\n      await this.callAPI(prompt, options, {\n        responseFormat: OllamaCompletionResponseFormat.json,\n      })\n    );\n  }\n\n  restoreGeneratedTexts(rawResponse: unknown) {\n    return this.processTextGenerationResponse(\n      validateTypes({\n        value: rawResponse,\n        schema: zodSchema(ollamaCompletionResponseSchema),\n      })\n    );\n  }\n\n  processTextGenerationResponse(rawResponse: OllamaCompletionResponse) {\n    return {\n      rawResponse,\n      textGenerationResults: [\n        {\n          text: rawResponse.response,\n          finishReason: \"unknown\" as const,\n        },\n      ],\n    };\n  }\n\n  doStreamText(prompt: OllamaCompletionPrompt, options: FunctionCallOptions) {\n    return this.callAPI(prompt, options, {\n      ...options,\n      responseFormat: OllamaCompletionResponseFormat.deltaIterable,\n    });\n  }\n\n  extractTextDelta(delta: unknown) {\n    const chunk = delta as OllamaCompletionStreamChunk;\n    return chunk.done === true ? undefined : chunk.response;\n  }\n\n  asObjectGenerationModel<INPUT_PROMPT, OllamaCompletionPrompt>(\n    promptTemplate:\n      | ObjectFromTextPromptTemplate<INPUT_PROMPT, OllamaCompletionPrompt>\n      | FlexibleObjectFromTextPromptTemplate<INPUT_PROMPT, unknown>\n  ) {\n    return \"adaptModel\" in promptTemplate\n      ? new ObjectFromTextStreamingModel({\n          model: promptTemplate.adaptModel(this),\n          template: promptTemplate,\n        })\n      : new ObjectFromTextStreamingModel({\n          model: this as TextStreamingModel<OllamaCompletionPrompt>,\n          template: promptTemplate,\n        });\n  }\n\n  asToolCallGenerationModel<INPUT_PROMPT>(\n    promptTemplate: ToolCallPromptTemplate<INPUT_PROMPT, OllamaCompletionPrompt>\n  ) {\n    return new TextGenerationToolCallModel({\n      model: this,\n      template: promptTemplate,\n    });\n  }\n\n  asToolCallsOrTextGenerationModel<INPUT_PROMPT>(\n    promptTemplate: ToolCallsPromptTemplate<\n      INPUT_PROMPT,\n      OllamaCompletionPrompt\n    >\n  ) {\n    return new TextGenerationToolCallsModel({\n      model: this,\n      template: promptTemplate,\n    });\n  }\n\n  private get promptTemplateProvider(): TextGenerationPromptTemplateProvider<OllamaCompletionPrompt> {\n    return this.settings.promptTemplate ?? Text;\n  }\n\n  withJsonOutput() {\n    return this.withSettings({ format: \"json\" });\n  }\n\n  withTextPrompt(): PromptTemplateTextStreamingModel<\n    string,\n    OllamaCompletionPrompt,\n    OllamaCompletionModelSettings<CONTEXT_WINDOW_SIZE>,\n    this\n  > {\n    return this.withPromptTemplate(this.promptTemplateProvider.text());\n  }\n\n  withInstructionPrompt(): PromptTemplateTextStreamingModel<\n    InstructionPrompt,\n    OllamaCompletionPrompt,\n    OllamaCompletionModelSettings<CONTEXT_WINDOW_SIZE>,\n    this\n  > {\n    return this.withPromptTemplate(this.promptTemplateProvider.instruction());\n  }\n\n  withChatPrompt(): PromptTemplateTextStreamingModel<\n    ChatPrompt,\n    OllamaCompletionPrompt,\n    OllamaCompletionModelSettings<CONTEXT_WINDOW_SIZE>,\n    this\n  > {\n    return this.withPromptTemplate(this.promptTemplateProvider.chat());\n  }\n\n  withPromptTemplate<INPUT_PROMPT>(\n    promptTemplate: TextGenerationPromptTemplate<\n      INPUT_PROMPT,\n      OllamaCompletionPrompt\n    >\n  ): PromptTemplateTextStreamingModel<\n    INPUT_PROMPT,\n    OllamaCompletionPrompt,\n    OllamaCompletionModelSettings<CONTEXT_WINDOW_SIZE>,\n    this\n  > {\n    return new PromptTemplateTextStreamingModel({\n      model: this.withSettings({\n        stopSequences: [\n          ...(this.settings.stopSequences ?? []),\n          ...promptTemplate.stopSequences,\n        ],\n      }),\n      promptTemplate,\n    });\n  }\n\n  withSettings(\n    additionalSettings: Partial<\n      OllamaCompletionModelSettings<CONTEXT_WINDOW_SIZE>\n    >\n  ) {\n    return new OllamaCompletionModel(\n      Object.assign({}, this.settings, additionalSettings)\n    ) as this;\n  }\n}\n\nconst ollamaCompletionResponseSchema = z.object({\n  done: z.literal(true),\n  model: z.string(),\n  created_at: z.string(),\n  response: z.string(),\n  total_duration: z.number(),\n  load_duration: z.number().optional(),\n  prompt_eval_count: z.number().optional(),\n  prompt_eval_duration: z.number().optional(),\n  eval_count: z.number(),\n  eval_duration: z.number(),\n  context: z.array(z.number()).optional(),\n});\n\nexport type OllamaCompletionResponse = z.infer<\n  typeof ollamaCompletionResponseSchema\n>;\n\nconst ollamaCompletionStreamChunkSchema = z.discriminatedUnion(\"done\", [\n  z.object({\n    done: z.literal(false),\n    model: z.string(),\n    created_at: z.string(),\n    response: z.string(),\n  }),\n  z.object({\n    done: z.literal(true),\n    model: z.string(),\n    created_at: z.string(),\n    total_duration: z.number(),\n    load_duration: z.number().optional(),\n    sample_count: z.number().optional(),\n    sample_duration: z.number().optional(),\n    prompt_eval_count: z.number().optional(),\n    prompt_eval_duration: z.number().optional(),\n    eval_count: z.number(),\n    eval_duration: z.number(),\n    context: z.array(z.number()).optional(),\n  }),\n]);\n\nexport type OllamaCompletionStreamChunk = z.infer<\n  typeof ollamaCompletionStreamChunkSchema\n>;\n\nexport type OllamaCompletionResponseFormatType<T> = {\n  stream: boolean;\n  handler: ResponseHandler<T>;\n};\n\nexport const OllamaCompletionResponseFormat = {\n  /**\n   * Returns the response as a JSON object.\n   */\n  json: {\n    stream: false,\n    handler: (async ({ response, url, requestBodyValues }) => {\n      const responseBody = await response.text();\n\n      const parsedResult = safeParseJSON({\n        text: responseBody,\n        schema: zodSchema(\n          z.union([\n            ollamaCompletionResponseSchema,\n            z.object({\n              done: z.literal(false),\n              model: z.string(),\n              created_at: z.string(),\n              response: z.string(),\n            }),\n          ])\n        ),\n      });\n\n      if (!parsedResult.success) {\n        throw new ApiCallError({\n          message: \"Invalid JSON response\",\n          cause: parsedResult.error,\n          statusCode: response.status,\n          responseBody,\n          url,\n          requestBodyValues,\n        });\n      }\n\n      if (parsedResult.value.done === false) {\n        throw new ApiCallError({\n          message: \"Incomplete Ollama response received\",\n          statusCode: response.status,\n          responseBody,\n          url,\n          requestBodyValues,\n          isRetryable: true,\n        });\n      }\n\n      return parsedResult.value;\n    }) satisfies ResponseHandler<OllamaCompletionResponse>,\n  } satisfies OllamaCompletionResponseFormatType<OllamaCompletionResponse>,\n\n  /**\n   * Returns an async iterable over the full deltas (all choices, including full current state at time of event)\n   * of the response stream.\n   */\n  deltaIterable: {\n    stream: true,\n    handler: createJsonStreamResponseHandler(\n      zodSchema(ollamaCompletionStreamChunkSchema)\n    ),\n  },\n};\n","import { TextGenerationPromptTemplate } from \"../../model-function/generate-text/TextGenerationPromptTemplate\";\nimport * as alpacaPrompt from \"../../model-function/generate-text/prompt-template/AlpacaPromptTemplate\";\nimport * as chatMlPrompt from \"../../model-function/generate-text/prompt-template/ChatMLPromptTemplate\";\nimport * as llama2Prompt from \"../../model-function/generate-text/prompt-template/Llama2PromptTemplate\";\nimport * as mistralPrompt from \"../../model-function/generate-text/prompt-template/MistralInstructPromptTemplate\";\nimport * as neuralChatPrompt from \"../../model-function/generate-text/prompt-template/NeuralChatPromptTemplate\";\nimport { TextGenerationPromptTemplateProvider } from \"../../model-function/generate-text/prompt-template/PromptTemplateProvider\";\nimport * as synthiaPrompt from \"../../model-function/generate-text/prompt-template/SynthiaPromptTemplate\";\nimport * as textPrompt from \"../../model-function/generate-text/prompt-template/TextPromptTemplate\";\nimport * as vicunaPrompt from \"../../model-function/generate-text/prompt-template/VicunaPromptTemplate\";\nimport { OllamaCompletionPrompt } from \"./OllamaCompletionModel\";\n\nexport function asOllamaCompletionPromptTemplate<SOURCE_PROMPT>(\n  promptTemplate: TextGenerationPromptTemplate<SOURCE_PROMPT, string>\n): TextGenerationPromptTemplate<SOURCE_PROMPT, OllamaCompletionPrompt> {\n  return {\n    format: (prompt) => ({\n      prompt: promptTemplate.format(prompt),\n    }),\n    stopSequences: promptTemplate.stopSequences,\n  };\n}\n\nexport function asOllamaCompletionTextPromptTemplateProvider(\n  promptTemplateProvider: TextGenerationPromptTemplateProvider<string>\n): TextGenerationPromptTemplateProvider<OllamaCompletionPrompt> {\n  return {\n    text: () => asOllamaCompletionPromptTemplate(promptTemplateProvider.text()),\n\n    instruction: () =>\n      asOllamaCompletionPromptTemplate(promptTemplateProvider.instruction()),\n\n    chat: () => asOllamaCompletionPromptTemplate(promptTemplateProvider.chat()),\n  };\n}\n\nexport const Text = asOllamaCompletionTextPromptTemplateProvider(textPrompt);\n\n/**\n * Formats text, instruction or chat prompts as a Mistral instruct prompt.\n *\n * Note that Mistral does not support system prompts. We emulate them.\n *\n * Text prompt:\n * ```\n * <s>[INST] { instruction } [/INST]\n * ```\n *\n * Instruction prompt when system prompt is set:\n * ```\n * <s>[INST] ${ system prompt } [/INST] </s>[INST] ${instruction} [/INST] ${ response prefix }\n * ```\n *\n * Instruction prompt template when there is no system prompt:\n * ```\n * <s>[INST] ${ instruction } [/INST] ${ response prefix }\n * ```\n *\n * Chat prompt when system prompt is set:\n * ```\n * <s>[INST] ${ system prompt } [/INST] </s> [INST] ${ user msg 1 } [/INST] ${ model response 1 } [INST] ${ user msg 2 } [/INST] ${ model response 2 } [INST] ${ user msg 3 } [/INST]\n * ```\n *\n * Chat prompt when there is no system prompt:\n * ```\n * <s>[INST] ${ user msg 1 } [/INST] ${ model response 1 } </s>[INST] ${ user msg 2 } [/INST] ${ model response 2 } [INST] ${ user msg 3 } [/INST]\n * ```\n *\n * @see https://docs.mistral.ai/models/#chat-template\n */\nexport const Mistral =\n  asOllamaCompletionTextPromptTemplateProvider(mistralPrompt);\n\nexport const ChatML =\n  asOllamaCompletionTextPromptTemplateProvider(chatMlPrompt);\nexport const Llama2 =\n  asOllamaCompletionTextPromptTemplateProvider(llama2Prompt);\nexport const NeuralChat =\n  asOllamaCompletionTextPromptTemplateProvider(neuralChatPrompt);\nexport const Alpaca =\n  asOllamaCompletionTextPromptTemplateProvider(alpacaPrompt);\nexport const Synthia =\n  asOllamaCompletionTextPromptTemplateProvider(synthiaPrompt);\nexport const Vicuna =\n  asOllamaCompletionTextPromptTemplateProvider(vicunaPrompt);\n","import { PartialBaseUrlPartsApiConfigurationOptions } from \"../../core/api/BaseUrlApiConfiguration\";\nimport { OllamaApiConfiguration } from \"./OllamaApiConfiguration\";\nimport { OllamaChatModel, OllamaChatModelSettings } from \"./OllamaChatModel\";\nimport {\n  OllamaCompletionModel,\n  OllamaCompletionModelSettings,\n} from \"./OllamaCompletionModel\";\nimport {\n  OllamaTextEmbeddingModel,\n  OllamaTextEmbeddingModelSettings,\n} from \"./OllamaTextEmbeddingModel\";\n\n/**\n * Creates an API configuration for the Ollama API.\n * It calls the API at http://127.0.0.1:11434 by default.\n */\nexport function Api(settings: PartialBaseUrlPartsApiConfigurationOptions) {\n  return new OllamaApiConfiguration(settings);\n}\n\nexport function CompletionTextGenerator<CONTEXT_WINDOW_SIZE extends number>(\n  settings: OllamaCompletionModelSettings<CONTEXT_WINDOW_SIZE>\n) {\n  return new OllamaCompletionModel(settings);\n}\n\nexport function ChatTextGenerator(settings: OllamaChatModelSettings) {\n  return new OllamaChatModel(settings);\n}\n\nexport function TextEmbedder(settings: OllamaTextEmbeddingModelSettings) {\n  return new OllamaTextEmbeddingModel(settings);\n}\n\nexport {\n  OllamaChatMessage as ChatMessage,\n  OllamaChatPrompt as ChatPrompt,\n} from \"./OllamaChatModel\";\n\nexport * as prompt from \"./OllamaCompletionPrompt\";\n","import { z } from \"zod\";\nimport { FunctionCallOptions } from \"../../core/FunctionOptions\";\nimport { ApiConfiguration } from \"../../core/api/ApiConfiguration\";\nimport { callWithRetryAndThrottle } from \"../../core/api/callWithRetryAndThrottle\";\nimport {\n  createJsonResponseHandler,\n  postJsonToApi,\n} from \"../../core/api/postToApi\";\nimport { zodSchema } from \"../../core/schema/ZodSchema\";\nimport { AbstractModel } from \"../../model-function/AbstractModel\";\nimport {\n  EmbeddingModel,\n  EmbeddingModelSettings,\n} from \"../../model-function/embed/EmbeddingModel\";\nimport { OllamaApiConfiguration } from \"./OllamaApiConfiguration\";\nimport { failedOllamaCallResponseHandler } from \"./OllamaError\";\n\nexport interface OllamaTextEmbeddingModelSettings\n  extends EmbeddingModelSettings {\n  api?: ApiConfiguration;\n  model: string;\n  dimensions?: number;\n  isParallelizable?: boolean;\n}\n\nexport class OllamaTextEmbeddingModel\n  extends AbstractModel<OllamaTextEmbeddingModelSettings>\n  implements EmbeddingModel<string, OllamaTextEmbeddingModelSettings>\n{\n  constructor(settings: OllamaTextEmbeddingModelSettings) {\n    super({ settings });\n  }\n\n  readonly provider = \"ollama\" as const;\n  get modelName() {\n    return null;\n  }\n\n  readonly maxValuesPerCall = 1;\n  get isParallelizable() {\n    return this.settings.isParallelizable ?? false;\n  }\n\n  get dimensions() {\n    return this.settings.dimensions;\n  }\n\n  async callAPI(\n    texts: Array<string>,\n    callOptions: FunctionCallOptions\n  ): Promise<OllamaTextEmbeddingResponse> {\n    if (texts.length > this.maxValuesPerCall) {\n      throw new Error(\n        `The Ollama embedding API only supports ${this.maxValuesPerCall} texts per API call.`\n      );\n    }\n\n    const api = this.settings.api ?? new OllamaApiConfiguration();\n    const abortSignal = callOptions.run?.abortSignal;\n\n    return callWithRetryAndThrottle({\n      retry: api.retry,\n      throttle: api.throttle,\n      call: async () =>\n        postJsonToApi({\n          url: api.assembleUrl(`/api/embeddings`),\n          headers: api.headers({\n            functionType: callOptions.functionType,\n            functionId: callOptions.functionId,\n            run: callOptions.run,\n            callId: callOptions.callId,\n          }),\n          body: {\n            model: this.settings.model,\n            prompt: texts[0],\n          },\n          failedResponseHandler: failedOllamaCallResponseHandler,\n          successfulResponseHandler: createJsonResponseHandler(\n            zodSchema(ollamaTextEmbeddingResponseSchema)\n          ),\n          abortSignal,\n        }),\n    });\n  }\n\n  get settingsForEvent(): Partial<OllamaTextEmbeddingModelSettings> {\n    return {\n      dimensions: this.settings.dimensions,\n    };\n  }\n\n  async doEmbedValues(texts: string[], options: FunctionCallOptions) {\n    const rawResponse = await this.callAPI(texts, options);\n\n    return {\n      rawResponse,\n      embeddings: [rawResponse.embedding],\n    };\n  }\n\n  withSettings(additionalSettings: Partial<OllamaTextEmbeddingModelSettings>) {\n    return new OllamaTextEmbeddingModel(\n      Object.assign({}, this.settings, additionalSettings)\n    ) as this;\n  }\n}\n\nconst ollamaTextEmbeddingResponseSchema = z.object({\n  embedding: z.array(z.number()),\n});\n\nexport type OllamaTextEmbeddingResponse = z.infer<\n  typeof ollamaTextEmbeddingResponseSchema\n>;\n","import { z } from \"zod\";\nimport { FunctionCallOptions } from \"../../core/FunctionOptions\";\nimport { ApiConfiguration } from \"../../core/api/ApiConfiguration\";\nimport { callWithRetryAndThrottle } from \"../../core/api/callWithRetryAndThrottle\";\nimport {\n  ResponseHandler,\n  createJsonResponseHandler,\n  postJsonToApi,\n} from \"../../core/api/postToApi\";\nimport { zodSchema } from \"../../core/schema/ZodSchema\";\nimport { parseJSON } from \"../../core/schema/parseJSON\";\nimport { validateTypes } from \"../../core/schema/validateTypes\";\nimport { AbstractModel } from \"../../model-function/AbstractModel\";\nimport { TextGenerationModelSettings } from \"../../model-function/generate-text/TextGenerationModel\";\nimport { TextGenerationFinishReason } from \"../../model-function/generate-text/TextGenerationResult\";\nimport { ToolDefinition } from \"../../tool/ToolDefinition\";\nimport { createEventSourceResponseHandler } from \"../../util/streaming/createEventSourceResponseHandler\";\nimport { OpenAIApiConfiguration } from \"./OpenAIApiConfiguration\";\nimport { OpenAIChatMessage } from \"./OpenAIChatMessage\";\nimport { failedOpenAICallResponseHandler } from \"./OpenAIError\";\n\nexport interface AbstractOpenAIChatSettings\n  extends TextGenerationModelSettings {\n  api?: ApiConfiguration;\n\n  model: string;\n\n  functions?: Array<{\n    name: string;\n    description?: string;\n    parameters: unknown;\n  }>;\n  functionCall?: \"none\" | \"auto\" | { name: string };\n\n  tools?: Array<{\n    type: \"function\";\n    function: {\n      name: string;\n      description?: string;\n      parameters: unknown;\n    };\n  }>;\n  toolChoice?:\n    | \"none\"\n    | \"auto\"\n    | { type: \"function\"; function: { name: string } };\n\n  /**\n   * `temperature`: Controls the randomness and creativity in the model's responses.\n   * A lower temperature (close to 0) results in more predictable, conservative text, while a higher temperature (close to 1) produces more varied and creative output.\n   * Adjust this to balance between consistency and creativity in the model's replies.\n   * Example: temperature: 0.5\n   */\n  temperature?: number;\n\n  /**\n   *  This parameter sets a threshold for token selection based on probability.\n   * The model will only consider the most likely tokens that cumulatively exceed this threshold while generating a response.\n   * It's a way to control the randomness of the output, balancing between diverse responses and sticking to more likely words.\n   * This means a topP of .1 will be far less random than one at .9\n   * Example: topP: 0.2\n   */\n  topP?: number;\n\n  /**\n   * Used to set the initial state for the random number generator in the model.\n   * Providing a specific seed value ensures consistent outputs for the same inputs across different runs - useful for testing and reproducibility.\n   * A `null` value (or not setting it) results in varied, non-repeatable outputs each time.\n   * Example: seed: 89 (or) seed: null\n   */\n  seed?: number | null;\n\n  /**\n   * Discourages the model from repeating the same information or context already mentioned in the conversation or prompt.\n   * Increasing this value encourages the model to introduce new topics or ideas, rather than reiterating what has been said.\n   * This is useful for maintaining a diverse and engaging conversation or for brainstorming sessions where varied ideas are needed.\n   * Example: presencePenalty: 1.0 // Strongly discourages repeating the same content.\n   */\n  presencePenalty?: number;\n\n  /**\n   * This parameter reduces the likelihood of the model repeatedly using the same words or phrases in its responses.\n   * A higher frequency penalty promotes a wider variety of language and expressions in the output.\n   * This is particularly useful in creative writing or content generation tasks where diversity in language is desirable.\n   * Example: frequencyPenalty: 0.5 // Moderately discourages repetitive language.\n   */\n  frequencyPenalty?: number;\n\n  responseFormat?: {\n    type?: \"text\" | \"json_object\";\n  };\n\n  logitBias?: Record<number, number>;\n\n  isUserIdForwardingEnabled?: boolean;\n}\n\nexport type OpenAIChatPrompt = OpenAIChatMessage[];\n\n/**\n * Abstract text generation model that calls an API that is compatible with the OpenAI chat API.\n *\n * @see https://platform.openai.com/docs/api-reference/chat/create\n */\nexport abstract class AbstractOpenAIChatModel<\n  SETTINGS extends AbstractOpenAIChatSettings,\n> extends AbstractModel<SETTINGS> {\n  constructor(settings: SETTINGS) {\n    super({ settings });\n  }\n\n  async callAPI<RESULT>(\n    messages: OpenAIChatPrompt,\n    callOptions: FunctionCallOptions,\n    options: {\n      responseFormat: OpenAIChatResponseFormatType<RESULT>;\n      functions?: AbstractOpenAIChatSettings[\"functions\"];\n      functionCall?: AbstractOpenAIChatSettings[\"functionCall\"];\n      tools?: AbstractOpenAIChatSettings[\"tools\"];\n      toolChoice?: AbstractOpenAIChatSettings[\"toolChoice\"];\n    }\n  ): Promise<RESULT> {\n    const api = this.settings.api ?? new OpenAIApiConfiguration();\n    const responseFormat = options.responseFormat;\n    const abortSignal = callOptions.run?.abortSignal;\n    const user = this.settings.isUserIdForwardingEnabled\n      ? callOptions.run?.userId\n      : undefined;\n    const openAIResponseFormat = this.settings.responseFormat;\n\n    // function & tool calling:\n    const functions = options.functions ?? this.settings.functions;\n    const functionCall = options.functionCall ?? this.settings.functionCall;\n    const tools = options.tools ?? this.settings.tools;\n    const toolChoice = options.toolChoice ?? this.settings.toolChoice;\n\n    let { stopSequences } = this.settings;\n\n    return callWithRetryAndThrottle({\n      retry: this.settings.api?.retry,\n      throttle: this.settings.api?.throttle,\n      call: async () => {\n        // empty arrays are not allowed for stopSequences:\n        if (\n          stopSequences != null &&\n          Array.isArray(stopSequences) &&\n          stopSequences.length === 0\n        ) {\n          stopSequences = undefined;\n        }\n\n        return postJsonToApi({\n          url: api.assembleUrl(\"/chat/completions\"),\n          headers: api.headers({\n            functionType: callOptions.functionType,\n            functionId: callOptions.functionId,\n            run: callOptions.run,\n            callId: callOptions.callId,\n          }),\n          body: {\n            stream: responseFormat.stream,\n            model: this.settings.model,\n            messages,\n            functions,\n            function_call: functionCall,\n            tools,\n            tool_choice: toolChoice,\n            temperature: this.settings.temperature,\n            top_p: this.settings.topP,\n            n: this.settings.numberOfGenerations,\n            stop: stopSequences,\n            max_tokens: this.settings.maxGenerationTokens,\n            presence_penalty: this.settings.presencePenalty,\n            frequency_penalty: this.settings.frequencyPenalty,\n            logit_bias: this.settings.logitBias,\n            seed: this.settings.seed,\n            response_format: openAIResponseFormat,\n            user,\n          },\n          failedResponseHandler: failedOpenAICallResponseHandler,\n          successfulResponseHandler: responseFormat.handler,\n          abortSignal,\n        });\n      },\n    });\n  }\n\n  async doGenerateTexts(\n    prompt: OpenAIChatPrompt,\n    options: FunctionCallOptions\n  ) {\n    return this.processTextGenerationResponse(\n      await this.callAPI(prompt, options, {\n        responseFormat: OpenAIChatResponseFormat.json,\n      })\n    );\n  }\n\n  restoreGeneratedTexts(rawResponse: unknown) {\n    return this.processTextGenerationResponse(\n      validateTypes({\n        value: rawResponse,\n        schema: zodSchema(openAIChatResponseSchema),\n      })\n    );\n  }\n\n  processTextGenerationResponse(rawResponse: OpenAIChatResponse) {\n    return {\n      rawResponse,\n      textGenerationResults: rawResponse.choices.map((choice) => ({\n        text: choice.message.content ?? \"\",\n        finishReason: this.translateFinishReason(choice.finish_reason),\n      })),\n      usage: this.extractUsage(rawResponse),\n    };\n  }\n\n  private translateFinishReason(\n    finishReason: string | null | undefined\n  ): TextGenerationFinishReason {\n    switch (finishReason) {\n      case \"stop\":\n        return \"stop\";\n      case \"length\":\n        return \"length\";\n      case \"content_filter\":\n        return \"content-filter\";\n      case \"function_call\":\n      case \"tool_calls\":\n        return \"tool-calls\";\n      default:\n        return \"unknown\";\n    }\n  }\n\n  doStreamText(prompt: OpenAIChatPrompt, options: FunctionCallOptions) {\n    return this.callAPI(prompt, options, {\n      responseFormat: OpenAIChatResponseFormat.deltaIterable,\n    });\n  }\n\n  extractTextDelta(delta: unknown) {\n    const chunk = delta as OpenAIChatChunk;\n\n    if (\n      chunk.object !== \"chat.completion.chunk\" &&\n      chunk.object !== \"chat.completion\" // for OpenAI-compatible models\n    ) {\n      return undefined;\n    }\n\n    const chatChunk = chunk as OpenAIChatChunk;\n\n    const firstChoice = chatChunk.choices[0];\n\n    if (firstChoice.index > 0) {\n      return undefined;\n    }\n\n    return firstChoice.delta.content ?? undefined;\n  }\n\n  async doGenerateToolCall(\n    tool: ToolDefinition<string, unknown>,\n    prompt: OpenAIChatPrompt,\n    options: FunctionCallOptions\n  ) {\n    const rawResponse = await this.callAPI(prompt, options, {\n      responseFormat: OpenAIChatResponseFormat.json,\n      toolChoice: {\n        type: \"function\",\n        function: { name: tool.name },\n      },\n      tools: [\n        {\n          type: \"function\",\n          function: {\n            name: tool.name,\n            description: tool.description,\n            parameters: tool.parameters.getJsonSchema(),\n          },\n        },\n      ],\n    });\n\n    const toolCalls = rawResponse.choices[0]?.message.tool_calls;\n\n    return {\n      rawResponse,\n      toolCall:\n        toolCalls == null || toolCalls.length === 0\n          ? null\n          : {\n              id: toolCalls[0].id,\n              args: parseJSON({ text: toolCalls[0].function.arguments }),\n            },\n      usage: this.extractUsage(rawResponse),\n    };\n  }\n\n  async doGenerateToolCalls(\n    tools: Array<ToolDefinition<string, unknown>>,\n    prompt: OpenAIChatPrompt,\n    options: FunctionCallOptions\n  ) {\n    const rawResponse = await this.callAPI(prompt, options, {\n      responseFormat: OpenAIChatResponseFormat.json,\n      toolChoice: \"auto\",\n      tools: tools.map((tool) => ({\n        type: \"function\",\n        function: {\n          name: tool.name,\n          description: tool.description,\n          parameters: tool.parameters.getJsonSchema(),\n        },\n      })),\n    });\n\n    const message = rawResponse.choices[0]?.message;\n\n    return {\n      rawResponse,\n      text: message.content ?? null,\n      toolCalls:\n        message.tool_calls?.map((toolCall) => ({\n          id: toolCall.id,\n          name: toolCall.function.name,\n          args: parseJSON({ text: toolCall.function.arguments }),\n        })) ?? null,\n      usage: this.extractUsage(rawResponse),\n    };\n  }\n\n  extractUsage(response: OpenAIChatResponse) {\n    return {\n      promptTokens: response.usage.prompt_tokens,\n      completionTokens: response.usage.completion_tokens,\n      totalTokens: response.usage.total_tokens,\n    };\n  }\n}\n\nconst openAIChatResponseSchema = z.object({\n  id: z.string(),\n  choices: z.array(\n    z.object({\n      message: z.object({\n        role: z.literal(\"assistant\"),\n        content: z.string().nullable(),\n        function_call: z\n          .object({\n            name: z.string(),\n            arguments: z.string(),\n          })\n          .optional(),\n        tool_calls: z\n          .array(\n            z.object({\n              id: z.string(),\n              type: z.literal(\"function\"),\n              function: z.object({\n                name: z.string(),\n                arguments: z.string(),\n              }),\n            })\n          )\n          .optional(),\n      }),\n      index: z.number().optional(), // optional for OpenAI compatible models\n      logprobs: z.nullable(z.any()),\n      finish_reason: z\n        .enum([\n          \"stop\",\n          \"length\",\n          \"tool_calls\",\n          \"content_filter\",\n          \"function_call\",\n        ])\n        .optional()\n        .nullable(),\n    })\n  ),\n  created: z.number(),\n  model: z.string(),\n  system_fingerprint: z.string().optional().nullable(),\n  object: z.literal(\"chat.completion\"),\n  usage: z.object({\n    prompt_tokens: z.number(),\n    completion_tokens: z.number(),\n    total_tokens: z.number(),\n  }),\n});\n\nexport type OpenAIChatResponse = z.infer<typeof openAIChatResponseSchema>;\n\nconst openaiChatChunkSchema = z.object({\n  object: z.string(), // generalized for openai compatible providers, z.literal(\"chat.completion.chunk\")\n  id: z.string(),\n  choices: z.array(\n    z.object({\n      delta: z.object({\n        role: z.enum([\"assistant\", \"user\"]).optional(),\n        content: z.string().nullable().optional(),\n        function_call: z\n          .object({\n            name: z.string().optional(),\n            arguments: z.string().optional(),\n          })\n          .optional(),\n        tool_calls: z\n          .array(\n            z.object({\n              id: z.string(),\n              type: z.literal(\"function\"),\n              function: z.object({\n                name: z.string(),\n                arguments: z.string(),\n              }),\n            })\n          )\n          .optional(),\n      }),\n      finish_reason: z\n        .enum([\n          \"stop\",\n          \"length\",\n          \"tool_calls\",\n          \"content_filter\",\n          \"function_call\",\n        ])\n        .nullable()\n        .optional(),\n      index: z.number(),\n    })\n  ),\n  created: z.number(),\n  model: z.string().optional(), // optional for OpenAI compatible models\n  system_fingerprint: z.string().optional().nullable(),\n});\n\nexport type OpenAIChatChunk = z.infer<typeof openaiChatChunkSchema>;\n\nexport type OpenAIChatResponseFormatType<T> = {\n  stream: boolean;\n  handler: ResponseHandler<T>;\n};\n\nexport const OpenAIChatResponseFormat = {\n  /**\n   * Returns the response as a JSON object.\n   */\n  json: {\n    stream: false,\n    handler: createJsonResponseHandler(zodSchema(openAIChatResponseSchema)),\n  },\n\n  /**\n   * Returns an async iterable over the text deltas (only the tex different of the first choice).\n   */\n  deltaIterable: {\n    stream: true,\n    handler: createEventSourceResponseHandler(zodSchema(openaiChatChunkSchema)),\n  },\n};\n","import {\n  BaseUrlApiConfigurationWithDefaults,\n  PartialBaseUrlPartsApiConfigurationOptions,\n} from \"../../core/api/BaseUrlApiConfiguration\";\nimport { loadApiKey } from \"../../core/api/loadApiKey\";\n\n/**\n * Creates an API configuration for the OpenAI API.\n * It calls the API at https://api.openai.com/v1 and uses the `OPENAI_API_KEY` env variable by default.\n */\nexport class OpenAIApiConfiguration extends BaseUrlApiConfigurationWithDefaults {\n  constructor(\n    settings: PartialBaseUrlPartsApiConfigurationOptions & {\n      apiKey?: string;\n    } = {}\n  ) {\n    super({\n      ...settings,\n      headers: {\n        Authorization: `Bearer ${loadApiKey({\n          apiKey: settings.apiKey,\n          environmentVariableName: \"OPENAI_API_KEY\",\n          description: \"OpenAI\",\n        })}`,\n      },\n      baseUrlDefaults: {\n        protocol: \"https\",\n        host: \"api.openai.com\",\n        port: \"443\",\n        path: \"/v1\",\n      },\n    });\n  }\n}\n","import { z } from \"zod\";\nimport { createJsonErrorResponseHandler } from \"../../core/api/postToApi\";\nimport { zodSchema } from \"../../core/schema/ZodSchema\";\n\nconst openAIErrorDataSchema = z.object({\n  error: z.object({\n    message: z.string(),\n    type: z.string(),\n    param: z.any().nullable(),\n    code: z.string().nullable(),\n  }),\n});\n\nexport type OpenAIErrorData = z.infer<typeof openAIErrorDataSchema>;\n\nexport const failedOpenAICallResponseHandler = createJsonErrorResponseHandler({\n  errorSchema: zodSchema(openAIErrorDataSchema),\n  errorToMessage: (data) => data.error.message,\n  isRetryable: (response, error) =>\n    response.status >= 500 ||\n    (response.status === 429 &&\n      // insufficient_quota is also reported as a 429, but it's not retryable:\n      error?.error.type !== \"insufficient_quota\"),\n});\n","import { z } from \"zod\";\nimport { FunctionCallOptions } from \"../../core/FunctionOptions\";\nimport { ApiConfiguration } from \"../../core/api/ApiConfiguration\";\nimport { callWithRetryAndThrottle } from \"../../core/api/callWithRetryAndThrottle\";\nimport {\n  ResponseHandler,\n  createJsonResponseHandler,\n  postJsonToApi,\n} from \"../../core/api/postToApi\";\nimport { zodSchema } from \"../../core/schema/ZodSchema\";\nimport { validateTypes } from \"../../core/schema/validateTypes\";\nimport { AbstractModel } from \"../../model-function/AbstractModel\";\nimport { TextGenerationModelSettings } from \"../../model-function/generate-text/TextGenerationModel\";\nimport { TextGenerationFinishReason } from \"../../model-function/generate-text/TextGenerationResult\";\nimport { createEventSourceResponseHandler } from \"../../util/streaming/createEventSourceResponseHandler\";\nimport { OpenAIApiConfiguration } from \"./OpenAIApiConfiguration\";\nimport { failedOpenAICallResponseHandler } from \"./OpenAIError\";\n\nexport interface AbstractOpenAICompletionModelSettings\n  extends TextGenerationModelSettings {\n  api?: ApiConfiguration;\n\n  model: string;\n\n  suffix?: string;\n  temperature?: number;\n  topP?: number;\n  logprobs?: number;\n  echo?: boolean;\n  presencePenalty?: number;\n  frequencyPenalty?: number;\n  bestOf?: number;\n  logitBias?: Record<number, number>;\n  seed?: number | null;\n\n  isUserIdForwardingEnabled?: boolean;\n}\n\n/**\n * Abstract completion model that calls an API that is compatible with the OpenAI completions API.\n *\n * @see https://platform.openai.com/docs/api-reference/completions/create\n */\nexport abstract class AbstractOpenAICompletionModel<\n  SETTINGS extends AbstractOpenAICompletionModelSettings,\n> extends AbstractModel<SETTINGS> {\n  constructor(settings: SETTINGS) {\n    super({ settings });\n  }\n\n  async callAPI<RESULT>(\n    prompt: string,\n    callOptions: FunctionCallOptions,\n    options: {\n      responseFormat: OpenAITextResponseFormatType<RESULT>;\n    }\n  ): Promise<RESULT> {\n    const api = this.settings.api ?? new OpenAIApiConfiguration();\n    const user = this.settings.isUserIdForwardingEnabled\n      ? callOptions.run?.userId\n      : undefined;\n    const abortSignal = callOptions.run?.abortSignal;\n    const openaiResponseFormat = options.responseFormat;\n\n    // empty arrays are not allowed for stop:\n    const stopSequences =\n      this.settings.stopSequences != null &&\n      Array.isArray(this.settings.stopSequences) &&\n      this.settings.stopSequences.length === 0\n        ? undefined\n        : this.settings.stopSequences;\n\n    return callWithRetryAndThrottle({\n      retry: api.retry,\n      throttle: api.throttle,\n      call: async () =>\n        postJsonToApi({\n          url: api.assembleUrl(\"/completions\"),\n          headers: api.headers({\n            functionType: callOptions.functionType,\n            functionId: callOptions.functionId,\n            run: callOptions.run,\n            callId: callOptions.callId,\n          }),\n          body: {\n            stream: openaiResponseFormat.stream,\n            model: this.settings.model,\n            prompt,\n            suffix: this.settings.suffix,\n            max_tokens: this.settings.maxGenerationTokens,\n            temperature: this.settings.temperature,\n            top_p: this.settings.topP,\n            n: this.settings.numberOfGenerations,\n            logprobs: this.settings.logprobs,\n            echo: this.settings.echo,\n            stop: stopSequences,\n            seed: this.settings.seed,\n            presence_penalty: this.settings.presencePenalty,\n            frequency_penalty: this.settings.frequencyPenalty,\n            best_of: this.settings.bestOf,\n            logit_bias: this.settings.logitBias,\n            user,\n          },\n          failedResponseHandler: failedOpenAICallResponseHandler,\n          successfulResponseHandler: openaiResponseFormat.handler,\n          abortSignal,\n        }),\n    });\n  }\n\n  async doGenerateTexts(prompt: string, options: FunctionCallOptions) {\n    return this.processTextGenerationResponse(\n      await this.callAPI(prompt, options, {\n        responseFormat: OpenAITextResponseFormat.json,\n      })\n    );\n  }\n\n  restoreGeneratedTexts(rawResponse: unknown) {\n    return this.processTextGenerationResponse(\n      validateTypes({\n        value: rawResponse,\n        schema: zodSchema(OpenAICompletionResponseSchema),\n      })\n    );\n  }\n\n  private processTextGenerationResponse(rawResponse: OpenAICompletionResponse) {\n    return {\n      rawResponse,\n      textGenerationResults: rawResponse.choices.map((choice) => {\n        return {\n          finishReason: this.translateFinishReason(choice.finish_reason),\n          text: choice.text,\n        };\n      }),\n      usage: {\n        promptTokens: rawResponse.usage.prompt_tokens,\n        completionTokens: rawResponse.usage.completion_tokens,\n        totalTokens: rawResponse.usage.total_tokens,\n      },\n    };\n  }\n\n  private translateFinishReason(\n    finishReason: string | null | undefined\n  ): TextGenerationFinishReason {\n    switch (finishReason) {\n      case \"stop\":\n        return \"stop\";\n      case \"length\":\n        return \"length\";\n      case \"content_filter\":\n        return \"content-filter\";\n      default:\n        return \"unknown\";\n    }\n  }\n\n  doStreamText(prompt: string, options: FunctionCallOptions) {\n    return this.callAPI(prompt, options, {\n      responseFormat: OpenAITextResponseFormat.deltaIterable,\n    });\n  }\n\n  extractTextDelta(delta: unknown) {\n    const chunk = delta as OpenAICompletionStreamChunk;\n\n    const firstChoice = chunk.choices[0];\n\n    if (firstChoice.index > 0) {\n      return undefined;\n    }\n\n    return chunk.choices[0].text;\n  }\n\n  withJsonOutput(): this {\n    return this;\n  }\n}\n\nconst OpenAICompletionResponseSchema = z.object({\n  id: z.string(),\n  choices: z.array(\n    z.object({\n      finish_reason: z\n        .enum([\"stop\", \"length\", \"content_filter\"])\n        .optional()\n        .nullable(),\n      index: z.number(),\n      logprobs: z.nullable(z.any()),\n      text: z.string(),\n    })\n  ),\n  created: z.number(),\n  model: z.string(),\n  system_fingerprint: z.string().optional(),\n  object: z.literal(\"text_completion\"),\n  usage: z.object({\n    prompt_tokens: z.number(),\n    completion_tokens: z.number(),\n    total_tokens: z.number(),\n  }),\n});\n\nexport type OpenAICompletionResponse = z.infer<\n  typeof OpenAICompletionResponseSchema\n>;\n\nconst openaiCompletionStreamChunkSchema = z.object({\n  choices: z.array(\n    z.object({\n      text: z.string(),\n      finish_reason: z\n        .enum([\"stop\", \"length\", \"content_filter\"])\n        .optional()\n        .nullable(),\n      index: z.number(),\n    })\n  ),\n  created: z.number(),\n  id: z.string(),\n  model: z.string(),\n  system_fingerprint: z.string().optional(),\n  object: z.literal(\"text_completion\"),\n});\n\ntype OpenAICompletionStreamChunk = z.infer<\n  typeof openaiCompletionStreamChunkSchema\n>;\n\nexport type OpenAITextResponseFormatType<T> = {\n  stream: boolean;\n  handler: ResponseHandler<T>;\n};\n\nexport const OpenAITextResponseFormat = {\n  /**\n   * Returns the response as a JSON object.\n   */\n  json: {\n    stream: false,\n    handler: createJsonResponseHandler(\n      zodSchema(OpenAICompletionResponseSchema)\n    ),\n  },\n\n  /**\n   * Returns an async iterable over the full deltas (all choices, including full current state at time of event)\n   * of the response stream.\n   */\n  deltaIterable: {\n    stream: true,\n    handler: createEventSourceResponseHandler(\n      zodSchema(openaiCompletionStreamChunkSchema)\n    ),\n  },\n};\n","import { z } from \"zod\";\nimport { FunctionCallOptions } from \"../../core/FunctionOptions\";\nimport { ApiConfiguration } from \"../../core/api/ApiConfiguration\";\nimport { callWithRetryAndThrottle } from \"../../core/api/callWithRetryAndThrottle\";\nimport {\n  createJsonResponseHandler,\n  postJsonToApi,\n} from \"../../core/api/postToApi\";\nimport { zodSchema } from \"../../core/schema/ZodSchema\";\nimport { AbstractModel } from \"../../model-function/AbstractModel\";\nimport { EmbeddingModelSettings } from \"../../model-function/embed/EmbeddingModel\";\nimport { OpenAIApiConfiguration } from \"./OpenAIApiConfiguration\";\nimport { failedOpenAICallResponseHandler } from \"./OpenAIError\";\n\nexport interface AbstractOpenAITextEmbeddingModelSettings\n  extends EmbeddingModelSettings {\n  api?: ApiConfiguration;\n\n  model: string;\n\n  dimensions?: number;\n  maxValuesPerCall?: number | undefined;\n  isUserIdForwardingEnabled?: boolean;\n}\n\n/**\n * Abstract text embedding model that calls an API that is compatible with the OpenAI embedding API.\n *\n * @see https://platform.openai.com/docs/api-reference/embeddings\n */\nexport abstract class AbstractOpenAITextEmbeddingModel<\n  SETTINGS extends AbstractOpenAITextEmbeddingModelSettings,\n> extends AbstractModel<SETTINGS> {\n  constructor(settings: SETTINGS) {\n    super({ settings });\n  }\n\n  get maxValuesPerCall() {\n    return this.settings.maxValuesPerCall ?? 2048;\n  }\n\n  readonly isParallelizable = true;\n\n  async callAPI(\n    texts: Array<string>,\n    callOptions: FunctionCallOptions\n  ): Promise<OpenAITextEmbeddingResponse> {\n    const api = this.settings.api ?? new OpenAIApiConfiguration();\n    const abortSignal = callOptions.run?.abortSignal;\n\n    return callWithRetryAndThrottle({\n      retry: api.retry,\n      throttle: api.throttle,\n      call: async () =>\n        postJsonToApi({\n          url: api.assembleUrl(\"/embeddings\"),\n          headers: api.headers({\n            functionType: callOptions.functionType,\n            functionId: callOptions.functionId,\n            run: callOptions.run,\n            callId: callOptions.callId,\n          }),\n          body: {\n            model: this.modelName,\n            input: texts,\n            dimensions: this.settings.dimensions,\n            user: this.settings.isUserIdForwardingEnabled\n              ? callOptions.run?.userId\n              : undefined,\n          },\n          failedResponseHandler: failedOpenAICallResponseHandler,\n          successfulResponseHandler: createJsonResponseHandler(\n            zodSchema(openAITextEmbeddingResponseSchema)\n          ),\n          abortSignal,\n        }),\n    });\n  }\n\n  async doEmbedValues(texts: string[], callOptions: FunctionCallOptions) {\n    if (texts.length > this.maxValuesPerCall) {\n      throw new Error(\n        `The OpenAI embedding API only supports ${this.maxValuesPerCall} texts per API call.`\n      );\n    }\n\n    const rawResponse = await this.callAPI(texts, callOptions);\n\n    return {\n      rawResponse,\n      embeddings: rawResponse.data.map((data) => data.embedding),\n    };\n  }\n}\n\nconst openAITextEmbeddingResponseSchema = z.object({\n  object: z.literal(\"list\"),\n  data: z.array(\n    z.object({\n      object: z.literal(\"embedding\"),\n      embedding: z.array(z.number()),\n      index: z.number(),\n    })\n  ),\n  model: z.string(),\n  usage: z\n    .object({\n      prompt_tokens: z.number(),\n      total_tokens: z.number(),\n    })\n    .optional(), // for openai-compatible models\n});\n\nexport type OpenAITextEmbeddingResponse = z.infer<\n  typeof openAITextEmbeddingResponseSchema\n>;\n","import { AbstractApiConfiguration } from \"../../core/api/AbstractApiConfiguration\";\nimport { RetryFunction } from \"../../core/api/RetryFunction\";\nimport { ThrottleFunction } from \"../../core/api/ThrottleFunction\";\nimport { loadApiKey } from \"../../core/api/loadApiKey\";\n\nexport type AzureOpenAIApiConfigurationOptions = {\n  resourceName: string;\n  deploymentId: string;\n  apiVersion: string;\n  apiKey?: string;\n  retry?: RetryFunction;\n  throttle?: ThrottleFunction;\n};\n\n/**\n * Configuration for the Azure OpenAI API. This class is responsible for constructing URLs specific to the Azure OpenAI deployment.\n * It creates URLs of the form\n * `https://[resourceName].openai.azure.com/openai/deployments/[deploymentId]/[path]?api-version=[apiVersion]`\n *\n * @see https://learn.microsoft.com/en-us/azure/ai-services/openai/reference\n */\nexport class AzureOpenAIApiConfiguration extends AbstractApiConfiguration {\n  readonly resourceName: string;\n  readonly deploymentId: string;\n  readonly apiVersion: string;\n\n  readonly fixedHeaderValue: Record<string, string>;\n\n  constructor({\n    resourceName,\n    deploymentId,\n    apiVersion,\n    apiKey,\n    retry,\n    throttle,\n  }: AzureOpenAIApiConfigurationOptions) {\n    super({ retry, throttle });\n\n    this.resourceName = resourceName;\n    this.deploymentId = deploymentId;\n    this.apiVersion = apiVersion;\n\n    this.fixedHeaderValue = {\n      \"api-key\": loadApiKey({\n        apiKey,\n        environmentVariableName: \"AZURE_OPENAI_API_KEY\",\n        description: \"Azure OpenAI\",\n      }),\n    };\n  }\n\n  assembleUrl(path: string): string {\n    return `https://${this.resourceName}.openai.azure.com/openai/deployments/${this.deploymentId}${path}?api-version=${this.apiVersion}`;\n  }\n\n  fixedHeaders() {\n    return this.fixedHeaderValue;\n  }\n}\n","import {\n  ImagePart,\n  TextPart,\n} from \"../../model-function/generate-text/prompt-template/ContentPart\";\nimport { ToolCall } from \"../../tool/ToolCall\";\nimport { convertDataContentToBase64String } from \"../../util/format/DataContent\";\n\nexport type OpenAIChatMessage =\n  | {\n      role: \"system\";\n      content: string;\n      name?: string;\n    }\n  | {\n      role: \"user\";\n      content:\n        | string\n        | Array<\n            | { type: \"text\"; text: string }\n            | {\n                type: \"image_url\";\n                image_url:\n                  | string\n                  | {\n                      url: string;\n                      detail: \"low\" | \"high\" | \"auto\";\n                    };\n              }\n          >;\n      name?: string;\n    }\n  | {\n      role: \"assistant\";\n      content: string | null;\n      name?: string;\n      tool_calls?: Array<{\n        id: string;\n        type: \"function\";\n        function: {\n          name: string;\n          arguments: string;\n        };\n      }>;\n      function_call?: {\n        name: string;\n        arguments: string;\n      };\n    }\n  | {\n      role: \"tool\";\n      tool_call_id: string;\n      content: string | null;\n    }\n  | {\n      role: \"function\";\n      content: string;\n      name: string;\n    };\n\nexport const OpenAIChatMessage = {\n  /**\n   * Creates a system chat message.\n   */\n  system(content: string): OpenAIChatMessage {\n    return { role: \"system\", content };\n  },\n\n  /**\n   * Creates a user chat message. The message can be a string or a multi-modal input.\n   */\n  user(\n    content: string | Array<TextPart | ImagePart>,\n    options?: { name?: string }\n  ): OpenAIChatMessage {\n    return {\n      role: \"user\",\n      content:\n        typeof content === \"string\"\n          ? content\n          : content.map((part) => {\n              switch (part.type) {\n                case \"text\": {\n                  return { type: \"text\", text: part.text };\n                }\n                case \"image\": {\n                  return {\n                    type: \"image_url\",\n                    image_url: `data:${\n                      part.mimeType ?? \"image/jpeg\"\n                    };base64,${convertDataContentToBase64String(part.image)}`,\n                  };\n                }\n              }\n            }),\n      name: options?.name,\n    };\n  },\n\n  /**\n   * Creates an assistant chat message.\n   * The assistant message can optionally contain tool calls\n   * or a function call (function calls are deprecated).\n   */\n  assistant(\n    content: string | null,\n    options?: {\n      functionCall?: { name: string; arguments: string };\n      toolCalls?: Array<ToolCall<string, unknown>> | null | undefined;\n    }\n  ): OpenAIChatMessage {\n    return {\n      role: \"assistant\",\n      content,\n      function_call:\n        options?.functionCall == null\n          ? undefined\n          : {\n              name: options.functionCall.name,\n              arguments: options.functionCall.arguments,\n            },\n      tool_calls:\n        options?.toolCalls?.map((toolCall) => ({\n          id: toolCall.id,\n          type: \"function\",\n          function: {\n            name: toolCall.name,\n            arguments: JSON.stringify(toolCall.args),\n          },\n        })) ?? undefined,\n    };\n  },\n\n  /**\n   * Creates a function result chat message for tool call results.\n   *\n   * @deprecated OpenAI functions are deprecated in favor of tools.\n   */\n  fn({\n    fnName,\n    content,\n  }: {\n    fnName: string;\n    content: unknown;\n  }): OpenAIChatMessage {\n    return { role: \"function\", name: fnName, content: JSON.stringify(content) };\n  },\n\n  /**\n   * Creates a tool result chat message with the result of a tool call.\n   */\n  tool({\n    toolCallId,\n    content,\n  }: {\n    toolCallId: string;\n    content: unknown;\n  }): OpenAIChatMessage {\n    return {\n      role: \"tool\" as const,\n      tool_call_id: toolCallId,\n      content: JSON.stringify(content),\n    };\n  },\n};\n","import SecureJSON from \"secure-json-parse\";\n\nimport { FunctionCallOptions } from \"../../core/FunctionOptions\";\nimport { JsonSchemaProducer } from \"../../core/schema/JsonSchemaProducer\";\nimport { Schema } from \"../../core/schema/Schema\";\nimport { ObjectStreamingModel } from \"../../model-function/generate-object/ObjectGenerationModel\";\nimport { ObjectParseError } from \"../../model-function/generate-object/ObjectParseError\";\nimport { TextGenerationPromptTemplate } from \"../../model-function/generate-text/TextGenerationPromptTemplate\";\nimport { parsePartialJson } from \"../../util/parsePartialJson\";\nimport {\n  OpenAIChatChunk,\n  OpenAIChatPrompt,\n  OpenAIChatResponseFormat,\n} from \"./AbstractOpenAIChatModel\";\nimport { OpenAIChatModel, OpenAIChatSettings } from \"./OpenAIChatModel\";\nimport { chat, instruction, text } from \"./OpenAIChatPromptTemplate\";\n\nexport class OpenAIChatFunctionCallObjectGenerationModel<\n  PROMPT_TEMPLATE extends TextGenerationPromptTemplate<\n    unknown,\n    OpenAIChatPrompt\n  >,\n> implements\n    ObjectStreamingModel<\n      Parameters<PROMPT_TEMPLATE[\"format\"]>[0], // first argument of the function\n      OpenAIChatSettings\n    >\n{\n  readonly model: OpenAIChatModel;\n  readonly fnName: string;\n  readonly fnDescription?: string;\n  readonly promptTemplate: PROMPT_TEMPLATE;\n\n  constructor({\n    model,\n    fnName,\n    fnDescription,\n    promptTemplate,\n  }: {\n    model: OpenAIChatModel;\n    fnName: string;\n    fnDescription?: string;\n    promptTemplate: PROMPT_TEMPLATE;\n  }) {\n    this.model = model;\n    this.fnName = fnName;\n    this.fnDescription = fnDescription;\n    this.promptTemplate = promptTemplate;\n  }\n\n  get modelInformation() {\n    return this.model.modelInformation;\n  }\n\n  get settings() {\n    return {\n      ...this.model.settings,\n      fnName: this.fnName,\n      fnDescription: this.fnDescription,\n    };\n  }\n\n  get settingsForEvent() {\n    return {\n      ...this.model.settingsForEvent,\n      fnName: this.fnName,\n      fnDescription: this.fnDescription,\n    };\n  }\n\n  /**\n   * Returns this model with a text prompt template.\n   */\n  withTextPrompt() {\n    return this.withPromptTemplate(text());\n  }\n\n  /**\n   * Returns this model with an instruction prompt template.\n   */\n  withInstructionPrompt() {\n    return this.withPromptTemplate(instruction());\n  }\n\n  /**\n   * Returns this model with a chat prompt template.\n   */\n  withChatPrompt() {\n    return this.withPromptTemplate(chat());\n  }\n\n  withPromptTemplate<\n    TARGET_PROMPT_FORMAT extends TextGenerationPromptTemplate<\n      unknown,\n      OpenAIChatPrompt\n    >,\n  >(\n    promptTemplate: TARGET_PROMPT_FORMAT\n  ): OpenAIChatFunctionCallObjectGenerationModel<TARGET_PROMPT_FORMAT> {\n    return new OpenAIChatFunctionCallObjectGenerationModel({\n      model: this.model,\n      fnName: this.fnName,\n      fnDescription: this.fnDescription,\n      promptTemplate,\n    });\n  }\n\n  withSettings(additionalSettings: Partial<OpenAIChatSettings>) {\n    return new OpenAIChatFunctionCallObjectGenerationModel({\n      model: this.model.withSettings(additionalSettings),\n      fnName: this.fnName,\n      fnDescription: this.fnDescription,\n      promptTemplate: this.promptTemplate,\n    }) as this;\n  }\n\n  /**\n   * JSON generation uses the OpenAI GPT function calling API.\n   * It provides a single function specification and instructs the model to provide parameters for calling the function.\n   * The result is returned as parsed JSON.\n   *\n   * @see https://platform.openai.com/docs/guides/gpt/function-calling\n   */\n  async doGenerateObject(\n    schema: Schema<unknown> & JsonSchemaProducer,\n    prompt: Parameters<PROMPT_TEMPLATE[\"format\"]>[0], // first argument of the function\n    options: FunctionCallOptions\n  ) {\n    const expandedPrompt = this.promptTemplate.format(prompt);\n\n    const rawResponse = await this.model\n      .withSettings({\n        stopSequences: [\n          ...(this.settings.stopSequences ?? []),\n          ...this.promptTemplate.stopSequences,\n        ],\n      })\n      .callAPI(expandedPrompt, options, {\n        responseFormat: OpenAIChatResponseFormat.json,\n        functionCall: { name: this.fnName },\n        functions: [\n          {\n            name: this.fnName,\n            description: this.fnDescription,\n            parameters: schema.getJsonSchema(),\n          },\n        ],\n      });\n\n    const valueText = rawResponse.choices[0]!.message.function_call!.arguments;\n\n    try {\n      return {\n        rawResponse,\n        valueText,\n        value: SecureJSON.parse(valueText),\n        usage: this.model.extractUsage(rawResponse),\n      };\n    } catch (error) {\n      throw new ObjectParseError({\n        valueText,\n        cause: error,\n      });\n    }\n  }\n\n  async doStreamObject(\n    schema: Schema<unknown> & JsonSchemaProducer,\n    prompt: Parameters<PROMPT_TEMPLATE[\"format\"]>[0], // first argument of the function\n    options: FunctionCallOptions\n  ) {\n    const expandedPrompt = this.promptTemplate.format(prompt);\n\n    return this.model.callAPI(expandedPrompt, options, {\n      responseFormat: OpenAIChatResponseFormat.deltaIterable,\n      functionCall: { name: this.fnName },\n      functions: [\n        {\n          name: this.fnName,\n          description: this.fnDescription,\n          parameters: schema.getJsonSchema(),\n        },\n      ],\n    });\n  }\n\n  extractObjectTextDelta(delta: unknown) {\n    const chunk = delta as OpenAIChatChunk;\n\n    if (chunk.object !== \"chat.completion.chunk\") {\n      return undefined;\n    }\n\n    const chatChunk = chunk as OpenAIChatChunk;\n\n    const firstChoice = chatChunk.choices[0];\n\n    if (firstChoice.index > 0) {\n      return undefined;\n    }\n\n    return firstChoice.delta.function_call?.arguments;\n  }\n\n  parseAccumulatedObjectText(accumulatedText: string) {\n    return parsePartialJson(accumulatedText);\n  }\n}\n","import { TextGenerationPromptTemplate } from \"../../model-function/generate-text/TextGenerationPromptTemplate\";\nimport { ChatPrompt } from \"../../model-function/generate-text/prompt-template/ChatPrompt\";\nimport { InstructionPrompt } from \"../../model-function/generate-text/prompt-template/InstructionPrompt\";\nimport { OpenAIChatPrompt } from \"./AbstractOpenAIChatModel\";\nimport { OpenAIChatMessage } from \"./OpenAIChatMessage\";\n\n/**\n * OpenAIMessage[] identity chat format.\n */\nexport function identity(): TextGenerationPromptTemplate<\n  OpenAIChatPrompt,\n  OpenAIChatPrompt\n> {\n  return { format: (prompt) => prompt, stopSequences: [] };\n}\n\n/**\n * Formats a text prompt as an OpenAI chat prompt.\n */\nexport function text(): TextGenerationPromptTemplate<string, OpenAIChatPrompt> {\n  return {\n    format: (prompt) => [OpenAIChatMessage.user(prompt)],\n    stopSequences: [],\n  };\n}\n\n/**\n * Formats an instruction prompt as an OpenAI chat prompt.\n */\nexport function instruction(): TextGenerationPromptTemplate<\n  InstructionPrompt,\n  OpenAIChatPrompt\n> {\n  return {\n    format(prompt) {\n      const messages: OpenAIChatPrompt = [];\n\n      if (prompt.system != null) {\n        messages.push(OpenAIChatMessage.system(prompt.system));\n      }\n\n      messages.push(OpenAIChatMessage.user(prompt.instruction));\n\n      return messages;\n    },\n    stopSequences: [],\n  };\n}\n\n/**\n * Formats a chat prompt as an OpenAI chat prompt.\n */\nexport function chat(): TextGenerationPromptTemplate<\n  ChatPrompt,\n  OpenAIChatPrompt\n> {\n  return {\n    format(prompt) {\n      const messages: Array<OpenAIChatMessage> = [];\n\n      if (prompt.system != null) {\n        messages.push(OpenAIChatMessage.system(prompt.system));\n      }\n\n      for (const { role, content } of prompt.messages) {\n        switch (role) {\n          case \"user\": {\n            messages.push(OpenAIChatMessage.user(content));\n            break;\n          }\n          case \"assistant\": {\n            if (typeof content === \"string\") {\n              messages.push(OpenAIChatMessage.assistant(content));\n            } else {\n              let text = \"\";\n              const toolCalls: Array<{\n                id: string;\n                type: \"function\";\n                function: { name: string; arguments: string };\n              }> = [];\n\n              for (const part of content) {\n                switch (part.type) {\n                  case \"text\": {\n                    text += part.text;\n                    break;\n                  }\n                  case \"tool-call\": {\n                    toolCalls.push({\n                      id: part.id,\n                      type: \"function\",\n                      function: {\n                        name: part.name,\n                        arguments: JSON.stringify(part.args),\n                      },\n                    });\n                    break;\n                  }\n                  default: {\n                    const _exhaustiveCheck: never = part;\n                    throw new Error(`Unsupported part: ${_exhaustiveCheck}`);\n                  }\n                }\n              }\n\n              messages.push({\n                role: \"assistant\",\n                content: text,\n                tool_calls: toolCalls,\n              });\n            }\n\n            break;\n          }\n          case \"tool\": {\n            for (const toolResponse of content) {\n              messages.push({\n                role: \"tool\",\n                tool_call_id: toolResponse.id,\n                content: JSON.stringify(toolResponse.response),\n              });\n            }\n            break;\n          }\n          default: {\n            const _exhaustiveCheck: never = role;\n            throw new Error(`Unsupported role: ${_exhaustiveCheck}`);\n          }\n        }\n      }\n\n      return messages;\n    },\n    stopSequences: [],\n  };\n}\n","import { Tiktoken } from \"js-tiktoken/lite\";\nimport cl100k_base from \"js-tiktoken/ranks/cl100k_base\";\nimport { FullTokenizer } from \"../../model-function/tokenize-text/Tokenizer\";\nimport { never } from \"../../util/never\";\nimport { OpenAIChatBaseModelType } from \"./OpenAIChatModel\";\nimport { OpenAICompletionModelType } from \"./OpenAICompletionModel\";\nimport { OpenAITextEmbeddingModelType } from \"./OpenAITextEmbeddingModel\";\n\nexport type TikTokenTokenizerSettings = {\n  model:\n    | OpenAIChatBaseModelType\n    | OpenAICompletionModelType\n    | OpenAITextEmbeddingModelType;\n};\n\n/**\n * TikToken tokenizer for OpenAI language models.\n *\n * @see https://github.com/openai/tiktoken\n *\n * @example\n * const tokenizer = new TikTokenTokenizer({ model: \"gpt-4\" });\n *\n * const text = \"At first, Nox didn't know what to do with the pup.\";\n *\n * const tokenCount = await countTokens(tokenizer, text);\n * const tokens = await tokenizer.tokenize(text);\n * const tokensAndTokenTexts = await tokenizer.tokenizeWithTexts(text);\n * const reconstructedText = await tokenizer.detokenize(tokens);\n */\nexport class TikTokenTokenizer implements FullTokenizer {\n  /**\n   * Get a TikToken tokenizer for a specific model or encoding.\n   */\n  constructor(settings: TikTokenTokenizerSettings) {\n    this.tiktoken = new Tiktoken(getTiktokenBPE(settings.model));\n  }\n\n  private readonly tiktoken: Tiktoken;\n\n  async tokenize(text: string) {\n    return this.tiktoken.encode(text);\n  }\n\n  async tokenizeWithTexts(text: string) {\n    const tokens = this.tiktoken.encode(text);\n\n    return {\n      tokens,\n      tokenTexts: tokens.map((token) => this.tiktoken.decode([token])),\n    };\n  }\n\n  async detokenize(tokens: number[]) {\n    return this.tiktoken.decode(tokens);\n  }\n}\n\n// implemented here (instead of using js-tiktoken) to be able to quickly updated it\n// when new models are released\nfunction getTiktokenBPE(\n  model:\n    | OpenAIChatBaseModelType\n    | OpenAICompletionModelType\n    | OpenAITextEmbeddingModelType\n) {\n  switch (model) {\n    case \"gpt-3.5-turbo\":\n    case \"gpt-3.5-turbo-0301\":\n    case \"gpt-3.5-turbo-0613\":\n    case \"gpt-3.5-turbo-1106\":\n    case \"gpt-3.5-turbo-0125\":\n    case \"gpt-3.5-turbo-16k\":\n    case \"gpt-3.5-turbo-16k-0613\":\n    case \"gpt-3.5-turbo-instruct\":\n    case \"gpt-4\":\n    case \"gpt-4o\":\n    case \"gpt-4-0314\":\n    case \"gpt-4-0613\":\n    case \"gpt-4-turbo-preview\":\n    case \"gpt-4-1106-preview\":\n    case \"gpt-4-0125-preview\":\n    case \"gpt-4-vision-preview\":\n    case \"gpt-4-32k\":\n    case \"gpt-4-32k-0314\":\n    case \"gpt-4-32k-0613\":\n    case \"text-embedding-3-small\":\n    case \"text-embedding-3-large\":\n    case \"text-embedding-ada-002\": {\n      return cl100k_base;\n    }\n    default: {\n      never(model);\n      throw new Error(`Unknown model: ${model}`);\n    }\n  }\n}\n","// eslint-disable-next-line @typescript-eslint/no-unused-vars\nexport function never(_: never) {}\n","import { countTokens } from \"../../model-function/tokenize-text/countTokens\";\nimport { TikTokenTokenizer } from \"./TikTokenTokenizer\";\nimport { OpenAIChatMessage } from \"./OpenAIChatMessage\";\nimport {\n  OpenAIChatModelType,\n  getOpenAIChatModelInformation,\n} from \"./OpenAIChatModel\";\n\n/**\n * Prompt tokens that are included automatically for every full\n * chat prompt (several messages) that is sent to OpenAI.\n */\nexport const OPENAI_CHAT_PROMPT_BASE_TOKEN_COUNT = 2;\n\n/**\n * Prompt tokens that are included automatically for every\n * message that is sent to OpenAI.\n */\nexport const OPENAI_CHAT_MESSAGE_BASE_TOKEN_COUNT = 5;\n\nexport async function countOpenAIChatMessageTokens({\n  message,\n  model,\n}: {\n  message: OpenAIChatMessage;\n  model: OpenAIChatModelType;\n}) {\n  const tokenizer = new TikTokenTokenizer({\n    model: getOpenAIChatModelInformation(model).baseModel,\n  });\n\n  // case: function call without content\n  if (message.content == null) {\n    return OPENAI_CHAT_MESSAGE_BASE_TOKEN_COUNT;\n  }\n\n  // case: simple text content\n  if (typeof message.content === \"string\") {\n    return (\n      OPENAI_CHAT_MESSAGE_BASE_TOKEN_COUNT +\n      (await countTokens(tokenizer, message.content))\n    );\n  }\n\n  // case: array of content objects\n  let contentTokenCount = OPENAI_CHAT_MESSAGE_BASE_TOKEN_COUNT;\n  for (const content of message.content) {\n    if (content.type === \"text\") {\n      contentTokenCount += await countTokens(tokenizer, content.text);\n    }\n  }\n\n  return contentTokenCount;\n}\n\nexport async function countOpenAIChatPromptTokens({\n  messages,\n  model,\n}: {\n  messages: OpenAIChatMessage[];\n  model: OpenAIChatModelType;\n}) {\n  let tokens = OPENAI_CHAT_PROMPT_BASE_TOKEN_COUNT;\n  for (const message of messages) {\n    tokens += await countOpenAIChatMessageTokens({ message, model });\n  }\n  return tokens;\n}\n","import {\n  FlexibleObjectFromTextPromptTemplate,\n  ObjectFromTextPromptTemplate,\n} from \"../../model-function/generate-object/ObjectFromTextPromptTemplate\";\nimport { ObjectFromTextStreamingModel } from \"../../model-function/generate-object/ObjectFromTextStreamingModel\";\nimport { PromptTemplateFullTextModel } from \"../../model-function/generate-text/PromptTemplateFullTextModel\";\nimport {\n  TextStreamingBaseModel,\n  TextStreamingModel,\n  textGenerationModelProperties,\n} from \"../../model-function/generate-text/TextGenerationModel\";\nimport { TextGenerationPromptTemplate } from \"../../model-function/generate-text/TextGenerationPromptTemplate\";\nimport { ToolCallGenerationModel } from \"../../tool/generate-tool-call/ToolCallGenerationModel\";\nimport { ToolCallsGenerationModel } from \"../../tool/generate-tool-calls/ToolCallsGenerationModel\";\nimport {\n  AbstractOpenAIChatModel,\n  AbstractOpenAIChatSettings,\n  OpenAIChatPrompt,\n} from \"./AbstractOpenAIChatModel\";\nimport { OpenAIChatFunctionCallObjectGenerationModel } from \"./OpenAIChatFunctionCallObjectGenerationModel\";\nimport { chat, identity, instruction, text } from \"./OpenAIChatPromptTemplate\";\nimport { TikTokenTokenizer } from \"./TikTokenTokenizer\";\nimport { countOpenAIChatPromptTokens } from \"./countOpenAIChatMessageTokens\";\n\n// https://platform.openai.com/docs/models\n// Open AI base chat models and their context window sizes.\nexport const CHAT_MODEL_CONTEXT_WINDOW_SIZES = {\n  \"gpt-4\": 8192,\n  \"gpt-4o\": 128000,\n  \"gpt-4-0314\": 8192,\n  \"gpt-4-0613\": 8192,\n  \"gpt-4-turbo-preview\": 128000,\n  \"gpt-4-1106-preview\": 128000,\n  \"gpt-4-0125-preview\": 128000,\n  \"gpt-4-vision-preview\": 128000,\n  \"gpt-4-32k\": 32768,\n  \"gpt-4-32k-0314\": 32768,\n  \"gpt-4-32k-0613\": 32768,\n  \"gpt-3.5-turbo\": 4096,\n  \"gpt-3.5-turbo-0125\": 16385,\n  \"gpt-3.5-turbo-1106\": 16385,\n  \"gpt-3.5-turbo-0301\": 4096,\n  \"gpt-3.5-turbo-0613\": 4096,\n  \"gpt-3.5-turbo-16k\": 16384,\n  \"gpt-3.5-turbo-16k-0613\": 16384,\n} as const;\n\nexport function getOpenAIChatModelInformation(model: OpenAIChatModelType): {\n  baseModel: OpenAIChatBaseModelType;\n  isFineTuned: boolean;\n  contextWindowSize: number;\n} {\n  // Model is already a base model:\n  if (model in CHAT_MODEL_CONTEXT_WINDOW_SIZES) {\n    const contextWindowSize =\n      CHAT_MODEL_CONTEXT_WINDOW_SIZES[model as OpenAIChatBaseModelType];\n\n    return {\n      baseModel: model as OpenAIChatBaseModelType,\n      isFineTuned: false,\n      contextWindowSize,\n    };\n  }\n\n  // Extract the base model from the fine-tuned model:\n  // eslint-disable-next-line @typescript-eslint/no-unused-vars\n  const [_, baseModel, ___, ____, _____] = model.split(\":\");\n\n  if (\n    [\"gpt-3.5-turbo\", \"gpt-3.5-turbo-0613\", \"gpt-4-0613\", \"gpt-4o\"].includes(\n      baseModel\n    )\n  ) {\n    const contextWindowSize =\n      CHAT_MODEL_CONTEXT_WINDOW_SIZES[\n        baseModel as FineTuneableOpenAIChatModelType\n      ];\n\n    return {\n      baseModel: baseModel as FineTuneableOpenAIChatModelType,\n      isFineTuned: true,\n      contextWindowSize,\n    };\n  }\n\n  throw new Error(`Unknown OpenAI chat base model ${baseModel}.`);\n}\n\ntype FineTuneableOpenAIChatModelType =\n  | `gpt-3.5-turbo`\n  | `gpt-3.5-turbo-0613`\n  | `gpt-4-0613`\n  | `gpt-4o`;\n\ntype FineTunedOpenAIChatModelType =\n  `ft:${FineTuneableOpenAIChatModelType}:${string}:${string}:${string}`;\n\nexport type OpenAIChatBaseModelType =\n  keyof typeof CHAT_MODEL_CONTEXT_WINDOW_SIZES;\n\nexport type OpenAIChatModelType =\n  | OpenAIChatBaseModelType\n  | FineTunedOpenAIChatModelType;\nexport interface OpenAIChatSettings extends AbstractOpenAIChatSettings {\n  model: OpenAIChatModelType;\n}\n\n/**\n * Create a text generation model that calls the OpenAI chat API.\n *\n * @see https://platform.openai.com/docs/api-reference/chat/create\n *\n * @example\n * const model = new OpenAIChatModel({\n *   model: \"gpt-3.5-turbo\",\n *   temperature: 0.7,\n *   maxGenerationTokens: 500,\n * });\n *\n * const text = await generateText([\n *   model,\n *   openai.ChatMessage.system(\n *     \"Write a short story about a robot learning to love:\"\n *   ),\n * ]);\n */\nexport class OpenAIChatModel\n  extends AbstractOpenAIChatModel<OpenAIChatSettings>\n  implements\n    TextStreamingBaseModel<OpenAIChatPrompt, OpenAIChatSettings>,\n    ToolCallGenerationModel<OpenAIChatPrompt, OpenAIChatSettings>,\n    ToolCallsGenerationModel<OpenAIChatPrompt, OpenAIChatSettings>\n{\n  constructor(settings: OpenAIChatSettings) {\n    super(settings);\n\n    const modelInformation = getOpenAIChatModelInformation(this.settings.model);\n\n    this.tokenizer = new TikTokenTokenizer({\n      model: modelInformation.baseModel,\n    });\n    this.contextWindowSize = modelInformation.contextWindowSize;\n  }\n\n  readonly provider = \"openai\" as const;\n  get modelName() {\n    return this.settings.model;\n  }\n\n  readonly contextWindowSize: number;\n  readonly tokenizer: TikTokenTokenizer;\n\n  /**\n   * Counts the prompt tokens required for the messages. This includes the message base tokens\n   * and the prompt base tokens.\n   */\n  countPromptTokens(messages: OpenAIChatPrompt) {\n    return countOpenAIChatPromptTokens({\n      messages,\n      model: this.modelName,\n    });\n  }\n\n  get settingsForEvent(): Partial<OpenAIChatSettings> {\n    const eventSettingProperties: Array<string> = [\n      ...textGenerationModelProperties,\n\n      \"functions\",\n      \"functionCall\",\n      \"temperature\",\n      \"topP\",\n      \"presencePenalty\",\n      \"frequencyPenalty\",\n      \"logitBias\",\n      \"seed\",\n      \"responseFormat\",\n    ] satisfies (keyof OpenAIChatSettings)[];\n\n    return Object.fromEntries(\n      Object.entries(this.settings).filter(([key]) =>\n        eventSettingProperties.includes(key)\n      )\n    );\n  }\n\n  asFunctionCallObjectGenerationModel({\n    fnName,\n    fnDescription,\n  }: {\n    fnName: string;\n    fnDescription?: string;\n  }) {\n    return new OpenAIChatFunctionCallObjectGenerationModel({\n      model: this,\n      fnName,\n      fnDescription,\n      promptTemplate: identity(),\n    });\n  }\n\n  asObjectGenerationModel<INPUT_PROMPT, OpenAIChatPrompt>(\n    promptTemplate:\n      | ObjectFromTextPromptTemplate<INPUT_PROMPT, OpenAIChatPrompt>\n      | FlexibleObjectFromTextPromptTemplate<INPUT_PROMPT, unknown>\n  ) {\n    return \"adaptModel\" in promptTemplate\n      ? new ObjectFromTextStreamingModel({\n          model: promptTemplate.adaptModel(this),\n          template: promptTemplate,\n        })\n      : new ObjectFromTextStreamingModel({\n          model: this as TextStreamingModel<OpenAIChatPrompt>,\n          template: promptTemplate,\n        });\n  }\n\n  withTextPrompt() {\n    return this.withPromptTemplate(text());\n  }\n\n  withInstructionPrompt() {\n    return this.withPromptTemplate(instruction());\n  }\n\n  withChatPrompt() {\n    return this.withPromptTemplate(chat());\n  }\n\n  withPromptTemplate<INPUT_PROMPT>(\n    promptTemplate: TextGenerationPromptTemplate<INPUT_PROMPT, OpenAIChatPrompt>\n  ): PromptTemplateFullTextModel<\n    INPUT_PROMPT,\n    OpenAIChatPrompt,\n    OpenAIChatSettings,\n    this\n  > {\n    return new PromptTemplateFullTextModel({\n      model: this.withSettings({\n        stopSequences: [\n          ...(this.settings.stopSequences ?? []),\n          ...promptTemplate.stopSequences,\n        ],\n      }),\n      promptTemplate,\n    });\n  }\n\n  withJsonOutput() {\n    return this.withSettings({ responseFormat: { type: \"json_object\" } });\n  }\n\n  withSettings(additionalSettings: Partial<OpenAIChatSettings>) {\n    return new OpenAIChatModel(\n      Object.assign({}, this.settings, additionalSettings)\n    ) as this;\n  }\n}\n","import { PromptTemplateTextStreamingModel } from \"../../model-function/generate-text/PromptTemplateTextStreamingModel\";\nimport {\n  TextStreamingBaseModel,\n  textGenerationModelProperties,\n} from \"../../model-function/generate-text/TextGenerationModel\";\nimport { TextGenerationPromptTemplate } from \"../../model-function/generate-text/TextGenerationPromptTemplate\";\nimport {\n  chat,\n  instruction,\n  text,\n} from \"../../model-function/generate-text/prompt-template/TextPromptTemplate\";\nimport { countTokens } from \"../../model-function/tokenize-text/countTokens\";\nimport {\n  AbstractOpenAICompletionModel,\n  AbstractOpenAICompletionModelSettings,\n} from \"./AbstractOpenAICompletionModel\";\nimport { TikTokenTokenizer } from \"./TikTokenTokenizer\";\n\n/**\n * @see https://platform.openai.com/docs/models/\n * @see https://openai.com/pricing\n */\nexport const OPENAI_TEXT_GENERATION_MODELS = {\n  \"gpt-3.5-turbo-instruct\": {\n    contextWindowSize: 4097,\n  },\n};\n\nexport function getOpenAICompletionModelInformation(\n  model: OpenAICompletionModelType\n): {\n  contextWindowSize: number;\n} {\n  return OPENAI_TEXT_GENERATION_MODELS[model];\n}\n\nexport type OpenAICompletionModelType =\n  keyof typeof OPENAI_TEXT_GENERATION_MODELS;\n\nexport interface OpenAICompletionModelSettings\n  extends AbstractOpenAICompletionModelSettings {\n  model: OpenAICompletionModelType;\n}\n\n/**\n * Create a text generation model that calls the OpenAI text completion API.\n *\n * @see https://platform.openai.com/docs/api-reference/completions/create\n *\n * @example\n * const model = new OpenAICompletionModel({\n *   model: \"gpt-3.5-turbo-instruct\",\n *   temperature: 0.7,\n *   maxGenerationTokens: 500,\n *   retry: retryWithExponentialBackoff({ maxTries: 5 }),\n * });\n *\n * const text = await generateText(\n *   model,\n *   \"Write a short story about a robot learning to love:\\n\\n\"\n * );\n */\nexport class OpenAICompletionModel\n  extends AbstractOpenAICompletionModel<OpenAICompletionModelSettings>\n  implements TextStreamingBaseModel<string, OpenAICompletionModelSettings>\n{\n  constructor(settings: OpenAICompletionModelSettings) {\n    super(settings);\n\n    const modelInformation = getOpenAICompletionModelInformation(\n      this.settings.model\n    );\n\n    this.tokenizer = new TikTokenTokenizer({\n      model: this.settings.model,\n    });\n    this.contextWindowSize = modelInformation.contextWindowSize;\n  }\n\n  readonly provider = \"openai\" as const;\n  get modelName() {\n    return this.settings.model;\n  }\n\n  readonly contextWindowSize: number;\n  readonly tokenizer: TikTokenTokenizer;\n\n  async countPromptTokens(input: string) {\n    return countTokens(this.tokenizer, input);\n  }\n\n  get settingsForEvent(): Partial<OpenAICompletionModelSettings> {\n    const eventSettingProperties: Array<string> = [\n      ...textGenerationModelProperties,\n\n      \"suffix\",\n      \"temperature\",\n      \"topP\",\n      \"logprobs\",\n      \"echo\",\n      \"presencePenalty\",\n      \"frequencyPenalty\",\n      \"bestOf\",\n      \"logitBias\",\n      \"seed\",\n    ] satisfies (keyof OpenAICompletionModelSettings)[];\n\n    return Object.fromEntries(\n      Object.entries(this.settings).filter(([key]) =>\n        eventSettingProperties.includes(key)\n      )\n    );\n  }\n\n  withTextPrompt() {\n    return this.withPromptTemplate(text());\n  }\n\n  withInstructionPrompt() {\n    return this.withPromptTemplate(instruction());\n  }\n\n  withChatPrompt(options?: { user?: string; assistant?: string }) {\n    return this.withPromptTemplate(chat(options));\n  }\n\n  withPromptTemplate<INPUT_PROMPT>(\n    promptTemplate: TextGenerationPromptTemplate<INPUT_PROMPT, string>\n  ): PromptTemplateTextStreamingModel<\n    INPUT_PROMPT,\n    string,\n    OpenAICompletionModelSettings,\n    this\n  > {\n    return new PromptTemplateTextStreamingModel({\n      model: this.withSettings({\n        stopSequences: [\n          ...(this.settings.stopSequences ?? []),\n          ...promptTemplate.stopSequences,\n        ],\n      }),\n      promptTemplate,\n    });\n  }\n\n  withSettings(additionalSettings: Partial<OpenAICompletionModelSettings>) {\n    return new OpenAICompletionModel(\n      Object.assign({}, this.settings, additionalSettings)\n    ) as this;\n  }\n}\n","import { PartialBaseUrlPartsApiConfigurationOptions } from \"../../core/api/BaseUrlApiConfiguration\";\nimport {\n  AzureOpenAIApiConfiguration,\n  AzureOpenAIApiConfigurationOptions,\n} from \"./AzureOpenAIApiConfiguration\";\nimport { OpenAIApiConfiguration } from \"./OpenAIApiConfiguration\";\nimport { OpenAIChatModel, OpenAIChatSettings } from \"./OpenAIChatModel\";\nimport {\n  OpenAICompletionModel,\n  OpenAICompletionModelSettings,\n} from \"./OpenAICompletionModel\";\nimport {\n  OpenAIImageGenerationModel,\n  OpenAIImageGenerationSettings,\n} from \"./OpenAIImageGenerationModel\";\nimport {\n  OpenAISpeechModel,\n  OpenAISpeechModelSettings,\n} from \"./OpenAISpeechModel\";\nimport {\n  OpenAITextEmbeddingModel,\n  OpenAITextEmbeddingModelSettings,\n} from \"./OpenAITextEmbeddingModel\";\nimport {\n  OpenAITranscriptionModel,\n  OpenAITranscriptionModelSettings,\n} from \"./OpenAITranscriptionModel\";\nimport {\n  TikTokenTokenizer,\n  TikTokenTokenizerSettings,\n} from \"./TikTokenTokenizer\";\n\n/**\n * Creates an API configuration for the OpenAI API.\n * It calls the API at https://api.openai.com/v1 and uses the `OPENAI_API_KEY` env variable by default.\n */\nexport function Api(\n  settings: PartialBaseUrlPartsApiConfigurationOptions & {\n    apiKey?: string;\n  }\n) {\n  return new OpenAIApiConfiguration(settings);\n}\n\n/**\n * Configuration for the Azure OpenAI API. This class is responsible for constructing URLs specific to the Azure OpenAI deployment.\n * It creates URLs of the form\n * `https://[resourceName].openai.azure.com/openai/deployments/[deploymentId]/[path]?api-version=[apiVersion]`\n *\n * @see https://learn.microsoft.com/en-us/azure/ai-services/openai/reference\n */\nexport function AzureApi(settings: AzureOpenAIApiConfigurationOptions) {\n  return new AzureOpenAIApiConfiguration(settings);\n}\n\n/**\n * Create a text generation model that calls the OpenAI text completion API.\n *\n * @see https://platform.openai.com/docs/api-reference/completions/create\n *\n * @example\n * const model = openai.CompletionTextGenerator({\n *   model: \"gpt-3.5-turbo-instruct\",\n *   temperature: 0.7,\n *   maxGenerationTokens: 500,\n *   retry: retryWithExponentialBackoff({ maxTries: 5 }),\n * });\n *\n * const text = await generateText({\n *   model,\n *   prompt: \"Write a short story about a robot learning to love:\\n\\n\"\n * });\n *\n * @return A new instance of {@link OpenAICompletionModel}.\n */\nexport function CompletionTextGenerator(\n  settings: OpenAICompletionModelSettings\n) {\n  return new OpenAICompletionModel(settings);\n}\n\n/**\n * Create a text generation model that calls the OpenAI chat completion API.\n *\n * @see https://platform.openai.com/docs/api-reference/chat/create\n *\n * @example\n * const model = openai.ChatTextGenerator({\n *   model: \"gpt-3.5-turbo\",\n *   temperature: 0.7,\n *   maxGenerationTokens: 500,\n * });\n *\n * const text = await generateText({\n *   model,\n *   prompt: [\n *     openai.ChatMessage.system(\n *       \"Write a short story about a robot learning to love:\"\n *     ),\n *   ]\n * });\n */\nexport function ChatTextGenerator(settings: OpenAIChatSettings) {\n  return new OpenAIChatModel(settings);\n}\n\n/**\n * Create a text embedding model that calls the OpenAI embedding API.\n *\n * @see https://platform.openai.com/docs/api-reference/embeddings\n *\n * @example\n * const embeddings = await embedMany({\n *   model: openai.TextEmbedder({ model: \"text-embedding-ada-002\" }),\n *   values: [\n *     \"At first, Nox didn't know what to do with the pup.\",\n *     \"He keenly observed and absorbed everything around him, from the birds in the sky to the trees in the forest.\",\n *   ]\n * });\n *\n * @returns A new instance of {@link OpenAITextEmbeddingModel}.\n */\nexport function TextEmbedder(settings: OpenAITextEmbeddingModelSettings) {\n  return new OpenAITextEmbeddingModel(settings);\n}\n\n/**\n * Synthesize speech using the OpenAI API.\n *\n * @see https://platform.openai.com/docs/api-reference/audio/createSpeech\n *\n * @returns A new instance of {@link OpenAISpeechModel}.\n */\nexport function SpeechGenerator(settings: OpenAISpeechModelSettings) {\n  return new OpenAISpeechModel(settings);\n}\n\n/**\n * Create a transcription model that calls the OpenAI transcription API.\n *\n * @see https://platform.openai.com/docs/api-reference/audio/create\n *\n * @example\n * const data = await fs.promises.readFile(\"data/test.mp3\");\n *\n * const transcription = await transcribe({\n *   model: openai.Transcriber({ model: \"whisper-1\" }),\n *   data: {\n *     type: \"mp3\",\n *     data,\n *   }\n * });\n *\n * @returns A new instance of {@link OpenAITranscriptionModel}.\n */\nexport function Transcriber(settings: OpenAITranscriptionModelSettings) {\n  return new OpenAITranscriptionModel(settings);\n}\n\n/**\n * Create an image generation model that calls the OpenAI AI image creation API.\n *\n * @see https://platform.openai.com/docs/api-reference/images/create\n *\n * @example\n * const image = await generateImage({\n *   model: new OpenAIImageGenerationModel({ size: \"512x512\" }),\n *   prompt: \"the wicked witch of the west in the style of early 19th century painting\"\n * });\n *\n * @returns A new instance of {@link OpenAIImageGenerationModel}.\n */\nexport function ImageGenerator(settings: OpenAIImageGenerationSettings) {\n  return new OpenAIImageGenerationModel(settings);\n}\n\n/**\n * Creates a TikToken tokenizer for OpenAI language models.\n *\n * @see https://github.com/openai/tiktoken\n *\n * @example\n * const tokenizer = openai.Tokenizer({ model: \"gpt-4\" });\n *\n * const text = \"At first, Nox didn't know what to do with the pup.\";\n *\n * const tokenCount = await countTokens(tokenizer, text);\n * const tokens = await tokenizer.tokenize(text);\n * const tokensAndTokenTexts = await tokenizer.tokenizeWithTexts(text);\n * const reconstructedText = await tokenizer.detokenize(tokens);\n *\n * @returns A new instance of {@link TikTokenTokenizer}.\n */\nexport function Tokenizer(settings: TikTokenTokenizerSettings) {\n  return new TikTokenTokenizer(settings);\n}\n\nexport { OpenAIChatPrompt as ChatPrompt } from \"./AbstractOpenAIChatModel\";\nexport { OpenAIChatMessage as ChatMessage } from \"./OpenAIChatMessage\";\n","import { z } from \"zod\";\nimport { FunctionCallOptions } from \"../../core/FunctionOptions\";\nimport { ApiConfiguration } from \"../../core/api/ApiConfiguration\";\nimport { callWithRetryAndThrottle } from \"../../core/api/callWithRetryAndThrottle\";\nimport {\n  ResponseHandler,\n  createJsonResponseHandler,\n  postJsonToApi,\n} from \"../../core/api/postToApi\";\nimport { zodSchema } from \"../../core/schema/ZodSchema\";\nimport { AbstractModel } from \"../../model-function/AbstractModel\";\nimport { PromptTemplate } from \"../../model-function/PromptTemplate\";\nimport {\n  ImageGenerationModel,\n  ImageGenerationModelSettings,\n} from \"../../model-function/generate-image/ImageGenerationModel\";\nimport { PromptTemplateImageGenerationModel } from \"../../model-function/generate-image/PromptTemplateImageGenerationModel\";\nimport { OpenAIApiConfiguration } from \"./OpenAIApiConfiguration\";\nimport { failedOpenAICallResponseHandler } from \"./OpenAIError\";\n\nexport interface OpenAIImageGenerationCallSettings {\n  model: \"dall-e-2\" | \"dall-e-3\";\n  size?: \"256x256\" | \"512x512\" | \"1024x1024\" | \"1792x1024\" | \"1024x1792\";\n  quality?: \"standard\" | \"hd\";\n  style?: \"vivid\" | \"natural\";\n}\n\nexport interface OpenAIImageGenerationSettings\n  extends ImageGenerationModelSettings,\n    OpenAIImageGenerationCallSettings {\n  api?: ApiConfiguration;\n  isUserIdForwardingEnabled?: boolean;\n}\n\n/**\n * Create an image generation model that calls the OpenAI AI image creation API.\n *\n * @see https://platform.openai.com/docs/api-reference/images/create\n *\n * @example\n * const image = await generateImage(\n *   new OpenAIImageGenerationModel({ size: \"512x512\" }),\n *   \"the wicked witch of the west in the style of early 19th century painting\"\n * );\n */\nexport class OpenAIImageGenerationModel\n  extends AbstractModel<OpenAIImageGenerationSettings>\n  implements ImageGenerationModel<string, OpenAIImageGenerationSettings>\n{\n  constructor(settings: OpenAIImageGenerationSettings) {\n    super({ settings });\n  }\n\n  readonly provider = \"openai\" as const;\n  get modelName() {\n    return this.settings.model;\n  }\n\n  async callAPI<RESULT>(\n    prompt: string,\n    callOptions: FunctionCallOptions,\n    options: {\n      responseFormat: OpenAIImageGenerationResponseFormatType<RESULT>;\n    }\n  ): Promise<RESULT> {\n    const api = this.settings.api ?? new OpenAIApiConfiguration();\n    const abortSignal = callOptions.run?.abortSignal;\n    const userId = callOptions.run?.userId;\n    const responseFormat = options.responseFormat;\n\n    return callWithRetryAndThrottle({\n      retry: api.retry,\n      throttle: api.throttle,\n      call: async () =>\n        postJsonToApi({\n          url: api.assembleUrl(\"/images/generations\"),\n          headers: api.headers({\n            functionType: callOptions.functionType,\n            functionId: callOptions.functionId,\n            run: callOptions.run,\n            callId: callOptions.callId,\n          }),\n          body: {\n            prompt,\n            n: this.settings.numberOfGenerations,\n            size: this.settings.size,\n            response_format: responseFormat.type,\n            user: this.settings.isUserIdForwardingEnabled ? userId : undefined,\n          },\n          failedResponseHandler: failedOpenAICallResponseHandler,\n          successfulResponseHandler: responseFormat.handler,\n          abortSignal,\n        }),\n    });\n  }\n\n  get settingsForEvent(): Partial<OpenAIImageGenerationSettings> {\n    const eventSettingProperties: Array<string> = [\n      \"numberOfGenerations\",\n      \"size\",\n      \"quality\",\n      \"style\",\n    ] satisfies (keyof OpenAIImageGenerationSettings)[];\n\n    return Object.fromEntries(\n      Object.entries(this.settings).filter(([key]) =>\n        eventSettingProperties.includes(key)\n      )\n    );\n  }\n\n  async doGenerateImages(prompt: string, options: FunctionCallOptions) {\n    const rawResponse = await this.callAPI(prompt, options, {\n      responseFormat: OpenAIImageGenerationResponseFormat.base64Json,\n    });\n\n    return {\n      rawResponse,\n      base64Images: rawResponse.data.map((item) => item.b64_json),\n    };\n  }\n  withPromptTemplate<INPUT_PROMPT>(\n    promptTemplate: PromptTemplate<INPUT_PROMPT, string>\n  ): PromptTemplateImageGenerationModel<\n    INPUT_PROMPT,\n    string,\n    OpenAIImageGenerationSettings,\n    this\n  > {\n    return new PromptTemplateImageGenerationModel({\n      model: this,\n      promptTemplate,\n    });\n  }\n\n  withSettings(additionalSettings: Partial<OpenAIImageGenerationSettings>) {\n    return new OpenAIImageGenerationModel(\n      Object.assign({}, this.settings, additionalSettings)\n    ) as this;\n  }\n}\n\nexport type OpenAIImageGenerationResponseFormatType<T> = {\n  type: \"b64_json\" | \"url\";\n  handler: ResponseHandler<T>;\n};\n\nconst openAIImageGenerationUrlSchema = z.object({\n  created: z.number(),\n  data: z.array(\n    z.object({\n      url: z.string(),\n    })\n  ),\n});\n\nexport type OpenAIImageGenerationUrlResponse = z.infer<\n  typeof openAIImageGenerationUrlSchema\n>;\n\nconst openAIImageGenerationBase64JsonSchema = z.object({\n  created: z.number(),\n  data: z.array(\n    z.object({\n      b64_json: z.string(),\n    })\n  ),\n});\n\nexport type OpenAIImageGenerationBase64JsonResponse = z.infer<\n  typeof openAIImageGenerationBase64JsonSchema\n>;\n\nexport const OpenAIImageGenerationResponseFormat = {\n  url: {\n    type: \"url\" as const,\n    handler: createJsonResponseHandler(\n      zodSchema(openAIImageGenerationUrlSchema)\n    ),\n  },\n  base64Json: {\n    type: \"b64_json\" as const,\n    handler: createJsonResponseHandler(\n      zodSchema(openAIImageGenerationBase64JsonSchema)\n    ),\n  },\n};\n","import { FunctionCallOptions } from \"../../core/FunctionOptions\";\nimport { ApiConfiguration } from \"../../core/api/ApiConfiguration\";\nimport { callWithRetryAndThrottle } from \"../../core/api/callWithRetryAndThrottle\";\nimport {\n  createAudioMpegResponseHandler,\n  postJsonToApi,\n} from \"../../core/api/postToApi\";\nimport { AbstractModel } from \"../../model-function/AbstractModel\";\nimport {\n  SpeechGenerationModel,\n  SpeechGenerationModelSettings,\n} from \"../../model-function/generate-speech/SpeechGenerationModel\";\nimport { OpenAIApiConfiguration } from \"./OpenAIApiConfiguration\";\nimport { failedOpenAICallResponseHandler } from \"./OpenAIError\";\n\nexport type OpenAISpeechVoice =\n  | \"alloy\"\n  | \"echo\"\n  | \"fable\"\n  | \"onyx\"\n  | \"nova\"\n  | \"shimmer\";\ntype OpenAISpeechModelResponseFormat = \"mp3\" | \"opus\" | \"aac\" | \"flac\";\n\nexport type OpenAISpeechModelType = \"tts-1\" | \"tts-1-hd\";\n\nexport interface OpenAISpeechModelSettings\n  extends SpeechGenerationModelSettings {\n  api?: ApiConfiguration;\n\n  voice: OpenAISpeechVoice;\n  model: OpenAISpeechModelType;\n\n  /**\n   * The speed of the generated audio. Select a value from 0.25 to 4.0. 1.0 is the default.\n   */\n  speed?: number;\n\n  /**\n   * Defaults to mp3.\n   */\n  responseFormat?: OpenAISpeechModelResponseFormat;\n}\n\n/**\n * Synthesize speech using the OpenAI API.\n *\n * @see https://platform.openai.com/docs/api-reference/audio/createSpeech\n */\nexport class OpenAISpeechModel\n  extends AbstractModel<OpenAISpeechModelSettings>\n  implements SpeechGenerationModel<OpenAISpeechModelSettings>\n{\n  constructor(settings: OpenAISpeechModelSettings) {\n    super({ settings });\n  }\n\n  readonly provider = \"openai\" as const;\n\n  get voice() {\n    return this.settings.voice;\n  }\n\n  get modelName() {\n    return this.settings.model;\n  }\n\n  private async callAPI(\n    text: string,\n    callOptions: FunctionCallOptions\n  ): Promise<Uint8Array> {\n    const api = this.settings.api ?? new OpenAIApiConfiguration();\n    const abortSignal = callOptions.run?.abortSignal;\n\n    return callWithRetryAndThrottle({\n      retry: api.retry,\n      throttle: api.throttle,\n      call: async () =>\n        postJsonToApi({\n          url: api.assembleUrl(`/audio/speech`),\n          headers: api.headers({\n            functionType: callOptions.functionType,\n            functionId: callOptions.functionId,\n            run: callOptions.run,\n            callId: callOptions.callId,\n          }),\n          body: {\n            input: text,\n            voice: this.settings.voice,\n            speed: this.settings.speed,\n            model: this.settings.model,\n            response_format: this.settings.responseFormat,\n          },\n          failedResponseHandler: failedOpenAICallResponseHandler,\n          successfulResponseHandler: createAudioMpegResponseHandler(),\n          abortSignal,\n        }),\n    });\n  }\n\n  get settingsForEvent(): Partial<OpenAISpeechModelSettings> {\n    return {\n      voice: this.settings.voice,\n      speed: this.settings.speed,\n      model: this.settings.model,\n      responseFormat: this.settings.responseFormat,\n    };\n  }\n\n  doGenerateSpeechStandard(text: string, options: FunctionCallOptions) {\n    return this.callAPI(text, options);\n  }\n\n  withSettings(additionalSettings: Partial<OpenAISpeechModelSettings>) {\n    return new OpenAISpeechModel({\n      ...this.settings,\n      ...additionalSettings,\n    }) as this;\n  }\n}\n","import z from \"zod\";\nimport { EmbeddingModel } from \"../../model-function/embed/EmbeddingModel\";\nimport { countTokens } from \"../../model-function/tokenize-text/countTokens\";\nimport {\n  AbstractOpenAITextEmbeddingModel,\n  AbstractOpenAITextEmbeddingModelSettings,\n} from \"./AbstractOpenAITextEmbeddingModel\";\nimport { TikTokenTokenizer } from \"./TikTokenTokenizer\";\n\nexport const OPENAI_TEXT_EMBEDDING_MODELS = {\n  \"text-embedding-3-small\": {\n    contextWindowSize: 8192,\n    dimensions: 1536,\n  },\n  \"text-embedding-3-large\": {\n    contextWindowSize: 8192,\n    dimensions: 3072,\n  },\n\n  \"text-embedding-ada-002\": {\n    contextWindowSize: 8192,\n    dimensions: 1536,\n  },\n};\n\nexport type OpenAITextEmbeddingModelType =\n  keyof typeof OPENAI_TEXT_EMBEDDING_MODELS;\n\nexport interface OpenAITextEmbeddingModelSettings\n  extends AbstractOpenAITextEmbeddingModelSettings {\n  model: OpenAITextEmbeddingModelType;\n}\n\nexport const openAITextEmbeddingResponseSchema = z.object({\n  object: z.literal(\"list\"),\n  data: z.array(\n    z.object({\n      object: z.literal(\"embedding\"),\n      embedding: z.array(z.number()),\n      index: z.number(),\n    })\n  ),\n  model: z.string(),\n  usage: z\n    .object({\n      prompt_tokens: z.number(),\n      total_tokens: z.number(),\n    })\n    .optional(), // for openai-compatible models\n});\n\n/**\n * Create a text embedding model that calls the OpenAI embedding API.\n *\n * @see https://platform.openai.com/docs/api-reference/embeddings\n *\n * @example\n * const embeddings = await embedMany(\n *   new OpenAITextEmbeddingModel({ model: \"text-embedding-ada-002\" }),\n *   [\n *     \"At first, Nox didn't know what to do with the pup.\",\n *     \"He keenly observed and absorbed everything around him, from the birds in the sky to the trees in the forest.\",\n *   ]\n * );\n */\nexport class OpenAITextEmbeddingModel\n  extends AbstractOpenAITextEmbeddingModel<OpenAITextEmbeddingModelSettings>\n  implements EmbeddingModel<string, OpenAITextEmbeddingModelSettings>\n{\n  constructor(settings: OpenAITextEmbeddingModelSettings) {\n    super(settings);\n\n    this.tokenizer = new TikTokenTokenizer({ model: this.modelName });\n    this.contextWindowSize =\n      OPENAI_TEXT_EMBEDDING_MODELS[this.modelName].contextWindowSize;\n\n    this.dimensions =\n      this.settings.dimensions ??\n      OPENAI_TEXT_EMBEDDING_MODELS[this.modelName].dimensions;\n  }\n\n  readonly provider = \"openai\" as const;\n  get modelName() {\n    return this.settings.model;\n  }\n\n  readonly dimensions: number;\n\n  readonly tokenizer: TikTokenTokenizer;\n  readonly contextWindowSize: number;\n\n  async countTokens(input: string) {\n    return countTokens(this.tokenizer, input);\n  }\n\n  get settingsForEvent(): Partial<OpenAITextEmbeddingModelSettings> {\n    return {};\n  }\n\n  withSettings(additionalSettings: OpenAITextEmbeddingModelSettings) {\n    return new OpenAITextEmbeddingModel(\n      Object.assign({}, this.settings, additionalSettings)\n    ) as this;\n  }\n}\n","import { z } from \"zod\";\nimport { FunctionCallOptions } from \"../../core/FunctionOptions\";\nimport { ApiConfiguration } from \"../../core/api/ApiConfiguration\";\nimport { callWithRetryAndThrottle } from \"../../core/api/callWithRetryAndThrottle\";\nimport {\n  ResponseHandler,\n  createJsonResponseHandler,\n  createTextResponseHandler,\n  postToApi,\n} from \"../../core/api/postToApi\";\nimport { zodSchema } from \"../../core/schema/ZodSchema\";\nimport { AbstractModel } from \"../../model-function/AbstractModel\";\nimport {\n  TranscriptionModel,\n  TranscriptionModelSettings,\n} from \"../../model-function/generate-transcription/TranscriptionModel\";\nimport { getAudioFileExtension } from \"../../util/audio/getAudioFileExtension\";\nimport {\n  DataContent,\n  convertDataContentToUint8Array,\n} from \"../../util/format/DataContent\";\nimport { OpenAIApiConfiguration } from \"./OpenAIApiConfiguration\";\nimport { failedOpenAICallResponseHandler } from \"./OpenAIError\";\n\ntype OpenAITranscriptionModelType = \"whisper-1\";\n\nexport interface OpenAITranscriptionModelSettings\n  extends TranscriptionModelSettings {\n  api?: ApiConfiguration;\n\n  /**\n   * ID of the model to use. Only whisper-1 is currently available.\n   */\n  model: OpenAITranscriptionModelType;\n\n  /**\n   * The language of the input audio. Supplying the input language in ISO-639-1 format will improve accuracy and latency.\n   */\n  language?: string; // ISO-639-1 code\n\n  /**\n   * The sampling temperature, between 0 and 1.\n   * Higher values like 0.8 will make the output more random,\n   * while lower values like 0.2 will make it more focused and deterministic.\n   * If set to 0, the model will use log probability to automatically\n   * increase the temperature until certain thresholds are hit.\n   */\n  temperature?: number;\n\n  /**\n   * An optional text to guide the model's style or continue a previous audio segment. The prompt should match the audio language.\n   */\n  prompt?: string;\n}\n\n/**\n * Create a transcription model that calls the OpenAI transcription API.\n *\n * @see https://platform.openai.com/docs/api-reference/audio/create\n *\n * @example\n * const data = await fs.promises.readFile(\"data/test.mp3\");\n *\n * const transcription = await transcribe(\n *   new OpenAITranscriptionModel({ model: \"whisper-1\" }),\n *   {\n *     type: \"mp3\",\n *     data,\n *   }\n * );\n */\nexport class OpenAITranscriptionModel\n  extends AbstractModel<OpenAITranscriptionModelSettings>\n  implements TranscriptionModel<OpenAITranscriptionModelSettings>\n{\n  constructor(settings: OpenAITranscriptionModelSettings) {\n    super({ settings });\n  }\n\n  readonly provider = \"openai\" as const;\n  get modelName() {\n    return this.settings.model;\n  }\n\n  async doTranscribe(\n    {\n      audioData,\n      mimeType,\n    }: {\n      audioData: DataContent;\n      mimeType: string;\n    },\n    options: FunctionCallOptions\n  ) {\n    const rawResponse = await this.callAPI(\n      {\n        fileExtension: getAudioFileExtension(mimeType),\n        audioData: convertDataContentToUint8Array(audioData),\n      },\n      options,\n      { responseFormat: OpenAITranscriptionResponseFormat.verboseJson }\n    );\n\n    return {\n      rawResponse,\n      transcription: rawResponse.text,\n    };\n  }\n\n  async callAPI<RESULT>(\n    input: {\n      fileExtension: string;\n      audioData: Uint8Array;\n    },\n    callOptions: FunctionCallOptions,\n    options: {\n      responseFormat: OpenAITranscriptionResponseFormatType<RESULT>;\n    }\n  ): Promise<RESULT> {\n    const api = this.settings.api ?? new OpenAIApiConfiguration();\n    const abortSignal = callOptions?.run?.abortSignal;\n\n    return callWithRetryAndThrottle({\n      retry: api.retry,\n      throttle: api.throttle,\n      call: async () => {\n        const fileName = `audio.${input.fileExtension}`;\n\n        const formData = new FormData();\n        formData.append(\"file\", new Blob([input.audioData]), fileName);\n        formData.append(\"model\", this.settings.model);\n\n        if (this.settings.prompt != null) {\n          formData.append(\"prompt\", this.settings.prompt);\n        }\n\n        if (options.responseFormat != null) {\n          formData.append(\"response_format\", options.responseFormat.type);\n        }\n\n        if (this.settings.temperature != null) {\n          formData.append(\"temperature\", this.settings.temperature.toString());\n        }\n\n        if (this.settings.language != null) {\n          formData.append(\"language\", this.settings.language);\n        }\n\n        return postToApi({\n          url: api.assembleUrl(\"/audio/transcriptions\"),\n          headers: api.headers({\n            functionType: callOptions.functionType,\n            functionId: callOptions.functionId,\n            run: callOptions.run,\n            callId: callOptions.callId,\n          }),\n          body: {\n            content: formData,\n            values: {\n              model: this.settings.model,\n              prompt: this.settings.prompt,\n              response_format: options.responseFormat,\n              temperature: this.settings.temperature,\n              language: this.settings.language,\n            },\n          },\n          failedResponseHandler: failedOpenAICallResponseHandler,\n          successfulResponseHandler: options.responseFormat.handler,\n          abortSignal,\n        });\n      },\n    });\n  }\n\n  get settingsForEvent(): Partial<OpenAITranscriptionModelSettings> {\n    return {\n      language: this.settings.language,\n      temperature: this.settings.temperature,\n    };\n  }\n\n  withSettings(additionalSettings: OpenAITranscriptionModelSettings) {\n    return new OpenAITranscriptionModel(\n      Object.assign({}, this.settings, additionalSettings)\n    ) as this;\n  }\n}\n\nconst openAITranscriptionJsonSchema = z.object({\n  text: z.string(),\n});\n\nexport type OpenAITranscriptionJsonResponse = z.infer<\n  typeof openAITranscriptionJsonSchema\n>;\n\nconst openAITranscriptionVerboseJsonSchema = z.object({\n  task: z.literal(\"transcribe\"),\n  language: z.string(),\n  duration: z.number(),\n  segments: z.array(\n    z.object({\n      id: z.number(),\n      seek: z.number(),\n      start: z.number(),\n      end: z.number(),\n      text: z.string(),\n      tokens: z.array(z.number()),\n      temperature: z.number(),\n      avg_logprob: z.number(),\n      compression_ratio: z.number(),\n      no_speech_prob: z.number(),\n      transient: z.boolean().optional(),\n    })\n  ),\n  text: z.string(),\n});\n\nexport type OpenAITranscriptionVerboseJsonResponse = z.infer<\n  typeof openAITranscriptionVerboseJsonSchema\n>;\n\nexport type OpenAITranscriptionResponseFormatType<T> = {\n  type: \"json\" | \"text\" | \"srt\" | \"verbose_json\" | \"vtt\";\n  handler: ResponseHandler<T>;\n};\n\nexport const OpenAITranscriptionResponseFormat = {\n  json: {\n    type: \"json\" as const,\n    handler: createJsonResponseHandler(\n      zodSchema(openAITranscriptionJsonSchema)\n    ),\n  },\n  verboseJson: {\n    type: \"verbose_json\" as const,\n    handler: createJsonResponseHandler(\n      zodSchema(openAITranscriptionVerboseJsonSchema)\n    ),\n  },\n  text: {\n    type: \"text\" as const,\n    handler: createTextResponseHandler(),\n  },\n  srt: {\n    type: \"srt\" as const,\n    handler: createTextResponseHandler(),\n  },\n  vtt: {\n    type: \"vtt\" as const,\n    handler: createTextResponseHandler(),\n  },\n};\n","export function getAudioFileExtension(mimeType: string) {\n  const normalizedMimeType = mimeType.split(\";\")[0].toLowerCase();\n\n  switch (normalizedMimeType) {\n    case \"audio/webm\":\n      return \"webm\";\n    case \"audio/mp3\":\n      return \"mp3\";\n    case \"audio/wav\":\n      return \"wav\";\n    case \"audio/mp4\":\n      return \"mp4\";\n    case \"audio/mpeg\":\n    case \"audio/mpga\":\n      return \"mpeg\";\n    case \"audio/ogg\":\n    case \"audio/oga\":\n      return \"ogg\";\n    case \"audio/flac\":\n      return \"flac\";\n    case \"audio/m4a\":\n      return \"m4a\";\n    default:\n      throw new Error(`Unsupported audio format: ${mimeType}`);\n  }\n}\n","import {\n  BaseUrlApiConfigurationWithDefaults,\n  PartialBaseUrlPartsApiConfigurationOptions,\n} from \"../../core/api/BaseUrlApiConfiguration\";\nimport { loadApiKey } from \"../../core/api/loadApiKey\";\nimport { OpenAICompatibleApiConfiguration } from \"./OpenAICompatibleApiConfiguration\";\n\n/**\n * Configuration for the Fireworks.ai API.\n *\n * It calls the API at https://api.fireworks.ai/inference/v1 and uses the `FIREWORKS_API_KEY` api key environment variable.\n *\n * @see https://readme.fireworks.ai/docs/openai-compatibility\n */\nexport class FireworksAIApiConfiguration\n  extends BaseUrlApiConfigurationWithDefaults\n  implements OpenAICompatibleApiConfiguration\n{\n  constructor(\n    settings: PartialBaseUrlPartsApiConfigurationOptions & {\n      apiKey?: string;\n    } = {}\n  ) {\n    super({\n      ...settings,\n      headers: {\n        Authorization: `Bearer ${loadApiKey({\n          apiKey: settings.apiKey,\n          environmentVariableName: \"FIREWORKS_API_KEY\",\n          description: \"Fireworks AI\",\n        })}`,\n      },\n      baseUrlDefaults: {\n        protocol: \"https\",\n        host: \"api.fireworks.ai\",\n        port: \"443\",\n        path: \"/inference/v1\",\n      },\n    });\n  }\n\n  readonly provider = \"openaicompatible-fireworksai\";\n}\n","import {\n  FlexibleObjectFromTextPromptTemplate,\n  ObjectFromTextPromptTemplate,\n} from \"../../model-function/generate-object/ObjectFromTextPromptTemplate\";\nimport { ObjectFromTextStreamingModel } from \"../../model-function/generate-object/ObjectFromTextStreamingModel\";\nimport { PromptTemplateFullTextModel } from \"../../model-function/generate-text/PromptTemplateFullTextModel\";\nimport {\n  TextStreamingBaseModel,\n  TextStreamingModel,\n  textGenerationModelProperties,\n} from \"../../model-function/generate-text/TextGenerationModel\";\nimport { TextGenerationPromptTemplate } from \"../../model-function/generate-text/TextGenerationPromptTemplate\";\nimport { ToolCallGenerationModel } from \"../../tool/generate-tool-call/ToolCallGenerationModel\";\nimport { ToolCallsGenerationModel } from \"../../tool/generate-tool-calls/ToolCallsGenerationModel\";\nimport {\n  AbstractOpenAIChatModel,\n  AbstractOpenAIChatSettings,\n  OpenAIChatPrompt,\n} from \"../openai/AbstractOpenAIChatModel\";\nimport { chat, instruction, text } from \"../openai/OpenAIChatPromptTemplate\";\nimport {\n  OpenAICompatibleApiConfiguration,\n  OpenAICompatibleProviderName,\n} from \"./OpenAICompatibleApiConfiguration\";\n\nexport interface OpenAICompatibleChatSettings\n  extends AbstractOpenAIChatSettings {\n  api: OpenAICompatibleApiConfiguration; // required\n  provider?: OpenAICompatibleProviderName;\n}\n\n/**\n * Create a text generation model that calls an API that is compatible with OpenAI's chat API.\n *\n * Please note that many providers implement the API with slight differences, which can cause\n * unexpected errors and different behavior in less common scenarios.\n *\n * @see https://platform.openai.com/docs/api-reference/chat/create\n */\nexport class OpenAICompatibleChatModel\n  extends AbstractOpenAIChatModel<OpenAICompatibleChatSettings>\n  implements\n    TextStreamingBaseModel<OpenAIChatPrompt, OpenAICompatibleChatSettings>,\n    ToolCallGenerationModel<OpenAIChatPrompt, OpenAICompatibleChatSettings>,\n    ToolCallsGenerationModel<OpenAIChatPrompt, OpenAICompatibleChatSettings>\n{\n  constructor(settings: OpenAICompatibleChatSettings) {\n    super(settings);\n  }\n\n  get provider(): OpenAICompatibleProviderName {\n    return (\n      this.settings.provider ?? this.settings.api.provider ?? \"openaicompatible\"\n    );\n  }\n\n  get modelName() {\n    return this.settings.model;\n  }\n\n  readonly contextWindowSize = undefined;\n  readonly tokenizer = undefined;\n  readonly countPromptTokens = undefined;\n\n  get settingsForEvent(): Partial<OpenAICompatibleChatSettings> {\n    const eventSettingProperties: Array<string> = [\n      ...textGenerationModelProperties,\n\n      \"functions\",\n      \"functionCall\",\n      \"temperature\",\n      \"topP\",\n      \"presencePenalty\",\n      \"frequencyPenalty\",\n      \"logitBias\",\n      \"seed\",\n      \"responseFormat\",\n    ] satisfies (keyof OpenAICompatibleChatSettings)[];\n\n    return Object.fromEntries(\n      Object.entries(this.settings).filter(([key]) =>\n        eventSettingProperties.includes(key)\n      )\n    );\n  }\n\n  asObjectGenerationModel<INPUT_PROMPT, OpenAIChatPrompt>(\n    promptTemplate:\n      | ObjectFromTextPromptTemplate<INPUT_PROMPT, OpenAIChatPrompt>\n      | FlexibleObjectFromTextPromptTemplate<INPUT_PROMPT, unknown>\n  ) {\n    return \"adaptModel\" in promptTemplate\n      ? new ObjectFromTextStreamingModel({\n          model: promptTemplate.adaptModel(this),\n          template: promptTemplate,\n        })\n      : new ObjectFromTextStreamingModel({\n          model: this as TextStreamingModel<OpenAIChatPrompt>,\n          template: promptTemplate,\n        });\n  }\n\n  withTextPrompt() {\n    return this.withPromptTemplate(text());\n  }\n\n  withInstructionPrompt() {\n    return this.withPromptTemplate(instruction());\n  }\n\n  withChatPrompt() {\n    return this.withPromptTemplate(chat());\n  }\n\n  withPromptTemplate<INPUT_PROMPT>(\n    promptTemplate: TextGenerationPromptTemplate<INPUT_PROMPT, OpenAIChatPrompt>\n  ): PromptTemplateFullTextModel<\n    INPUT_PROMPT,\n    OpenAIChatPrompt,\n    OpenAICompatibleChatSettings,\n    this\n  > {\n    return new PromptTemplateFullTextModel({\n      model: this.withSettings({\n        stopSequences: [\n          ...(this.settings.stopSequences ?? []),\n          ...promptTemplate.stopSequences,\n        ],\n      }),\n      promptTemplate,\n    });\n  }\n\n  withJsonOutput() {\n    return this.withSettings({ responseFormat: { type: \"json_object\" } });\n  }\n\n  withSettings(additionalSettings: Partial<OpenAICompatibleChatSettings>) {\n    return new OpenAICompatibleChatModel(\n      Object.assign({}, this.settings, additionalSettings)\n    ) as this;\n  }\n}\n","import { PromptTemplateTextStreamingModel } from \"../../model-function/generate-text/PromptTemplateTextStreamingModel\";\nimport {\n  TextStreamingBaseModel,\n  textGenerationModelProperties,\n} from \"../../model-function/generate-text/TextGenerationModel\";\nimport { TextGenerationPromptTemplate } from \"../../model-function/generate-text/TextGenerationPromptTemplate\";\nimport {\n  chat,\n  instruction,\n  text,\n} from \"../../model-function/generate-text/prompt-template/TextPromptTemplate\";\nimport {\n  AbstractOpenAICompletionModel,\n  AbstractOpenAICompletionModelSettings,\n} from \"../openai/AbstractOpenAICompletionModel\";\nimport {\n  OpenAICompatibleApiConfiguration,\n  OpenAICompatibleProviderName,\n} from \"./OpenAICompatibleApiConfiguration\";\n\nexport interface OpenAICompatibleCompletionModelSettings\n  extends AbstractOpenAICompletionModelSettings {\n  api: OpenAICompatibleApiConfiguration; // required\n  provider?: OpenAICompatibleProviderName;\n}\n\n/**\n * Create a text generation model that calls an API that is compatible with OpenAI's completion API.\n *\n * Please note that many providers implement the API with slight differences, which can cause\n * unexpected errors and different behavior in less common scenarios.\n *\n * @see https://platform.openai.com/docs/api-reference/completions/create\n */\nexport class OpenAICompatibleCompletionModel\n  extends AbstractOpenAICompletionModel<OpenAICompatibleCompletionModelSettings>\n  implements\n    TextStreamingBaseModel<string, OpenAICompatibleCompletionModelSettings>\n{\n  constructor(settings: OpenAICompatibleCompletionModelSettings) {\n    super(settings);\n  }\n\n  get provider(): OpenAICompatibleProviderName {\n    return (\n      this.settings.provider ?? this.settings.api.provider ?? \"openaicompatible\"\n    );\n  }\n\n  get modelName() {\n    return this.settings.model;\n  }\n\n  readonly contextWindowSize = undefined;\n  readonly tokenizer = undefined;\n  readonly countPromptTokens = undefined;\n\n  get settingsForEvent(): Partial<OpenAICompatibleCompletionModelSettings> {\n    const eventSettingProperties: Array<string> = [\n      ...textGenerationModelProperties,\n\n      \"suffix\",\n      \"temperature\",\n      \"topP\",\n      \"logprobs\",\n      \"echo\",\n      \"presencePenalty\",\n      \"frequencyPenalty\",\n      \"bestOf\",\n      \"logitBias\",\n      \"seed\",\n    ] satisfies (keyof OpenAICompatibleCompletionModelSettings)[];\n\n    return Object.fromEntries(\n      Object.entries(this.settings).filter(([key]) =>\n        eventSettingProperties.includes(key)\n      )\n    );\n  }\n\n  withTextPrompt() {\n    return this.withPromptTemplate(text());\n  }\n\n  withInstructionPrompt() {\n    return this.withPromptTemplate(instruction());\n  }\n\n  withChatPrompt(options?: { user?: string; assistant?: string }) {\n    return this.withPromptTemplate(chat(options));\n  }\n\n  withPromptTemplate<INPUT_PROMPT>(\n    promptTemplate: TextGenerationPromptTemplate<INPUT_PROMPT, string>\n  ): PromptTemplateTextStreamingModel<\n    INPUT_PROMPT,\n    string,\n    OpenAICompatibleCompletionModelSettings,\n    this\n  > {\n    return new PromptTemplateTextStreamingModel({\n      model: this.withSettings({\n        stopSequences: [\n          ...(this.settings.stopSequences ?? []),\n          ...promptTemplate.stopSequences,\n        ],\n      }),\n      promptTemplate,\n    });\n  }\n\n  withSettings(\n    additionalSettings: Partial<OpenAICompatibleCompletionModelSettings>\n  ) {\n    return new OpenAICompatibleCompletionModel(\n      Object.assign({}, this.settings, additionalSettings)\n    ) as this;\n  }\n}\n","import { PartialBaseUrlPartsApiConfigurationOptions } from \"../../core/api/BaseUrlApiConfiguration\";\nimport { FireworksAIApiConfiguration } from \"./FireworksAIApiConfiguration\";\nimport {\n  OpenAICompatibleChatModel,\n  OpenAICompatibleChatSettings,\n} from \"./OpenAICompatibleChatModel\";\nimport { OpenAICompatibleCompletionModel } from \"./OpenAICompatibleCompletionModel\";\nimport {\n  OpenAICompatibleTextEmbeddingModel,\n  OpenAICompatibleTextEmbeddingModelSettings,\n} from \"./OpenAICompatibleTextEmbeddingModel\";\nimport { PerplexityApiConfiguration } from \"./PerplexityApiConfiguration\";\nimport { TogetherAIApiConfiguration } from \"./TogetherAIApiConfiguration\";\n\n/**\n * Configuration for the Fireworks.ai API.\n *\n * It calls the API at https://api.fireworks.ai/inference/v1 and uses the `FIREWORKS_API_KEY` api key environment variable.\n *\n * @see https://readme.fireworks.ai/docs/openai-compatibility\n */\nexport function FireworksAIApi(\n  settings: PartialBaseUrlPartsApiConfigurationOptions & {\n    apiKey?: string;\n  } = {}\n) {\n  return new FireworksAIApiConfiguration(settings);\n}\n\n/**\n * Configuration for the Perplexity API.\n *\n * It calls the API at https://api.perplexity.ai/ and uses the `PERPLEXITY_API_KEY` api key environment variable.\n *\n * @see https://docs.perplexity.ai/reference/post_chat_completions\n */\nexport function PerplexityApi(\n  settings: PartialBaseUrlPartsApiConfigurationOptions & {\n    apiKey?: string;\n  } = {}\n) {\n  return new PerplexityApiConfiguration(settings);\n}\n\n/**\n * Configuration for the Together.ai API.\n *\n * It calls the API at https://api.together.xyz/v1 and uses the `TOGETHER_API_KEY` api key environment variable.\n *\n * @see https://docs.together.ai/docs/openai-api-compatibility\n */\nexport function TogetherAIApi(\n  settings: PartialBaseUrlPartsApiConfigurationOptions & {\n    apiKey?: string;\n  } = {}\n) {\n  return new TogetherAIApiConfiguration(settings);\n}\n\n/**\n * Create a text generation model that calls an API that is compatible with OpenAI's completion API.\n *\n * Please note that many providers implement the API with slight differences, which can cause\n * unexpected errors and different behavior in less common scenarios.\n *\n * @see https://platform.openai.com/docs/api-reference/completions/create\n *\n * @example\n * ```ts\n * const model = openaicompatible.CompletionTextGenerator({\n *   model: \"provider-specific-model-name\",\n *   temperature: 0.7,\n *   maxGenerationTokens: 500,\n * });\n *\n * const text = await generateText({\n *   model,\n *   prompt: \"Write a short story about a robot learning to love:\"\n * });\n * ```\n */\nexport function CompletionTextGenerator(\n  settings: OpenAICompatibleChatSettings\n) {\n  return new OpenAICompatibleCompletionModel(settings);\n}\n\n/**\n * Create a text generation model that calls an API that is compatible with OpenAI's chat API.\n *\n * Please note that many providers implement the API with slight differences, which can cause\n * unexpected errors and different behavior in less common scenarios.\n *\n * @see https://platform.openai.com/docs/api-reference/chat/create\n *\n * @example\n * ```ts\n * const model = openaicompatible.ChatTextGenerator({\n *   model: \"provider-specific-model-name\",\n *   temperature: 0.7,\n *   maxGenerationTokens: 500,\n * });\n *\n * const text = await generateText({\n *   model,\n *   prompt: [\n *     openai.ChatMessage.user(\n *       \"Write a short story about a robot learning to love:\"\n *     ),\n *   ]\n * });\n * ```\n */\nexport function ChatTextGenerator(settings: OpenAICompatibleChatSettings) {\n  return new OpenAICompatibleChatModel(settings);\n}\n\n/**\n * Create a text embedding model that calls the OpenAI embedding API.\n *\n * @see https://platform.openai.com/docs/api-reference/embeddings\n *\n * @example\n * const embeddings = await embedMany({\n *   model: openaicompatible.TextEmbedder({ model: \"provider-specific-model-name\" }),\n *   values: [\n *     \"At first, Nox didn't know what to do with the pup.\",\n *     \"He keenly observed and absorbed everything around him, from the birds in the sky to the trees in the forest.\",\n *   ]\n * });\n *\n * @returns A new instance of {@link OpenAITextEmbeddingModel}.\n */\nexport function TextEmbedder(\n  settings: OpenAICompatibleTextEmbeddingModelSettings\n) {\n  return new OpenAICompatibleTextEmbeddingModel(settings);\n}\n","import { EmbeddingModel } from \"../../model-function/embed/EmbeddingModel\";\nimport {\n  AbstractOpenAITextEmbeddingModel,\n  AbstractOpenAITextEmbeddingModelSettings,\n} from \"../openai/AbstractOpenAITextEmbeddingModel\";\nimport {\n  OpenAICompatibleApiConfiguration,\n  OpenAICompatibleProviderName,\n} from \"./OpenAICompatibleApiConfiguration\";\n\nexport interface OpenAICompatibleTextEmbeddingModelSettings\n  extends AbstractOpenAITextEmbeddingModelSettings {\n  api: OpenAICompatibleApiConfiguration; // required\n  provider?: OpenAICompatibleProviderName;\n  model: string;\n  dimensions?: number;\n}\n\nexport class OpenAICompatibleTextEmbeddingModel\n  extends AbstractOpenAITextEmbeddingModel<OpenAICompatibleTextEmbeddingModelSettings>\n  implements EmbeddingModel<string, OpenAICompatibleTextEmbeddingModelSettings>\n{\n  constructor(settings: OpenAICompatibleTextEmbeddingModelSettings) {\n    super(settings);\n  }\n\n  get provider(): OpenAICompatibleProviderName {\n    return (\n      this.settings.provider ?? this.settings.api.provider ?? \"openaicompatible\"\n    );\n  }\n\n  get modelName() {\n    return this.settings.model;\n  }\n\n  get dimensions() {\n    return this.settings.dimensions;\n  }\n\n  get settingsForEvent(): Partial<OpenAICompatibleTextEmbeddingModelSettings> {\n    return {\n      dimensions: this.settings.dimensions,\n    };\n  }\n\n  withSettings(additionalSettings: OpenAICompatibleTextEmbeddingModelSettings) {\n    return new OpenAICompatibleTextEmbeddingModel(\n      Object.assign({}, this.settings, additionalSettings)\n    ) as this;\n  }\n}\n","import {\n  BaseUrlApiConfigurationWithDefaults,\n  PartialBaseUrlPartsApiConfigurationOptions,\n} from \"../../core/api/BaseUrlApiConfiguration\";\nimport { loadApiKey } from \"../../core/api/loadApiKey\";\nimport { OpenAICompatibleApiConfiguration } from \"./OpenAICompatibleApiConfiguration\";\n\n/**\n * Configuration for the Perplexity API.\n *\n * It calls the API at https://api.perplexity.ai/ and uses the `PERPLEXITY_API_KEY` api key environment variable.\n *\n * @see https://docs.perplexity.ai/reference/post_chat_completions\n */\nexport class PerplexityApiConfiguration\n  extends BaseUrlApiConfigurationWithDefaults\n  implements OpenAICompatibleApiConfiguration\n{\n  constructor(\n    settings: PartialBaseUrlPartsApiConfigurationOptions & {\n      apiKey?: string;\n    } = {}\n  ) {\n    super({\n      ...settings,\n      headers: {\n        Authorization: `Bearer ${loadApiKey({\n          apiKey: settings.apiKey,\n          environmentVariableName: \"PERPLEXITY_API_KEY\",\n          description: \"Perplexity\",\n        })}`,\n      },\n      baseUrlDefaults: {\n        protocol: \"https\",\n        host: \"api.perplexity.ai\",\n        port: \"443\",\n        path: \"\",\n      },\n    });\n  }\n\n  readonly provider = \"openaicompatible-perplexity\";\n}\n","import {\n  BaseUrlApiConfigurationWithDefaults,\n  PartialBaseUrlPartsApiConfigurationOptions,\n} from \"../../core/api/BaseUrlApiConfiguration\";\nimport { loadApiKey } from \"../../core/api/loadApiKey\";\nimport { OpenAICompatibleApiConfiguration } from \"./OpenAICompatibleApiConfiguration\";\n\n/**\n * Configuration for the Together.ai API.\n *\n * It calls the API at https://api.together.xyz/v1 and uses the `TOGETHER_API_KEY` api key environment variable.\n *\n * @see https://docs.together.ai/docs/openai-api-compatibility\n */\nexport class TogetherAIApiConfiguration\n  extends BaseUrlApiConfigurationWithDefaults\n  implements OpenAICompatibleApiConfiguration\n{\n  constructor(\n    settings: PartialBaseUrlPartsApiConfigurationOptions & {\n      apiKey?: string;\n    } = {}\n  ) {\n    super({\n      ...settings,\n      headers: {\n        Authorization: `Bearer ${loadApiKey({\n          apiKey: settings.apiKey,\n          environmentVariableName: \"TOGETHER_API_KEY\",\n          description: \"Together AI\",\n        })}`,\n      },\n      baseUrlDefaults: {\n        protocol: \"https\",\n        host: \"api.together.xyz\",\n        port: \"443\",\n        path: \"/v1\",\n      },\n    });\n  }\n\n  readonly provider = \"openaicompatible-togetherai\";\n}\n","import {\n  BaseUrlApiConfigurationWithDefaults,\n  PartialBaseUrlPartsApiConfigurationOptions,\n} from \"../../core/api/BaseUrlApiConfiguration\";\nimport { loadApiKey } from \"../../core/api/loadApiKey\";\n\n/**\n * Creates an API configuration for the Stability AI API.\n * It calls the API at https://api.stability.ai/v1 by default and uses the `STABILITY_API_KEY` environment variable.\n */\nexport class StabilityApiConfiguration extends BaseUrlApiConfigurationWithDefaults {\n  constructor(\n    settings: PartialBaseUrlPartsApiConfigurationOptions & {\n      apiKey?: string;\n    } = {}\n  ) {\n    super({\n      ...settings,\n      headers: settings.headers ?? {\n        Authorization: `Bearer ${loadApiKey({\n          apiKey: settings.apiKey,\n          environmentVariableName: \"STABILITY_API_KEY\",\n          description: \"Stability\",\n        })}`,\n      },\n      baseUrlDefaults: {\n        protocol: \"https\",\n        host: \"api.stability.ai\",\n        port: \"443\",\n        path: \"/v1\",\n      },\n    });\n  }\n}\n","import { z } from \"zod\";\nimport { ApiCallError } from \"../../core/api/ApiCallError\";\nimport {\n  ResponseHandler,\n  createJsonErrorResponseHandler,\n} from \"../../core/api/postToApi\";\nimport { zodSchema } from \"../../core/schema/ZodSchema\";\n\nconst stabilityErrorDataSchema = z.object({\n  message: z.string(),\n});\n\nexport type StabilityErrorData = z.infer<typeof stabilityErrorDataSchema>;\n\nexport const failedStabilityCallResponseHandler: ResponseHandler<ApiCallError> =\n  createJsonErrorResponseHandler({\n    errorSchema: zodSchema(stabilityErrorDataSchema),\n    errorToMessage: (error) => error.message,\n  });\n","import { BaseUrlPartsApiConfigurationOptions } from \"../../core/api/BaseUrlApiConfiguration\";\nimport { StabilityApiConfiguration } from \"./StabilityApiConfiguration\";\nimport {\n  StabilityImageGenerationModel,\n  StabilityImageGenerationSettings,\n} from \"./StabilityImageGenerationModel\";\n\n/**\n * Creates an API configuration for the Stability AI API.\n * It calls the API at https://api.stability.ai/v1 by default and uses the `STABILITY_API_KEY` environment variable.\n */\nexport function Api(\n  settings: Partial<BaseUrlPartsApiConfigurationOptions> & {\n    apiKey?: string;\n  }\n) {\n  return new StabilityApiConfiguration(settings);\n}\n\n/**\n * Create an image generation model that calls the Stability AI image generation API.\n *\n * @see https://api.stability.ai/docs#tag/v1generation/operation/textToImage\n *\n * @example\n * const image = await generateImage(\n *   stability.ImageGenerator({\n *     model: \"stable-diffusion-v1-6\",\n *     cfgScale: 7,\n *     clipGuidancePreset: \"FAST_BLUE\",\n *     height: 512,\n *     width: 512,\n *     steps: 30,\n *   })\n *   [\n *     { text: \"the wicked witch of the west\" },\n *     { text: \"style of early 19th century painting\", weight: 0.5 },\n *   ]\n * );\n *\n * @returns A new instance of {@link StabilityImageGenerationModel}.\n */\nexport function ImageGenerator(settings: StabilityImageGenerationSettings) {\n  return new StabilityImageGenerationModel(settings);\n}\n","import { z } from \"zod\";\nimport { FunctionCallOptions } from \"../../core/FunctionOptions\";\nimport { ApiConfiguration } from \"../../core/api/ApiConfiguration\";\nimport { callWithRetryAndThrottle } from \"../../core/api/callWithRetryAndThrottle\";\nimport {\n  createJsonResponseHandler,\n  postJsonToApi,\n} from \"../../core/api/postToApi\";\nimport { zodSchema } from \"../../core/schema/ZodSchema\";\nimport { AbstractModel } from \"../../model-function/AbstractModel\";\nimport { PromptTemplate } from \"../../model-function/PromptTemplate\";\nimport {\n  ImageGenerationModel,\n  ImageGenerationModelSettings,\n} from \"../../model-function/generate-image/ImageGenerationModel\";\nimport { PromptTemplateImageGenerationModel } from \"../../model-function/generate-image/PromptTemplateImageGenerationModel\";\nimport { StabilityApiConfiguration } from \"./StabilityApiConfiguration\";\nimport { failedStabilityCallResponseHandler } from \"./StabilityError\";\nimport {\n  StabilityImageGenerationPrompt,\n  mapBasicPromptToStabilityFormat,\n} from \"./StabilityImageGenerationPrompt\";\n\nconst stabilityImageGenerationModels = [\n  \"stable-diffusion-v1-6\",\n  \"stable-diffusion-xl-1024-v1-0\",\n] as const;\n\nexport type StabilityImageGenerationModelType =\n  | (typeof stabilityImageGenerationModels)[number]\n  // string & {} is used to enable auto-completion of literals\n  // while also allowing strings:\n  // eslint-disable-next-line @typescript-eslint/ban-types\n  | (string & {});\n\nexport type StabilityImageGenerationStylePreset =\n  | \"3d-model\"\n  | \"analog-film\"\n  | \"anime\"\n  | \"cinematic\"\n  | \"comic-book\"\n  | \"digital-art\"\n  | \"enhance\"\n  | \"fantasy-art\"\n  | \"isometric\"\n  | \"line-art\"\n  | \"low-poly\"\n  | \"modeling-compound\"\n  | \"neon-punk\"\n  | \"origami\"\n  | \"photographic\"\n  | \"pixel-art\"\n  | \"tile-texture\";\n\nexport type StabilityImageGenerationSampler =\n  | \"DDIM\"\n  | \"DDPM\"\n  | \"K_DPMPP_2M\"\n  | \"K_DPMPP_2S_ANCESTRAL\"\n  | \"K_DPM_2\"\n  | \"K_DPM_2_ANCESTRAL\"\n  | \"K_EULER\"\n  | \"K_EULER_ANCESTRAL\"\n  | \"K_HEUN\"\n  | \"K_LMS\";\n\nexport type StabilityClipGuidancePreset =\n  | \"FAST_BLUE\"\n  | \"FAST_GREEN\"\n  | \"NONE\"\n  | \"SIMPLE\"\n  | \"SLOW\"\n  | \"SLOWER\"\n  | \"SLOWEST\";\n\nexport interface StabilityImageGenerationSettings\n  extends ImageGenerationModelSettings {\n  api?: ApiConfiguration;\n\n  model: StabilityImageGenerationModelType;\n\n  height?: number;\n  width?: number;\n\n  /**\n   * How strictly the diffusion process adheres to the prompt text (higher values keep your image closer to your prompt)\n   */\n  cfgScale?: number;\n\n  clipGuidancePreset?: StabilityClipGuidancePreset;\n\n  /**\n   * Which sampler to use for the diffusion process.\n   * If this value is omitted we'll automatically select an appropriate sampler for you.\n   */\n  sampler?: StabilityImageGenerationSampler;\n\n  /**\n   * Random noise seed (omit this option or use 0 for a random seed).\n   */\n  seed?: number;\n\n  /**\n   * Number of diffusion steps to run.\n   */\n  steps?: number;\n\n  /**\n   * Pass in a style preset to guide the image model towards a particular style.\n   */\n  stylePreset?: StabilityImageGenerationStylePreset;\n}\n\n/**\n * Create an image generation model that calls the Stability AI image generation API.\n *\n * @see https://api.stability.ai/docs#tag/v1generation/operation/textToImage\n *\n * @example\n * const image = await generateImage(\n *   stability.ImageGenerator({\n *     model: \"stable-diffusion-v1-6\",\n *     cfgScale: 7,\n *     clipGuidancePreset: \"FAST_BLUE\",\n *     height: 512,\n *     width: 512,\n *     steps: 30,\n *   })\n *   [\n *     { text: \"the wicked witch of the west\" },\n *     { text: \"style of early 19th century painting\", weight: 0.5 },\n *   ]\n * );\n */\nexport class StabilityImageGenerationModel\n  extends AbstractModel<StabilityImageGenerationSettings>\n  implements\n    ImageGenerationModel<\n      StabilityImageGenerationPrompt,\n      StabilityImageGenerationSettings\n    >\n{\n  constructor(settings: StabilityImageGenerationSettings) {\n    super({ settings });\n  }\n\n  readonly provider = \"stability\" as const;\n\n  get modelName() {\n    return this.settings.model;\n  }\n\n  async callAPI(\n    input: StabilityImageGenerationPrompt,\n    callOptions: FunctionCallOptions\n  ): Promise<StabilityImageGenerationResponse> {\n    const api = this.settings.api ?? new StabilityApiConfiguration();\n    const abortSignal = callOptions.run?.abortSignal;\n\n    return callWithRetryAndThrottle({\n      retry: this.settings.api?.retry,\n      throttle: this.settings.api?.throttle,\n      call: async () =>\n        postJsonToApi({\n          url: api.assembleUrl(\n            `/generation/${this.settings.model}/text-to-image`\n          ),\n          headers: api.headers({\n            functionType: callOptions.functionType,\n            functionId: callOptions.functionId,\n            run: callOptions.run,\n            callId: callOptions.callId,\n          }),\n          body: {\n            height: this.settings.height,\n            width: this.settings.width,\n            text_prompts: input,\n            cfg_scale: this.settings.cfgScale,\n            clip_guidance_preset: this.settings.clipGuidancePreset,\n            sampler: this.settings.sampler,\n            samples: this.settings.numberOfGenerations,\n            seed: this.settings.seed,\n            steps: this.settings.steps,\n            style_preset: this.settings.stylePreset,\n          },\n          failedResponseHandler: failedStabilityCallResponseHandler,\n          successfulResponseHandler: createJsonResponseHandler(\n            zodSchema(stabilityImageGenerationResponseSchema)\n          ),\n          abortSignal,\n        }),\n    });\n  }\n\n  get settingsForEvent(): Partial<StabilityImageGenerationSettings> {\n    return {\n      numberOfGenerations: this.settings.numberOfGenerations,\n      height: this.settings.height,\n      width: this.settings.width,\n      cfgScale: this.settings.cfgScale,\n      clipGuidancePreset: this.settings.clipGuidancePreset,\n      sampler: this.settings.sampler,\n      seed: this.settings.seed,\n      steps: this.settings.steps,\n      stylePreset: this.settings.stylePreset,\n    };\n  }\n\n  async doGenerateImages(\n    prompt: StabilityImageGenerationPrompt,\n    callOptions: FunctionCallOptions\n  ) {\n    const rawResponse = await this.callAPI(prompt, callOptions);\n\n    return {\n      rawResponse,\n      base64Images: rawResponse.artifacts.map((artifact) => artifact.base64),\n    };\n  }\n\n  withTextPrompt() {\n    return this.withPromptTemplate(mapBasicPromptToStabilityFormat());\n  }\n\n  withPromptTemplate<INPUT_PROMPT>(\n    promptTemplate: PromptTemplate<INPUT_PROMPT, StabilityImageGenerationPrompt>\n  ): PromptTemplateImageGenerationModel<\n    INPUT_PROMPT,\n    StabilityImageGenerationPrompt,\n    StabilityImageGenerationSettings,\n    this\n  > {\n    return new PromptTemplateImageGenerationModel({\n      model: this,\n      promptTemplate,\n    });\n  }\n\n  withSettings(additionalSettings: Partial<StabilityImageGenerationSettings>) {\n    return new StabilityImageGenerationModel(\n      Object.assign({}, this.settings, additionalSettings)\n    ) as this;\n  }\n}\n\nconst stabilityImageGenerationResponseSchema = z.object({\n  artifacts: z.array(\n    z.object({\n      base64: z.string(),\n      seed: z.number(),\n      finishReason: z.enum([\"SUCCESS\", \"ERROR\", \"CONTENT_FILTERED\"]),\n    })\n  ),\n});\n\nexport type StabilityImageGenerationResponse = z.infer<\n  typeof stabilityImageGenerationResponseSchema\n>;\n","import { PromptTemplate } from \"../../model-function/PromptTemplate\";\n\nexport type StabilityImageGenerationPrompt = Array<{\n  text: string;\n  weight?: number;\n}>;\n\n/**\n * Formats a basic text prompt as a Stability prompt.\n */\nexport function mapBasicPromptToStabilityFormat(): PromptTemplate<\n  string,\n  StabilityImageGenerationPrompt\n> {\n  return {\n    format: (description) => [{ text: description }],\n  };\n}\n","import {\n  BaseUrlApiConfigurationWithDefaults,\n  PartialBaseUrlPartsApiConfigurationOptions,\n} from \"../../core/api/BaseUrlApiConfiguration\";\n\n/**\n * Creates an API configuration for the Whisper.cpp server.\n * It calls the API at http://127.0.0.1:8080 by default.\n */\nexport class WhisperCppApiConfiguration extends BaseUrlApiConfigurationWithDefaults {\n  constructor(settings: PartialBaseUrlPartsApiConfigurationOptions = {}) {\n    super({\n      ...settings,\n      baseUrlDefaults: {\n        protocol: \"http\",\n        host: \"127.0.0.1\",\n        port: \"8080\",\n        path: \"\",\n      },\n    });\n  }\n}\n","import { PartialBaseUrlPartsApiConfigurationOptions } from \"../../core/api/BaseUrlApiConfiguration\";\nimport { WhisperCppApiConfiguration } from \"./WhisperCppApiConfiguration\";\nimport {\n  WhisperCppTranscriptionModel,\n  WhisperCppTranscriptionModelSettings,\n} from \"./WhisperCppTranscriptionModel\";\n\n/**\n * Creates an API configuration for the Whisper.cpp server.\n * It calls the API at http://127.0.0.1:8080 by default.\n */\nexport function Api(settings: PartialBaseUrlPartsApiConfigurationOptions) {\n  return new WhisperCppApiConfiguration(settings);\n}\n\nexport function Transcriber(\n  settings: WhisperCppTranscriptionModelSettings = {}\n) {\n  return new WhisperCppTranscriptionModel(settings);\n}\n","import { z } from \"zod\";\nimport { FunctionCallOptions } from \"../../core/FunctionOptions\";\nimport { ApiCallError } from \"../../core/api/ApiCallError\";\nimport { ApiConfiguration } from \"../../core/api/ApiConfiguration\";\nimport { callWithRetryAndThrottle } from \"../../core/api/callWithRetryAndThrottle\";\nimport { ResponseHandler, postToApi } from \"../../core/api/postToApi\";\nimport { zodSchema } from \"../../core/schema/ZodSchema\";\nimport { safeParseJSON } from \"../../core/schema/parseJSON\";\nimport { AbstractModel } from \"../../model-function/AbstractModel\";\nimport {\n  TranscriptionModel,\n  TranscriptionModelSettings,\n} from \"../../model-function/generate-transcription/TranscriptionModel\";\nimport { getAudioFileExtension } from \"../../util/audio/getAudioFileExtension\";\nimport {\n  DataContent,\n  convertDataContentToUint8Array,\n} from \"../../util/format/DataContent\";\nimport { WhisperCppApiConfiguration } from \"./WhisperCppApiConfiguration\";\n\nexport interface WhisperCppTranscriptionModelSettings\n  extends TranscriptionModelSettings {\n  api?: ApiConfiguration;\n\n  temperature?: number;\n}\n\nexport class WhisperCppTranscriptionModel\n  extends AbstractModel<WhisperCppTranscriptionModelSettings>\n  implements TranscriptionModel<WhisperCppTranscriptionModelSettings>\n{\n  constructor(settings: WhisperCppTranscriptionModelSettings) {\n    super({ settings });\n  }\n\n  readonly provider = \"whispercpp\" as const;\n  readonly modelName = null;\n\n  async doTranscribe(\n    {\n      audioData,\n      mimeType,\n    }: {\n      audioData: DataContent;\n      mimeType: string;\n    },\n    options: FunctionCallOptions\n  ) {\n    const rawResponse = await this.callAPI(\n      {\n        fileExtension: getAudioFileExtension(mimeType),\n        audioData: convertDataContentToUint8Array(audioData),\n      },\n      options\n    );\n\n    return {\n      rawResponse,\n      transcription: rawResponse.text,\n    };\n  }\n\n  async callAPI(\n    input: {\n      fileExtension: string;\n      audioData: Uint8Array;\n    },\n    callOptions: FunctionCallOptions\n  ) {\n    const { temperature } = this.settings;\n    const api = this.settings.api ?? new WhisperCppApiConfiguration();\n    const abortSignal = callOptions.run?.abortSignal;\n\n    return callWithRetryAndThrottle({\n      retry: api.retry,\n      throttle: api.throttle,\n      call: async () => {\n        const formData = new FormData();\n        formData.append(\n          \"file\",\n          new Blob([input.audioData]),\n          `audio.${input.fileExtension}`\n        );\n        formData.append(\"response_format\", \"json\");\n\n        if (temperature != null) {\n          formData.append(\"temperature\", temperature.toString());\n        }\n\n        return postToApi({\n          url: api.assembleUrl(\"/inference\"),\n          headers: api.headers({\n            functionType: callOptions.functionType,\n            functionId: callOptions.functionId,\n            run: callOptions.run,\n            callId: callOptions.callId,\n          }),\n          body: {\n            content: formData,\n            values: { temperature },\n          },\n          failedResponseHandler,\n          successfulResponseHandler,\n          abortSignal,\n        });\n      },\n    });\n  }\n\n  get settingsForEvent(): Partial<WhisperCppTranscriptionModelSettings> {\n    return {\n      temperature: this.settings.temperature,\n    };\n  }\n\n  withSettings(additionalSettings: WhisperCppTranscriptionModelSettings) {\n    return new WhisperCppTranscriptionModel(\n      Object.assign({}, this.settings, additionalSettings)\n    ) as this;\n  }\n}\n\nconst whisperCppTranscriptionJsonSchema = z.union([\n  z.object({ text: z.string() }),\n  z.object({ error: z.string() }),\n]);\n\nconst successfulResponseHandler: ResponseHandler<{\n  text: string;\n}> = async ({ response, url, requestBodyValues }) => {\n  const responseBody = await response.text();\n\n  const parsedResult = safeParseJSON({\n    text: responseBody,\n    schema: zodSchema(whisperCppTranscriptionJsonSchema),\n  });\n\n  if (!parsedResult.success) {\n    throw new ApiCallError({\n      message: \"Invalid JSON response\",\n      cause: parsedResult.error,\n      statusCode: response.status,\n      responseBody,\n      url,\n      requestBodyValues,\n    });\n  }\n\n  if (\"error\" in parsedResult.value) {\n    throw new ApiCallError({\n      message: parsedResult.value.error,\n      statusCode: response.status,\n      responseBody,\n      url,\n      requestBodyValues,\n    });\n  }\n\n  return {\n    text: parsedResult.value.text.trim(),\n  };\n};\n\nconst failedResponseHandler: ResponseHandler<ApiCallError> = async ({\n  response,\n  url,\n  requestBodyValues,\n}) => {\n  const responseBody = await response.text();\n\n  return new ApiCallError({\n    message: responseBody,\n    url,\n    requestBodyValues,\n    statusCode: response.status,\n    responseBody,\n  });\n};\n","import { BaseUrlApiConfiguration } from \"../../core/api/BaseUrlApiConfiguration\";\nimport { CustomHeaderProvider } from \"../../core/api/CustomHeaderProvider\";\nimport { RetryFunction } from \"../../core/api/RetryFunction\";\nimport { ThrottleFunction } from \"../../core/api/ThrottleFunction\";\nimport { loadApiKey } from \"../../core/api/loadApiKey\";\n\nexport class HeliconeOpenAIApiConfiguration extends BaseUrlApiConfiguration {\n  constructor({\n    baseUrl = \"https://oai.hconeai.com/v1\",\n    openAIApiKey,\n    heliconeApiKey,\n    retry,\n    throttle,\n    customCallHeaders,\n  }: {\n    baseUrl?: string;\n    openAIApiKey?: string;\n    heliconeApiKey?: string;\n    retry?: RetryFunction;\n    throttle?: ThrottleFunction;\n    customCallHeaders?: CustomHeaderProvider;\n  } = {}) {\n    super({\n      baseUrl,\n      headers: {\n        Authorization: `Bearer ${loadApiKey({\n          apiKey: openAIApiKey,\n          environmentVariableName: \"OPENAI_API_KEY\",\n          apiKeyParameterName: \"openAIApiKey\",\n          description: \"OpenAI\",\n        })}`,\n        \"Helicone-Auth\": `Bearer ${loadApiKey({\n          apiKey: heliconeApiKey,\n          environmentVariableName: \"HELICONE_API_KEY\",\n          apiKeyParameterName: \"heliconeApiKey\",\n          description: \"Helicone\",\n        })}`,\n      },\n      retry,\n      throttle,\n      customCallHeaders,\n    });\n  }\n}\n","import { executeFunctionCall } from \"../core/executeFunctionCall\";\nimport { FunctionOptions } from \"../core/FunctionOptions\";\nimport { Retriever } from \"./Retriever\";\n\nexport async function retrieve<OBJECT, QUERY>(\n  retriever: Retriever<OBJECT, QUERY>,\n  query: QUERY,\n  options?: FunctionOptions\n): Promise<OBJECT[]> {\n  return executeFunctionCall({\n    options,\n    input: query,\n    functionType: \"retrieve\",\n    execute: (options) => retriever.retrieve(query, options),\n    inputPropertyName: \"query\",\n    outputPropertyName: \"results\",\n  });\n}\n","import { SplitFunction } from \"./SplitFunction\";\n\n/**\n * Splits text on a separator string.\n */\nexport function splitOnSeparator({\n  separator,\n}: {\n  separator: string;\n}): SplitFunction {\n  return async ({ text }: { text: string }) => text.split(separator);\n}\n","import { FullTokenizer } from \"../../model-function/tokenize-text/Tokenizer\";\nimport { SplitFunction } from \"./SplitFunction\";\n\n// when segments is a string, it splits by character, otherwise according to the provided segments\nfunction splitRecursively({\n  maxChunkSize,\n  segments,\n}: {\n  maxChunkSize: number;\n  segments: string | Array<string>;\n}): Array<string> {\n  if (segments.length < maxChunkSize) {\n    return Array.isArray(segments) ? [segments.join(\"\")] : [segments];\n  }\n\n  const half = Math.ceil(segments.length / 2);\n  const left = segments.slice(0, half);\n  const right = segments.slice(half);\n\n  return [\n    ...splitRecursively({\n      segments: left,\n      maxChunkSize,\n    }),\n    ...splitRecursively({\n      segments: right,\n      maxChunkSize,\n    }),\n  ];\n}\n\n/**\n * Splits text recursively until the resulting chunks are smaller than the `maxCharactersPerChunk`.\n * The text is recursively split in the middle, so that all chunks are roughtly the same size.\n */\nexport const splitAtCharacter =\n  ({\n    maxCharactersPerChunk,\n  }: {\n    maxCharactersPerChunk: number;\n  }): SplitFunction =>\n  async ({ text }: { text: string }) =>\n    splitRecursively({\n      maxChunkSize: maxCharactersPerChunk,\n      segments: text,\n    });\n\n/**\n * Splits text recursively until the resulting chunks are smaller than the `maxTokensPerChunk`,\n * while respecting the token boundaries.\n * The text is recursively split in the middle, so that all chunks are roughtly the same size.\n */\nexport const splitAtToken =\n  ({\n    tokenizer,\n    maxTokensPerChunk,\n  }: {\n    tokenizer: FullTokenizer;\n    maxTokensPerChunk: number;\n  }): SplitFunction =>\n  async ({ text }: { text: string }) =>\n    splitRecursively({\n      maxChunkSize: maxTokensPerChunk,\n      segments: (await tokenizer.tokenizeWithTexts(text)).tokenTexts,\n    });\n","import { TextChunk } from \"../TextChunk\";\nimport { SplitFunction } from \"./SplitFunction\";\n\nexport async function splitTextChunks<CHUNK extends TextChunk>(\n  splitFunction: SplitFunction,\n  inputs: CHUNK[]\n): Promise<CHUNK[]> {\n  const pageChunks = await Promise.all(\n    inputs.map((input) => splitTextChunk(splitFunction, input))\n  );\n  return pageChunks.flat();\n}\n\nexport async function splitTextChunk<CHUNK extends TextChunk>(\n  splitFunction: SplitFunction,\n  input: CHUNK\n): Promise<CHUNK[]> {\n  const parts = await splitFunction(input);\n  return parts.map((text) => ({\n    ...input,\n    text,\n  }));\n}\n","export class NoSuchToolDefinitionError extends Error {\n  readonly toolName: string;\n  readonly cause: unknown;\n  readonly parameters: unknown;\n\n  constructor({\n    toolName,\n    parameters,\n  }: {\n    toolName: string;\n    parameters: unknown;\n  }) {\n    super(\n      `Tool definition '${toolName}' not found. ` +\n        `Parameters: ${JSON.stringify(parameters)}.`\n    );\n\n    this.name = \"NoSuchToolDefinitionError\";\n\n    this.toolName = toolName;\n    this.parameters = parameters;\n  }\n\n  toJSON() {\n    return {\n      name: this.name,\n      message: this.message,\n      cause: this.cause,\n      stack: this.stack,\n\n      toolName: this.toolName,\n      parameter: this.parameters,\n    };\n  }\n}\n","import { FunctionCallOptions, FunctionOptions } from \"../core/FunctionOptions\";\nimport { JsonSchemaProducer } from \"../core/schema/JsonSchemaProducer\";\nimport { Schema } from \"../core/schema/Schema\";\nimport { ToolDefinition } from \"./ToolDefinition\";\n\n/**\n * A tool is a function with a name, description and defined inputs that can be used\n * by agents and chatbots.\n */\nexport class Tool<NAME extends string, PARAMETERS, RESULT>\n  implements ToolDefinition<NAME, PARAMETERS>\n{\n  /**\n   * The name of the tool.\n   * Should be understandable for language models and unique among the tools that they know.\n   */\n  readonly name: NAME;\n\n  /**\n   * A optional description of what the tool does. Will be used by the language model to decide whether to use the tool.\n   */\n  readonly description?: string;\n\n  /**\n   * The schema of the input that the tool expects. The language model will use this to generate the input.\n   * Use descriptions to make the input understandable for the language model.\n   */\n  readonly parameters: Schema<PARAMETERS> & JsonSchemaProducer;\n\n  /**\n   * An optional schema of the output that the tool produces. This will be used to validate the output.\n   */\n  readonly returnType?: Schema<RESULT>;\n\n  /**\n   * The actual execution function of the tool.\n   */\n  readonly execute: (\n    args: PARAMETERS,\n    options: FunctionCallOptions\n  ) => PromiseLike<RESULT>;\n\n  constructor({\n    name,\n    description,\n    parameters,\n    returnType,\n    execute,\n  }: {\n    name: NAME;\n    description?: string;\n    parameters: Schema<PARAMETERS> & JsonSchemaProducer;\n    returnType?: Schema<RESULT>;\n    execute(args: PARAMETERS, options?: FunctionOptions): PromiseLike<RESULT>;\n  }) {\n    this.name = name;\n    this.description = description;\n    this.parameters = parameters;\n    this.returnType = returnType;\n    this.execute = execute;\n  }\n}\n","import {\n  ObjectGenerationModel,\n  ObjectGenerationModelSettings,\n} from \"../model-function/generate-object/ObjectGenerationModel\";\nimport { Tool } from \"./Tool\";\nimport { JsonSchemaProducer } from \"../core/schema/JsonSchemaProducer\";\nimport { PromptFunction } from \"../core/PromptFunction\";\nimport { Schema } from \"../core/schema/Schema\";\nimport { generateObject } from \"../model-function/generate-object/generateObject\";\n\n/**\n * A tool that generates an object. You can configure it with a model, an input, an output schema, and a prompt.\n */\nexport class ObjectGeneratorTool<\n  NAME extends string,\n  PROMPT,\n  PARAMETERS,\n  OBJECT,\n> extends Tool<NAME, PARAMETERS, OBJECT> {\n  constructor({\n    name = \"object-generator\" as any, // eslint-disable-line @typescript-eslint/no-explicit-any\n    description,\n    model,\n    parameters,\n    objectSchema,\n    prompt,\n  }: {\n    name?: NAME;\n    description?: string;\n    model: ObjectGenerationModel<PROMPT, ObjectGenerationModelSettings>;\n    parameters: Schema<PARAMETERS> & JsonSchemaProducer;\n    objectSchema: Schema<OBJECT> & JsonSchemaProducer;\n    prompt: (input: PARAMETERS) => PromptFunction<PARAMETERS, PROMPT>;\n  }) {\n    super({\n      name,\n      description,\n      parameters,\n      execute: async (input, options) =>\n        generateObject({\n          model,\n          schema: objectSchema,\n          prompt: prompt(input),\n          ...options,\n        }),\n    });\n  }\n}\n","import { getErrorMessage } from \"../util/getErrorMessage\";\n\n/**\n * Thrown when the arguments of a tool call are invalid.\n *\n * This typically means they don't match the parameters schema that is expected the tool.\n */\nexport class ToolCallArgumentsValidationError extends Error {\n  readonly toolName: string;\n  readonly cause: unknown;\n  readonly args: unknown;\n\n  constructor({\n    toolName,\n    args,\n    cause,\n  }: {\n    toolName: string;\n    args: unknown;\n    cause: unknown;\n  }) {\n    super(\n      `Argument validation failed for tool '${toolName}'.\\n` +\n        `Arguments: ${JSON.stringify(args)}.\\n` +\n        `Error message: ${getErrorMessage(cause)}`\n    );\n\n    this.name = \"ToolCallArgumentsValidationError\";\n\n    this.toolName = toolName;\n    this.cause = cause;\n    this.args = args;\n  }\n\n  toJSON() {\n    return {\n      name: this.name,\n      message: this.message,\n      cause: this.cause,\n      stack: this.stack,\n\n      toolName: this.toolName,\n      args: this.args,\n    };\n  }\n}\n","import { getErrorMessage } from \"../util/getErrorMessage\";\nimport { ToolCall } from \"./ToolCall\";\n\nexport class ToolCallError extends Error {\n  readonly toolCall: ToolCall<string, unknown>;\n  readonly cause: unknown | undefined;\n\n  constructor({\n    cause,\n    toolCall,\n    message = getErrorMessage(cause),\n  }: {\n    toolCall: ToolCall<string, unknown>;\n    cause?: unknown;\n    message?: string;\n  }) {\n    super(`Tool call for tool '${toolCall.name}' failed: ${message}`);\n\n    this.name = \"ToolCallError\";\n\n    this.toolCall = toolCall;\n    this.cause = cause;\n  }\n\n  toJSON() {\n    return {\n      name: this.name,\n      cause: this.cause,\n      message: this.message,\n      stack: this.stack,\n\n      toolCall: this.toolCall,\n    };\n  }\n}\n","import { getErrorMessage } from \"../util/getErrorMessage\";\n\nexport class ToolCallGenerationError extends Error {\n  readonly toolName: string;\n  readonly cause: unknown;\n\n  constructor({ toolName, cause }: { toolName: string; cause: unknown }) {\n    super(\n      `Tool call generation failed for tool '${toolName}'. ` +\n        `Error message: ${getErrorMessage(cause)}`\n    );\n\n    this.name = \"ToolCallsGenerationError\";\n\n    this.toolName = toolName;\n    this.cause = cause;\n  }\n\n  toJSON() {\n    return {\n      name: this.name,\n      message: this.message,\n      cause: this.cause,\n      stack: this.stack,\n\n      toolName: this.toolName,\n    };\n  }\n}\n","import { getErrorMessage } from \"../util/getErrorMessage\";\n\nexport class ToolExecutionError extends Error {\n  readonly toolName: string;\n  readonly input: unknown;\n  readonly cause: unknown;\n\n  constructor({\n    toolName,\n    input,\n    cause,\n    message = getErrorMessage(cause),\n  }: {\n    toolName: string;\n    input: unknown;\n    cause: unknown | undefined;\n    message?: string;\n  }) {\n    super(`Error executing tool '${toolName}': ${message}`);\n\n    this.name = \"ToolExecutionError\";\n\n    this.toolName = toolName;\n    this.input = input;\n    this.cause = cause;\n  }\n\n  toJSON() {\n    return {\n      name: this.name,\n      cause: this.cause,\n      message: this.message,\n      stack: this.stack,\n\n      toolName: this.toolName,\n      input: this.input,\n    };\n  }\n}\n","import { z } from \"zod\";\nimport { FunctionOptions } from \"../core/FunctionOptions\";\nimport { zodSchema } from \"../core/schema/ZodSchema\";\nimport { Tool } from \"./Tool\";\n\nconst RETURN_TYPE_SCHEMA = zodSchema(\n  z.object({\n    results: z.array(\n      z.object({\n        title: z.string(),\n        link: z.string().url(),\n        snippet: z.string(),\n      })\n    ),\n  })\n);\n\n// expose the schemas to library consumers:\nconst createParameters = (description: string) =>\n  // same schema, but with description:\n  zodSchema(\n    z.object({\n      query: z.string().describe(description),\n    })\n  );\n\nexport type WebSearchToolInput = {\n  query: string;\n};\n\nexport type WebSearchToolOutput = {\n  results: {\n    title: string;\n    link: string;\n    snippet: string;\n  }[];\n};\n\n/**\n * A tool for searching the web.\n *\n * The input schema takes a query string.\n * ```ts\n * {\n *   query: \"How many people live in Berlin?\"\n * }\n * ```\n *\n * The output schema is an array of search results with title, link and snippet.\n * ```ts\n * {\n *  results:\n *   [\n *     {\n *       title: \"Berlin - Wikipedia\",\n *       link: \"https://en.wikipedia.org/wiki/Berlin\",\n *       snippet: \"Berlin is the capital and largest city of Germany by...\",\n *     },\n *     ...\n *   ]\n * }\n * ```\n */\nexport class WebSearchTool<NAME extends string> extends Tool<\n  NAME,\n  WebSearchToolInput,\n  WebSearchToolOutput\n> {\n  // output schema is always available\n  declare readonly returnType: typeof RETURN_TYPE_SCHEMA;\n\n  constructor({\n    name,\n    description,\n    queryDescription = \"Search query\",\n    execute,\n  }: {\n    name: NAME;\n    description?: string;\n    queryDescription?: string;\n    execute(\n      input: WebSearchToolInput,\n      options?: FunctionOptions\n    ): PromiseLike<WebSearchToolOutput>;\n  }) {\n    super({\n      name,\n      description,\n      parameters: createParameters(queryDescription),\n      returnType: RETURN_TYPE_SCHEMA,\n      execute,\n    });\n  }\n}\n","import { nanoid as createId } from \"nanoid\";\nimport { FunctionEventSource } from \"../../core/FunctionEventSource\";\nimport { FunctionOptions } from \"../../core/FunctionOptions\";\nimport {\n  getFunctionObservers,\n  getLogFormat,\n} from \"../../core/ModelFusionConfiguration\";\nimport { AbortError } from \"../../core/api/AbortError\";\nimport { getFunctionCallLogger } from \"../../core/getFunctionCallLogger\";\nimport { getRun } from \"../../core/getRun\";\nimport { startDurationMeasurement } from \"../../util/DurationMeasurement\";\nimport { runSafe } from \"../../util/runSafe\";\nimport { Tool } from \"../Tool\";\nimport { ToolExecutionError } from \"../ToolExecutionError\";\n\nexport type ExecuteToolMetadata = {\n  callId: string;\n  runId?: string;\n  sessionId?: string;\n  userId?: string;\n  functionId?: string;\n  startTimestamp: Date;\n  finishTimestamp: Date;\n  durationInMs: number;\n};\n\n/**\n * `executeTool` executes a tool with the given parameters.\n */\nexport async function executeTool<TOOL extends Tool<any, any, any>>( // eslint-disable-line @typescript-eslint/no-explicit-any\n  params: {\n    tool: TOOL;\n    args: TOOL[\"parameters\"][\"_type\"];\n    fullResponse?: false;\n  } & FunctionOptions\n): Promise<ReturnType<TOOL[\"execute\"]>>;\nexport async function executeTool<TOOL extends Tool<any, any, any>>( // eslint-disable-line @typescript-eslint/no-explicit-any\n  params: {\n    tool: TOOL;\n    args: TOOL[\"parameters\"][\"_type\"];\n    fullResponse: true;\n  } & FunctionOptions\n): Promise<{\n  output: Awaited<ReturnType<TOOL[\"execute\"]>>;\n  metadata: ExecuteToolMetadata;\n}>;\n// eslint-disable-next-line @typescript-eslint/no-explicit-any\nexport async function executeTool<TOOL extends Tool<any, any, any>>({\n  tool,\n  args,\n  fullResponse,\n  ...options\n}: {\n  tool: TOOL;\n  args: TOOL[\"parameters\"][\"_type\"];\n  fullResponse?: boolean;\n} & FunctionOptions): Promise<\n  | ReturnType<TOOL[\"execute\"]>\n  | {\n      output: Awaited<ReturnType<TOOL[\"execute\"]>>;\n      metadata: ExecuteToolMetadata;\n    }\n> {\n  const callResponse = await doExecuteTool({ tool, args, ...options });\n  return fullResponse ? callResponse : callResponse.output;\n}\n\n// eslint-disable-next-line @typescript-eslint/no-explicit-any\nasync function doExecuteTool<TOOL extends Tool<any, any, any>>({\n  tool,\n  args,\n  ...options\n}: {\n  tool: TOOL;\n  args: TOOL[\"parameters\"][\"_type\"];\n} & FunctionOptions): Promise<{\n  output: Awaited<ReturnType<TOOL[\"execute\"]>>;\n  metadata: ExecuteToolMetadata;\n}> {\n  const run = await getRun(options?.run);\n\n  const eventSource = new FunctionEventSource({\n    observers: [\n      ...getFunctionCallLogger(options?.logging ?? getLogFormat()),\n      ...getFunctionObservers(),\n      ...(run?.functionObserver != null ? [run.functionObserver] : []),\n      ...(options?.observers ?? []),\n    ],\n    errorHandler: run?.errorHandler,\n  });\n\n  const durationMeasurement = startDurationMeasurement();\n\n  const metadata = {\n    functionType: \"execute-tool\" as const,\n\n    callId: `call-${createId()}`,\n    parentCallId: options?.callId,\n    runId: run?.runId,\n    sessionId: run?.sessionId,\n    userId: run?.userId,\n    functionId: options?.functionId,\n\n    toolName: tool.name,\n    input: args,\n  };\n\n  eventSource.notify({\n    ...metadata,\n    eventType: \"started\",\n    timestamp: durationMeasurement.startDate,\n    startTimestamp: durationMeasurement.startDate,\n  });\n\n  const result = await runSafe(() =>\n    tool.execute(args, {\n      functionType: metadata.functionType,\n      callId: metadata.callId,\n      functionId: options?.functionId,\n      logging: options?.logging,\n      observers: options?.observers,\n      run,\n    })\n  );\n\n  const finishMetadata = {\n    ...metadata,\n    eventType: \"finished\" as const,\n    timestamp: new Date(),\n    startTimestamp: durationMeasurement.startDate,\n    finishTimestamp: new Date(),\n    durationInMs: durationMeasurement.durationInMs,\n  };\n\n  if (!result.ok) {\n    if (result.isAborted) {\n      eventSource.notify({\n        ...finishMetadata,\n        result: {\n          status: \"abort\",\n        },\n      });\n\n      throw new AbortError();\n    }\n\n    eventSource.notify({\n      ...finishMetadata,\n      result: {\n        status: \"error\",\n        error: result.error,\n      },\n    });\n\n    throw new ToolExecutionError({\n      toolName: tool.name,\n      input: args,\n      cause: result.error,\n      // eslint-disable-next-line @typescript-eslint/no-explicit-any\n      message: (result.error as any)?.message,\n    });\n  }\n\n  const output = result.value;\n\n  eventSource.notify({\n    ...finishMetadata,\n    result: {\n      status: \"success\",\n      value: output,\n    },\n  });\n\n  return {\n    output,\n    metadata: finishMetadata,\n  };\n}\n","import { FunctionOptions } from \"../../core/FunctionOptions\";\nimport { ModelCallMetadata } from \"../../model-function/ModelCallMetadata\";\nimport { executeStandardCall } from \"../../model-function/executeStandardCall\";\nimport { ToolCall } from \"../ToolCall\";\nimport { ToolCallArgumentsValidationError } from \"../ToolCallArgumentsValidationError\";\nimport { ToolCallGenerationError } from \"../ToolCallGenerationError\";\nimport { ToolDefinition } from \"../ToolDefinition\";\nimport {\n  ToolCallGenerationModel,\n  ToolCallGenerationModelSettings,\n} from \"./ToolCallGenerationModel\";\n\nexport async function generateToolCall<\n  PARAMETERS,\n  PROMPT,\n  NAME extends string,\n  SETTINGS extends ToolCallGenerationModelSettings,\n>(\n  params: {\n    model: ToolCallGenerationModel<PROMPT, SETTINGS>;\n    tool: ToolDefinition<NAME, PARAMETERS>;\n    prompt: PROMPT | ((tool: ToolDefinition<NAME, PARAMETERS>) => PROMPT);\n    fullResponse?: false;\n  } & FunctionOptions\n): Promise<ToolCall<NAME, PARAMETERS>>;\nexport async function generateToolCall<\n  PARAMETERS,\n  PROMPT,\n  NAME extends string,\n  SETTINGS extends ToolCallGenerationModelSettings,\n>(\n  params: {\n    model: ToolCallGenerationModel<PROMPT, SETTINGS>;\n    tool: ToolDefinition<NAME, PARAMETERS>;\n    prompt: PROMPT | ((tool: ToolDefinition<NAME, PARAMETERS>) => PROMPT);\n    fullResponse: true;\n  } & FunctionOptions\n): Promise<{\n  toolCall: ToolCall<NAME, PARAMETERS>;\n  rawResponse: unknown;\n  metadata: ModelCallMetadata;\n}>;\nexport async function generateToolCall<\n  PARAMETERS,\n  PROMPT,\n  NAME extends string,\n  SETTINGS extends ToolCallGenerationModelSettings,\n>({\n  model,\n  tool,\n  prompt,\n  fullResponse,\n  ...options\n}: {\n  model: ToolCallGenerationModel<PROMPT, SETTINGS>;\n  tool: ToolDefinition<NAME, PARAMETERS>;\n  prompt: PROMPT | ((tool: ToolDefinition<NAME, PARAMETERS>) => PROMPT);\n  fullResponse?: boolean;\n} & FunctionOptions): Promise<\n  | ToolCall<NAME, PARAMETERS>\n  | {\n      toolCall: ToolCall<NAME, PARAMETERS>;\n      rawResponse: unknown;\n      metadata: ModelCallMetadata;\n    }\n> {\n  // Note: PROMPT must not be a function.\n  const expandedPrompt =\n    typeof prompt === \"function\"\n      ? (prompt as (tool: ToolDefinition<NAME, PARAMETERS>) => PROMPT)(tool)\n      : prompt;\n\n  const callResponse = await executeStandardCall({\n    functionType: \"generate-tool-call\",\n    input: expandedPrompt,\n    model,\n    options,\n    generateResponse: async (options) => {\n      try {\n        const result = await model.doGenerateToolCall(\n          tool,\n          expandedPrompt,\n          options\n        );\n\n        const toolCall = result.toolCall;\n\n        if (toolCall === null) {\n          throw new ToolCallGenerationError({\n            toolName: tool.name,\n            cause: \"No tool call generated.\",\n          });\n        }\n\n        const parseResult = tool.parameters.validate(toolCall.args);\n\n        if (!parseResult.success) {\n          throw new ToolCallArgumentsValidationError({\n            toolName: tool.name,\n            args: toolCall.args,\n            cause: parseResult.error,\n          });\n        }\n\n        return {\n          rawResponse: result.rawResponse,\n          extractedValue: {\n            id: toolCall.id,\n            name: tool.name,\n            args: parseResult.value,\n          },\n          usage: result.usage,\n        };\n      } catch (error) {\n        if (\n          error instanceof ToolCallArgumentsValidationError ||\n          error instanceof ToolCallGenerationError\n        ) {\n          throw error;\n        }\n\n        throw new ToolCallGenerationError({\n          toolName: tool.name,\n          cause: error,\n        });\n      }\n    },\n  });\n\n  return fullResponse\n    ? {\n        toolCall: callResponse.value,\n        rawResponse: callResponse.rawResponse,\n        metadata: callResponse.metadata,\n      }\n    : callResponse.value;\n}\n","import { nanoid } from \"nanoid\";\nimport { parseJSON } from \"../../core/schema/parseJSON\";\nimport { InstructionPrompt } from \"../../model-function/generate-text/prompt-template/InstructionPrompt\";\nimport { ToolDefinition } from \"../../tool/ToolDefinition\";\nimport { ToolCallPromptTemplate } from \"./ToolCallPromptTemplate\";\n\nconst DEFAULT_TOOL_PROMPT = (tool: ToolDefinition<string, unknown>) =>\n  [\n    `You are calling the function \"${tool.name}\".`,\n    tool.description != null\n      ? `Function description: ${tool.description}`\n      : null,\n    `Function parameters JSON schema: ${JSON.stringify(\n      tool.parameters.getJsonSchema()\n    )}`,\n    ``,\n    `You MUST answer with a JSON object that matches the JSON schema above.`,\n  ]\n    .filter(Boolean)\n    .join(\"\\n\");\n\nexport const jsonToolCallPrompt = {\n  text({\n    toolPrompt,\n  }: {\n    toolPrompt?: (tool: ToolDefinition<string, unknown>) => string;\n  } = {}): ToolCallPromptTemplate<string, InstructionPrompt> {\n    return {\n      createPrompt(prompt: string, tool: ToolDefinition<string, unknown>) {\n        return {\n          system: createSystemPrompt({ tool, toolPrompt }),\n          instruction: prompt,\n        };\n      },\n      extractToolCall,\n      withJsonOutput: ({ model, schema }) => model.withJsonOutput(schema),\n    };\n  },\n\n  instruction({\n    toolPrompt,\n  }: {\n    toolPrompt?: (tool: ToolDefinition<string, unknown>) => string;\n  } = {}): ToolCallPromptTemplate<InstructionPrompt, InstructionPrompt> {\n    return {\n      createPrompt(\n        prompt: InstructionPrompt,\n        tool: ToolDefinition<string, unknown>\n      ): InstructionPrompt {\n        return {\n          system: createSystemPrompt({\n            originalSystemPrompt: prompt.system,\n            tool,\n            toolPrompt,\n          }),\n          instruction: prompt.instruction,\n        };\n      },\n      extractToolCall,\n      withJsonOutput: ({ model, schema }) => model.withJsonOutput(schema),\n    };\n  },\n};\n\nfunction createSystemPrompt({\n  originalSystemPrompt,\n  toolPrompt = DEFAULT_TOOL_PROMPT,\n  tool,\n}: {\n  originalSystemPrompt?: string;\n  toolPrompt?: (tool: ToolDefinition<string, unknown>) => string;\n  tool: ToolDefinition<string, unknown>;\n}) {\n  return [\n    originalSystemPrompt,\n    originalSystemPrompt != null ? \"\" : null,\n    toolPrompt(tool),\n  ]\n    .filter(Boolean)\n    .join(\"\\n\");\n}\n\nfunction extractToolCall(response: string) {\n  return { id: nanoid(), args: parseJSON({ text: response }) };\n}\n","import { FunctionOptions } from \"../../core/FunctionOptions\";\nimport { ModelCallMetadata } from \"../../model-function/ModelCallMetadata\";\nimport { executeStandardCall } from \"../../model-function/executeStandardCall\";\nimport { NoSuchToolDefinitionError } from \"../NoSuchToolDefinitionError\";\nimport { ToolCallArgumentsValidationError } from \"../ToolCallArgumentsValidationError\";\nimport { ToolDefinition } from \"../ToolDefinition\";\nimport {\n  ToolCallsGenerationModel,\n  ToolCallsGenerationModelSettings,\n} from \"./ToolCallsGenerationModel\";\n\n// In this file, using 'any' is required to allow for flexibility in the inputs. The actual types are\n// retrieved through lookups such as TOOL[\"name\"], such that any does not affect any client.\n/* eslint-disable @typescript-eslint/no-explicit-any */\n\n// [ { name: \"n\", parameters: Schema<PARAMETERS> } | { ... } ]\ntype ToolCallDefinitionArray<T extends ToolDefinition<any, any>[]> = T;\n\n// { n: { name: \"n\", parameters: Schema<PARAMETERS> }, ... }\ntype ToToolCallDefinitionMap<\n  T extends ToolCallDefinitionArray<ToolDefinition<any, any>[]>,\n> = {\n  [K in T[number][\"name\"]]: Extract<T[number], ToolDefinition<K, any>>;\n};\n\n// { tool: \"n\", parameters: PARAMETERS } | ...\ntype ToToolCallUnion<T> = {\n  [KEY in keyof T]: T[KEY] extends ToolDefinition<any, infer PARAMETERS>\n    ? { id: string; name: KEY; args: PARAMETERS }\n    : never;\n}[keyof T];\n\ntype ToOutputValue<\n  TOOL_CALLS extends ToolCallDefinitionArray<ToolDefinition<any, any>[]>,\n> = ToToolCallUnion<ToToolCallDefinitionMap<TOOL_CALLS>>;\n\nexport async function generateToolCalls<\n  TOOLS extends Array<ToolDefinition<any, any>>,\n  PROMPT,\n>(\n  params: {\n    model: ToolCallsGenerationModel<PROMPT, ToolCallsGenerationModelSettings>;\n    tools: TOOLS;\n    prompt: PROMPT | ((tools: TOOLS) => PROMPT);\n    fullResponse?: false;\n  } & FunctionOptions\n): Promise<{\n  text: string | null;\n  toolCalls: Array<ToOutputValue<TOOLS>> | null;\n}>;\nexport async function generateToolCalls<\n  TOOLS extends ToolDefinition<any, any>[],\n  PROMPT,\n>(\n  params: {\n    model: ToolCallsGenerationModel<PROMPT, ToolCallsGenerationModelSettings>;\n    tools: TOOLS;\n    prompt: PROMPT | ((tools: TOOLS) => PROMPT);\n    fullResponse: true;\n  } & FunctionOptions\n): Promise<{\n  value: { text: string | null; toolCalls: Array<ToOutputValue<TOOLS>> };\n  rawResponse: unknown;\n  metadata: ModelCallMetadata;\n}>;\nexport async function generateToolCalls<\n  TOOLS extends ToolDefinition<any, any>[],\n  PROMPT,\n>({\n  model,\n  tools,\n  prompt,\n  fullResponse,\n  ...options\n}: {\n  model: ToolCallsGenerationModel<PROMPT, ToolCallsGenerationModelSettings>;\n  tools: TOOLS;\n  prompt: PROMPT | ((tools: TOOLS) => PROMPT);\n  fullResponse?: boolean;\n} & FunctionOptions): Promise<\n  | { text: string | null; toolCalls: Array<ToOutputValue<TOOLS>> | null }\n  | {\n      value: {\n        text: string | null;\n        toolCalls: Array<ToOutputValue<TOOLS>> | null;\n      };\n      rawResponse: unknown;\n      metadata: ModelCallMetadata;\n    }\n> {\n  // Note: PROMPT must not be a function.\n  const expandedPrompt =\n    typeof prompt === \"function\"\n      ? (prompt as (objects: TOOLS) => PROMPT)(tools)\n      : prompt;\n\n  const callResponse = await executeStandardCall<\n    {\n      text: string | null;\n      toolCalls: Array<ToOutputValue<TOOLS>> | null;\n    },\n    ToolCallsGenerationModel<PROMPT, ToolCallsGenerationModelSettings>\n  >({\n    functionType: \"generate-tool-calls\",\n    input: expandedPrompt,\n    model,\n    options,\n    generateResponse: async (options) => {\n      const result = await model.doGenerateToolCalls(\n        tools,\n        expandedPrompt,\n        options\n      );\n\n      const { text, toolCalls: rawToolCalls } = result;\n\n      // no tool calls:\n      if (rawToolCalls == null) {\n        return {\n          rawResponse: result.rawResponse,\n          extractedValue: { text, toolCalls: null },\n          usage: result.usage,\n        };\n      }\n\n      // map tool calls:\n      const toolCalls = rawToolCalls.map((rawToolCall) => {\n        const tool = tools.find((tool) => tool.name === rawToolCall.name);\n\n        if (tool == undefined) {\n          throw new NoSuchToolDefinitionError({\n            toolName: rawToolCall.name,\n            parameters: rawToolCall.args,\n          });\n        }\n\n        const parseResult = tool.parameters.validate(rawToolCall.args);\n\n        if (!parseResult.success) {\n          throw new ToolCallArgumentsValidationError({\n            toolName: tool.name,\n            args: rawToolCall.args,\n            cause: parseResult.error,\n          });\n        }\n\n        return {\n          id: rawToolCall.id,\n          name: tool.name,\n          args: parseResult.value,\n        };\n      });\n\n      return {\n        rawResponse: result.rawResponse,\n        extractedValue: {\n          text,\n          toolCalls: toolCalls as Array<ToOutputValue<TOOLS>>,\n        },\n        usage: result.usage,\n      };\n    },\n  });\n\n  return fullResponse ? callResponse : callResponse.value;\n}\n","import { FunctionOptions } from \"../../core/FunctionOptions\";\nimport { Tool } from \"../Tool\";\nimport { ToolCall } from \"../ToolCall\";\nimport { ToolCallError } from \"../ToolCallError\";\nimport { ToolCallResult } from \"../ToolCallResult\";\nimport { ToolExecutionError } from \"../ToolExecutionError\";\nimport { executeTool } from \"./executeTool\";\n\nexport async function safeExecuteToolCall<\n  TOOL extends Tool<string, unknown, any>, // eslint-disable-line @typescript-eslint/no-explicit-any\n>(\n  tool: TOOL,\n  toolCall: ToolCall<TOOL[\"name\"], TOOL[\"parameters\"]>,\n  options?: FunctionOptions\n): Promise<\n  ToolCallResult<\n    TOOL[\"name\"],\n    TOOL[\"parameters\"],\n    Awaited<ReturnType<TOOL[\"execute\"]>>\n  >\n> {\n  try {\n    return {\n      tool: toolCall.name,\n      toolCall,\n      args: toolCall.args,\n      ok: true,\n      result: await executeTool({ tool, args: toolCall.args, ...options }),\n    };\n  } catch (error) {\n    // If the error is an AbortError, rethrow it.\n    if (error instanceof Error && error.name === \"AbortError\") {\n      throw error;\n    }\n\n    return {\n      tool: toolCall.name,\n      toolCall,\n      args: toolCall.args,\n      ok: false,\n      result: new ToolCallError({\n        toolCall,\n        cause: error instanceof ToolExecutionError ? error.cause : error,\n      }),\n    };\n  }\n}\n","import { FunctionOptions } from \"../../core/FunctionOptions\";\nimport { executeFunctionCall } from \"../../core/executeFunctionCall\";\nimport { Tool } from \"../Tool\";\nimport { ToolCallResult } from \"../ToolCallResult\";\nimport { safeExecuteToolCall } from \"../execute-tool/safeExecuteToolCall\";\nimport {\n  ToolCallGenerationModel,\n  ToolCallGenerationModelSettings,\n} from \"../generate-tool-call/ToolCallGenerationModel\";\nimport { generateToolCall } from \"../generate-tool-call/generateToolCall\";\n\n/**\n * `runTool` uses `generateToolCall` to generate parameters for a tool and\n * then executes the tool with the parameters using `executeTool`.\n *\n * @returns The result contains the name of the tool (`tool` property),\n * the parameters (`parameters` property, typed),\n * and the result of the tool execution (`result` property, typed).\n *\n * @see {@link generateToolCall}\n * @see {@link executeTool}\n */\nexport async function runTool<\n  PROMPT,\n  // Using 'any' is required to allow for flexibility in the inputs. The actual types are\n  // retrieved through lookups such as TOOL[\"name\"], such that any does not affect any client.\n  TOOL extends Tool<string, any, any>, // eslint-disable-line @typescript-eslint/no-explicit-any\n>({\n  model,\n  tool,\n  prompt,\n  ...options\n}: {\n  model: ToolCallGenerationModel<PROMPT, ToolCallGenerationModelSettings>;\n  tool: TOOL;\n  prompt: PROMPT | ((tool: TOOL) => PROMPT);\n} & FunctionOptions): Promise<\n  ToolCallResult<\n    TOOL[\"name\"],\n    TOOL[\"parameters\"],\n    Awaited<ReturnType<TOOL[\"execute\"]>>\n  >\n> {\n  // Note: PROMPT must not be a function.\n  const expandedPrompt =\n    typeof prompt === \"function\"\n      ? (prompt as (tool: TOOL) => PROMPT)(tool)\n      : prompt;\n\n  return executeFunctionCall({\n    options,\n    input: expandedPrompt,\n    functionType: \"run-tool\",\n    execute: async (options) =>\n      safeExecuteToolCall(\n        tool,\n        await generateToolCall<\n          TOOL[\"parameters\"],\n          PROMPT,\n          TOOL[\"name\"],\n          ToolCallGenerationModelSettings\n        >({ model, tool, prompt: expandedPrompt, ...options }),\n        options\n      ),\n  });\n}\n","import { FunctionOptions } from \"../../core/FunctionOptions\";\nimport { executeFunctionCall } from \"../../core/executeFunctionCall\";\nimport { Tool } from \"../Tool\";\nimport { ToolCall } from \"../ToolCall\";\nimport { ToolCallError } from \"../ToolCallError\";\nimport { ToolCallResult } from \"../ToolCallResult\";\nimport { safeExecuteToolCall } from \"../execute-tool/safeExecuteToolCall\";\nimport {\n  ToolCallsGenerationModel,\n  ToolCallsGenerationModelSettings,\n} from \"../generate-tool-calls/ToolCallsGenerationModel\";\nimport { generateToolCalls } from \"../generate-tool-calls/generateToolCalls\";\n\n// In this file, using 'any' is required to allow for flexibility in the inputs. The actual types are\n// retrieved through lookups such as TOOL[\"name\"], such that any does not affect any client.\n/* eslint-disable @typescript-eslint/no-explicit-any */\n\n// [ { name: \"n\", ... } | { ... } ]\ntype ToolArray<T extends Tool<any, any, any>[]> = T;\n\n// { n: { name: \"n\", ... }, ... }\ntype ToToolMap<T extends ToolArray<Tool<any, any, any>[]>> = {\n  [K in T[number][\"name\"]]: Extract<T[number], Tool<K, any, any>>;\n};\n\n// limit to only string keys:\ntype StringKeys<T> = Extract<keyof T, string>;\n\n// { tool: \"n\", result: ... } | { ... }\ntype ToToolCallUnion<T> = {\n  [KEY in StringKeys<T>]: T[KEY] extends Tool<\n    any,\n    infer PARAMETERS,\n    infer OUTPUT\n  >\n    ? ToolCallResult<KEY, PARAMETERS, OUTPUT>\n    : never;\n}[StringKeys<T>];\n\ntype ToOutputValue<TOOLS extends ToolArray<Tool<any, any, any>[]>> =\n  ToToolCallUnion<ToToolMap<TOOLS>>;\n\nexport async function runTools<\n  PROMPT,\n  TOOLS extends Array<Tool<any, any, any>>,\n>({\n  model,\n  tools,\n  prompt,\n  ...options\n}: {\n  model: ToolCallsGenerationModel<PROMPT, ToolCallsGenerationModelSettings>;\n  tools: TOOLS;\n  prompt: PROMPT | ((tools: TOOLS) => PROMPT);\n} & FunctionOptions): Promise<{\n  text: string | null;\n  toolResults: Array<ToOutputValue<TOOLS>> | null;\n}> {\n  // Note: PROMPT must not be a function.\n  const expandedPrompt =\n    typeof prompt === \"function\"\n      ? (prompt as (tools: TOOLS) => PROMPT)(tools)\n      : prompt;\n\n  return executeFunctionCall({\n    options,\n    input: expandedPrompt,\n    functionType: \"run-tools\",\n    execute: async (options) => {\n      const modelResponse = await generateToolCalls({\n        model,\n        tools,\n        prompt: expandedPrompt,\n        fullResponse: false,\n        ...options,\n      });\n\n      const { toolCalls, text } = modelResponse;\n\n      // no tool calls:\n      if (toolCalls == null) {\n        return { text, toolResults: null };\n      }\n\n      // execute tools in parallel:\n      const toolResults = await Promise.all(\n        toolCalls.map(async (toolCall) => {\n          const tool = tools.find((tool) => tool.name === toolCall.name);\n\n          if (tool == null) {\n            return {\n              tool: toolCall.name,\n              toolCall,\n              args: toolCall.args,\n              ok: false,\n              result: new ToolCallError({\n                message: `No tool with name '${toolCall.name}' found.`,\n                toolCall,\n              }),\n            };\n          }\n\n          return await safeExecuteToolCall(\n            tool,\n            toolCall as ToolCall<\n              (typeof tool)[\"name\"],\n              (typeof tool)[\"parameters\"]\n            >,\n            options\n          );\n        })\n      );\n\n      return {\n        text,\n        toolResults: toolResults as Array<ToOutputValue<TOOLS>>,\n      };\n    },\n  });\n}\n","const textEncoder = new TextEncoder();\n\nexport function createEventSourceStream(events: AsyncIterable<unknown>) {\n  return new ReadableStream({\n    async start(controller) {\n      try {\n        for await (const event of events) {\n          controller.enqueue(\n            textEncoder.encode(`data: ${JSON.stringify(event)}\\n\\n`)\n          );\n        }\n      } finally {\n        controller.close();\n      }\n    },\n  });\n}\n","import { FunctionOptions } from \"../core/FunctionOptions\";\nimport { embed } from \"../model-function/embed/embed\";\nimport {\n  EmbeddingModel,\n  EmbeddingModelSettings,\n} from \"../model-function/embed/EmbeddingModel\";\nimport { Retriever } from \"../retriever/Retriever\";\nimport { VectorIndex } from \"./VectorIndex\";\n\nexport interface VectorIndexRetrieverSettings<FILTER> {\n  maxResults?: number;\n  similarityThreshold?: number;\n  filter?: FILTER;\n}\n\nexport class VectorIndexRetriever<OBJECT, VALUE, INDEX, FILTER>\n  implements Retriever<OBJECT, VALUE>\n{\n  private readonly vectorIndex: VectorIndex<OBJECT, INDEX, FILTER>;\n  private readonly embeddingModel: EmbeddingModel<\n    VALUE,\n    EmbeddingModelSettings\n  >;\n  private readonly settings: VectorIndexRetrieverSettings<FILTER>;\n\n  constructor({\n    vectorIndex,\n    embeddingModel,\n    maxResults,\n    similarityThreshold,\n    filter,\n  }: {\n    vectorIndex: VectorIndex<OBJECT, INDEX, FILTER>;\n    embeddingModel: EmbeddingModel<VALUE, EmbeddingModelSettings>;\n  } & VectorIndexRetrieverSettings<FILTER>) {\n    this.vectorIndex = vectorIndex;\n    this.embeddingModel = embeddingModel;\n    this.settings = {\n      maxResults,\n      similarityThreshold,\n      filter,\n    };\n  }\n\n  async retrieve(query: VALUE, options?: FunctionOptions): Promise<OBJECT[]> {\n    const embedding = await embed({\n      model: this.embeddingModel,\n      value: query,\n      ...options,\n    });\n\n    const queryResult = await this.vectorIndex.queryByVector({\n      queryVector: embedding,\n      maxResults: this.settings.maxResults ?? 1,\n      similarityThreshold: this.settings.similarityThreshold,\n      filter: this.settings?.filter,\n    });\n\n    return queryResult.map((item) => item.data);\n  }\n\n  withSettings(\n    additionalSettings: Partial<VectorIndexRetrieverSettings<FILTER>>\n  ): this {\n    return new VectorIndexRetriever(\n      Object.assign({}, this.settings, additionalSettings, {\n        vectorIndex: this.vectorIndex,\n        embeddingModel: this.embeddingModel,\n      })\n    ) as this;\n  }\n}\n","import { z } from \"zod\";\nimport { Vector } from \"../../core/Vector\";\nimport { Schema } from \"../../core/schema/Schema\";\nimport { zodSchema } from \"../../core/schema/ZodSchema\";\nimport { parseJSON } from \"../../core/schema/parseJSON\";\nimport { cosineSimilarity } from \"../../util/cosineSimilarity\";\nimport { VectorIndex } from \"../VectorIndex\";\n\ntype Entry<DATA> = {\n  id: string;\n  vector: Vector;\n  data: DATA;\n};\n\nconst jsonDataSchema = zodSchema(\n  z.array(\n    z.object({\n      id: z.string(),\n      vector: z.array(z.number()),\n      data: z.unknown(),\n    })\n  )\n);\n\n/**\n * A very simple vector index that stores all entries in memory. Useful when you only have\n * a small number of entries and don't want to set up a real database, e.g. for conversational memory\n * that does not need to be persisted.\n */\nexport class MemoryVectorIndex<DATA>\n  implements\n    VectorIndex<DATA, MemoryVectorIndex<DATA>, (value: DATA) => boolean>\n{\n  static async deserialize<DATA>({\n    serializedData,\n    schema,\n  }: {\n    serializedData: string;\n    schema?: Schema<DATA>;\n  }) {\n    // validate the outer value:\n    const json = parseJSON({ text: serializedData, schema: jsonDataSchema });\n\n    if (schema != null) {\n      // when a schema is provided, validate all entries:\n      for (const entry of json) {\n        const validationResult = schema.validate(entry.data);\n        if (!validationResult.success) {\n          throw validationResult.error;\n        }\n      }\n    }\n\n    const vectorIndex = new MemoryVectorIndex<DATA>();\n\n    vectorIndex.upsertMany(\n      json as Array<{\n        id: string;\n        vector: Vector;\n        data: DATA;\n      }>\n    );\n\n    return vectorIndex;\n  }\n\n  private readonly entries: Map<string, Entry<DATA>> = new Map();\n\n  async upsertMany(\n    data: Array<{\n      id: string;\n      vector: Vector;\n      data: DATA;\n    }>\n  ) {\n    for (const entry of data) {\n      this.entries.set(entry.id, entry);\n    }\n  }\n\n  async queryByVector({\n    queryVector,\n    similarityThreshold,\n    maxResults,\n    filter,\n  }: {\n    queryVector: Vector;\n    maxResults: number;\n    similarityThreshold?: number;\n    filter?: (value: DATA) => boolean;\n  }): Promise<Array<{ id: string; data: DATA; similarity?: number }>> {\n    const results = [...this.entries.values()]\n      .filter((value) => filter?.(value.data) ?? true)\n      .map((entry) => ({\n        id: entry.id,\n        similarity: cosineSimilarity(entry.vector, queryVector),\n        data: entry.data,\n      }))\n      .filter(\n        (entry) =>\n          similarityThreshold == undefined ||\n          entry.similarity == undefined ||\n          entry.similarity > similarityThreshold\n      );\n\n    results.sort((a, b) => b.similarity - a.similarity);\n\n    return results.slice(0, maxResults);\n  }\n\n  serialize(): string {\n    return JSON.stringify([...this.entries.values()]);\n  }\n\n  asIndex(): MemoryVectorIndex<DATA> {\n    return this;\n  }\n}\n","import { nanoid as createId } from \"nanoid\";\nimport { FunctionOptions } from \"../core/FunctionOptions\";\nimport { executeFunctionCall } from \"../core/executeFunctionCall\";\nimport {\n  EmbeddingModel,\n  EmbeddingModelSettings,\n} from \"../model-function/embed/EmbeddingModel\";\nimport { embedMany } from \"../model-function/embed/embed\";\nimport { VectorIndex } from \"./VectorIndex\";\n\nexport async function upsertIntoVectorIndex<VALUE, OBJECT>(\n  {\n    vectorIndex,\n    embeddingModel,\n    generateId = createId,\n    objects,\n    getValueToEmbed,\n    getId,\n  }: {\n    vectorIndex: VectorIndex<OBJECT, unknown, unknown>;\n    embeddingModel: EmbeddingModel<VALUE, EmbeddingModelSettings>;\n    generateId?: () => string;\n    objects: OBJECT[];\n    getValueToEmbed: (object: OBJECT, index: number) => VALUE;\n    getId?: (object: OBJECT, index: number) => string | undefined;\n  },\n  options?: FunctionOptions\n) {\n  return executeFunctionCall({\n    options,\n    input: objects,\n    functionType: \"upsert-into-vector-index\",\n    inputPropertyName: \"objects\",\n    execute: async (options) => {\n      // many embedding models support bulk embedding, so we first embed all texts:\n      const embeddings = await embedMany({\n        model: embeddingModel,\n        values: objects.map(getValueToEmbed),\n        ...options,\n      });\n\n      await vectorIndex.upsertMany(\n        objects.map((object, i) => ({\n          id: getId?.(object, i) ?? generateId(),\n          vector: embeddings[i],\n          data: object,\n        }))\n      );\n    },\n  });\n}\n"],"mappings":";;;;;;;;;;;;;;AAAA,SAAS,UAAU,gBAAgB;;;ACI5B,IAAM,sBAAN,MAA0B;AAAA,EACtB;AAAA,EACA;AAAA,EAET,YAAY;AAAA,IACV;AAAA,IACA;AAAA,EACF,GAGG;AACD,SAAK,YAAY;AACjB,SAAK,eAAe,iBAAiB,CAAC,UAAU,QAAQ,MAAM,KAAK;AAAA,EACrE;AAAA,EAEA,OAAO,OAAsB;AAC3B,eAAW,YAAY,KAAK,WAAW;AACrC,UAAI;AACF,iBAAS,gBAAgB,KAAK;AAAA,MAChC,SAAS,OAAO;AACd,aAAK,aAAa,KAAK;AAAA,MACzB;AAAA,IACF;AAAA,EACF;AACF;;;ADnBO,IAAM,aAAN,MAAgC;AAAA,EAC5B;AAAA,EACA;AAAA,EACA;AAAA,EAEA;AAAA,EAEA;AAAA,EAEA,SAA0B,CAAC;AAAA,EAE5B;AAAA,EAER,YAAY;AAAA,IACV,QAAQ,OAAO,SAAS,CAAC;AAAA,IACzB;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,EACF,IAOI,CAAC,GAAG;AACN,SAAK,QAAQ;AACb,SAAK,YAAY;AACjB,SAAK,SAAS;AACd,SAAK,cAAc;AAEnB,SAAK,eAAe,iBAAiB,CAAC,UAAU,QAAQ,MAAM,KAAK;AAEnE,SAAK,sBAAsB,IAAI,oBAAoB;AAAA,MACjD,WAAW,aAAa,CAAC;AAAA,MACzB,cAAc,KAAK,aAAa,KAAK,IAAI;AAAA,IAC3C,CAAC;AAAA,EACH;AAAA,EAES,mBAAmB;AAAA,IAC1B,iBAAiB,CAAC,UAAyB;AACzC,WAAK,OAAO,KAAK,KAAK;AACtB,WAAK,oBAAoB,OAAO,KAAK;AAAA,IACvC;AAAA,EACF;AAAA,EAEA,0BAA0B;AACxB,WAAO,KAAK,OAAO;AAAA,MACjB,CACE,UAEA,WAAW,SACX,YAAY,SACZ,YAAY,MAAM,UAClB,MAAM,OAAO,WAAW;AAAA,IAC5B;AAAA,EACF;AACF;;;AEpEA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAGA,IAAI,kBAA6B;AACjC,IAAI,0BAA8C,CAAC;AAE5C,SAAS,qBACd,mBACM;AACN,4BAA0B;AAC5B;AAEO,SAAS,uBAA2C;AACzD,SAAO;AACT;AAEO,SAAS,aAAa,QAAyB;AACpD,oBAAkB;AACpB;AAEO,SAAS,eAA0B;AACxC,SAAO;AACT;;;ACfA,IAAM,uBAAuB,OAAO,gBAAgB;AAOpD,eAAsB,aACpB,QAIC;AACD,SAAO,iBAAiB,MAAM,IAAI,MAAM,OAAO,IAAI,EAAE,OAAO,QAAQ,OAAO;AAC7E;AAOO,SAAS,qBACd,IAI+B;AAG/B,EAAC,GAAW,oBAAoB,IAAI;AAEpC,SAAO;AACT;AAOO,SAAS,iBACd,IACqC;AAErC,QAAM,YAAa,GAAW,oBAAoB,MAAM;AACxD,QAAM,aAAa,OAAO,OAAO;AAEjC,SAAO,aAAa;AACtB;;;ACtDO,IAAM,aAAN,cAAyB,MAAM;AAAA,EACpC,YAAY,UAAU,WAAW;AAC/B,UAAM,OAAO;AAAA,EACf;AACF;;;ACJO,IAAM,eAAN,cAA2B,MAAM;AAAA,EAC7B;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EAET,YAAY;AAAA,IACV;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA,cAAc,cAAc,SACzB,eAAe,OAAO,cAAc;AAAA,IACvC;AAAA,EACF,GASG;AACD,UAAM,OAAO;AAEb,SAAK,OAAO;AAEZ,SAAK,MAAM;AACX,SAAK,oBAAoB;AACzB,SAAK,aAAa;AAClB,SAAK,eAAe;AACpB,SAAK,QAAQ;AACb,SAAK,cAAc;AACnB,SAAK,OAAO;AAAA,EACd;AAAA,EAEA,SAAS;AACP,WAAO;AAAA,MACL,MAAM,KAAK;AAAA,MACX,SAAS,KAAK;AAAA,MACd,KAAK,KAAK;AAAA,MACV,mBAAmB,KAAK;AAAA,MACxB,YAAY,KAAK;AAAA,MACjB,cAAc,KAAK;AAAA,MACnB,OAAO,KAAK;AAAA,MACZ,aAAa,KAAK;AAAA,MAClB,MAAM,KAAK;AAAA,IACb;AAAA,EACF;AACF;;;ACvDA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;;;ACGO,IAAM,aACX,MACA,OAAe,MACb,EAAE;;;ACNN,eAAsB,MAAM,WAAkC;AAC5D,SAAO,IAAI,QAAQ,CAAC,YAAY,WAAW,SAAS,SAAS,CAAC;AAChE;;;ACFO,SAAS,gBAAgB,OAA4B;AAC1D,MAAI,SAAS,MAAM;AACjB,WAAO;AAAA,EACT;AAEA,MAAI,OAAO,UAAU,UAAU;AAC7B,WAAO;AAAA,EACT;AAEA,MAAI,iBAAiB,OAAO;AAC1B,WAAO,MAAM;AAAA,EACf;AAEA,SAAO,KAAK,UAAU,KAAK;AAC7B;;;ACTO,IAAM,aAAN,cAAyB,MAAM;AAAA;AAAA,EAE3B;AAAA,EACA;AAAA,EACA;AAAA,EAET,YAAY;AAAA,IACV;AAAA,IACA;AAAA,IACA;AAAA,EACF,GAIG;AACD,UAAM,OAAO;AAEb,SAAK,OAAO;AACZ,SAAK,SAAS;AACd,SAAK,SAAS;AAGd,SAAK,YAAY,OAAO,OAAO,SAAS,CAAC;AAAA,EAC3C;AAAA,EAEA,SAAS;AACP,WAAO;AAAA,MACL,MAAM,KAAK;AAAA,MACX,SAAS,KAAK;AAAA,MACd,QAAQ,KAAK;AAAA,MACb,WAAW,KAAK;AAAA,MAChB,QAAQ,KAAK;AAAA,IACf;AAAA,EACF;AACF;;;AC7BO,IAAM,8BACX,CAAC;AAAA,EACC,WAAW;AAAA,EACX,mBAAmB;AAAA,EACnB,gBAAgB;AAClB,IAAI,CAAC,MACL,OAAe,MACb,6BAA6B,GAAG;AAAA,EAC9B;AAAA,EACA,WAAW;AAAA,EACX;AACF,CAAC;AAEL,eAAe,6BACb,GACA;AAAA,EACE;AAAA,EACA;AAAA,EACA;AACF,GACA,SAAoB,CAAC,GACJ;AACjB,MAAI;AACF,WAAO,MAAM,EAAE;AAAA,EACjB,SAAS,OAAO;AACd,UAAM,eAAe,gBAAgB,KAAK;AAC1C,UAAM,YAAY,CAAC,GAAG,QAAQ,KAAK;AACnC,UAAM,YAAY,UAAU;AAE5B,QAAI,aAAa,UAAU;AACzB,YAAM,IAAI,WAAW;AAAA,QACnB,SAAS,gBAAgB,SAAS,uBAAuB,YAAY;AAAA,QACrE,QAAQ;AAAA,QACR,QAAQ;AAAA,MACV,CAAC;AAAA,IACH;AAEA,QAAI,iBAAiB,OAAO;AAC1B,UAAI,MAAM,SAAS,cAAc;AAC/B,cAAM;AAAA,MACR;AAEA,UACE,iBAAiB,gBACjB,MAAM,eACN,YAAY,UACZ;AACA,cAAM,MAAM,SAAS;AACrB,eAAO;AAAA,UACL;AAAA,UACA,EAAE,UAAU,WAAW,gBAAgB,WAAW,cAAc;AAAA,UAChE;AAAA,QACF;AAAA,MACF;AAAA,IACF;AAEA,UAAM,IAAI,WAAW;AAAA,MACnB,SAAS,gBAAgB,SAAS,0CAA0C,YAAY;AAAA,MACxF,QAAQ;AAAA,MACR,QAAQ;AAAA,IACV,CAAC;AAAA,EACH;AACF;;;ACtEA,IAAM,0BAAN,MAA8B;AAAA,EACpB;AAAA,EACA;AAAA,EACA;AAAA,EAER,YAAY,EAAE,mBAAmB,GAAmC;AAClE,SAAK,qBAAqB;AAC1B,SAAK,kBAAkB;AACvB,SAAK,YAAY,CAAC;AAAA,EACpB;AAAA,EAEA,MAAM,IAAO,IAAsC;AACjD,WAAO,IAAI,QAAQ,CAAC,SAAS,WAAW;AACtC,YAAM,aAAa,YAAY;AAC7B,YAAI,KAAK,mBAAmB,KAAK;AAAoB;AAGrD,aAAK;AACL,cAAM,MAAM,KAAK,UAAU,QAAQ,UAAU;AAC7C,YAAI,QAAQ;AAAI,eAAK,UAAU,OAAO,KAAK,CAAC;AAE5C,YAAI;AACF,kBAAQ,MAAM,GAAG,CAAC;AAAA,QACpB,SAAS,OAAO;AACd,iBAAO,KAAK;AAAA,QACd,UAAE;AACA,eAAK;AACL,cAAI,KAAK,UAAU,SAAS,GAAG;AAC7B,iBAAK,UAAU,CAAC,EAAE;AAAA,UACpB;AAAA,QACF;AAAA,MACF;AAEA,WAAK,UAAU,KAAK,UAAU;AAE9B,UAAI,KAAK,kBAAkB,KAAK,oBAAoB;AAClD,mBAAW;AAAA,MACb;AAAA,IACF,CAAC;AAAA,EACH;AACF;AAKO,SAAS,uBAAuB;AAAA,EACrC;AACF,GAEqB;AACnB,QAAM,YAAY,IAAI,wBAAwB,EAAE,mBAAmB,CAAC;AACpE,SAAO,CAAI,OAA6B,UAAU,IAAI,EAAE;AAC1D;;;ACjDO,IAAM,cAAc,MAAwB,CAAC,OAAO,GAAG;;;ACAvD,IAAe,2BAAf,MAAoE;AAAA,EAChE;AAAA,EACA;AAAA,EAEU;AAAA,EAEnB,YAAY;AAAA,IACV;AAAA,IACA;AAAA,IACA,oBAAoB,OAAO,CAAC;AAAA,EAC9B,GAIG;AACD,SAAK,QAAQ;AACb,SAAK,WAAW;AAChB,SAAK,oBAAoB;AAAA,EAC3B;AAAA,EAOA,QAAQ,QAAkD;AACxD,WAAO,OAAO;AAAA,MACZ;AAAA,QACE,GAAG,OAAO,QAAQ,KAAK,aAAa,MAAM,CAAC;AAAA,QAC3C,GAAG,OAAO,QAAQ,KAAK,kBAAkB,MAAM,CAAC;AAAA,MAClD,EAAE;AAAA;AAAA,QAEA,CAAC,UAAqC,OAAO,MAAM,CAAC,MAAM;AAAA,MAC5D;AAAA,IACF;AAAA,EACF;AACF;;;AChBO,IAAM,0BAAN,cAAsC,yBAAyB;AAAA,EAC3D;AAAA,EAEU;AAAA,EAEnB,YAAY;AAAA,IACV;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,EACF,GAAwC;AACtC,UAAM,EAAE,OAAO,UAAU,kBAAkB,CAAC;AAC5C,SAAK,UAAU,OAAO,WAAW,WAAW,aAAa,OAAO,IAAI;AACpE,SAAK,oBAAoB,WAAW,CAAC;AAAA,EACvC;AAAA,EAEA,eAAe;AACb,WAAO,KAAK;AAAA,EACd;AAAA,EAEA,YAAY,MAAsB;AAChC,QAAI,WAAW,KAAK,QAAQ;AAG5B,QAAI,SAAS,SAAS,GAAG,GAAG;AAC1B,iBAAW,SAAS,MAAM,GAAG,EAAE;AAAA,IACjC;AAGA,QAAI,CAAC,KAAK,WAAW,GAAG,GAAG;AACzB,aAAO,IAAI,IAAI;AAAA,IACjB;AAEA,WAAO,GAAG,KAAK,QAAQ,QAAQ,MAAM,KAAK,QAAQ,IAAI,IAAI,KAAK,QAAQ,IAAI,GAAG,QAAQ,GAAG,IAAI;AAAA,EAC/F;AACF;AASO,IAAM,sCAAN,cAAkD,wBAAwB;AAAA,EAC/E,YAAY;AAAA,IACV;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,EACF,GAEgD;AAC9C,UAAM;AAAA,MACJ,SAAS,eAAe,SAAS,eAAe;AAAA,MAChD;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,IACF,CAAC;AAAA,EACH;AACF;AAEA,SAAS,aAAa,SAA2B;AAC/C,QAAM,MAAM,IAAI,IAAI,OAAO;AAE3B,SAAO;AAAA,IACL,UAAU,IAAI,SAAS,MAAM,GAAG,EAAE;AAAA;AAAA,IAClC,MAAM,IAAI;AAAA,IACV,MAAM,IAAI;AAAA,IACV,MAAM,IAAI;AAAA,EACZ;AACF;AAEA,SAAS,eACP,UAAkD,CAAC,GACnD,iBACmB;AACnB,MAAI,OAAO,WAAW,UAAU;AAC9B,WAAO;AAAA,EACT,OAAO;AACL,WAAO;AAAA,MACL,UAAU,QAAQ,YAAY,gBAAgB;AAAA,MAC9C,MAAM,QAAQ,QAAQ,gBAAgB;AAAA,MACtC,MAAM,QAAQ,QAAQ,gBAAgB;AAAA,MACtC,MAAM,QAAQ,QAAQ,gBAAgB;AAAA,IACxC;AAAA,EACF;AACF;;;AClHO,IAAM,cAAN,MAAmC;AAAA,EACvB,QAAQ,oBAAI,IAAqB;AAAA,EAE1C,QAAQ,KAIb;AACD,WAAO,KAAK,UAAU,GAAG;AAAA,EAC3B;AAAA,EAEA,MAAM,YAAY,KAIS;AACzB,WAAO,KAAK,MAAM,IAAI,KAAK,QAAQ,GAAG,CAAC,KAAK;AAAA,EAC9C;AAAA,EAEA,MAAM,WACJ,KAKA,OACe;AACf,SAAK,MAAM,IAAI,KAAK,QAAQ,GAAG,GAAG,KAAK;AAAA,EACzC;AACF;;;AC/BA,SAAS,UAAUA,iBAAgB;;;ACI5B,SAAS,sBACd,SACyB;AACzB,UAAQ,SAAS;AAAA,IACf,KAAK;AACH,aAAO,CAAC,iBAAiB;AAAA,IAE3B,KAAK;AACH,aAAO,CAAC,sBAAsB;AAAA,IAEhC,KAAK;AACH,aAAO,CAAC,oBAAoB;AAAA,IAE9B,KAAK;AAAA,IACL;AACE,aAAO,CAAC;AAAA,EACZ;AACF;AAEA,IAAM,oBAAsC;AAAA,EAC1C,gBAAgB,OAAsB;AACpC,UAAMC,SAAO,IAAI,MAAM,UAAU,YAAY,CAAC,KAAK,MAAM,MAAM,GAC7D,MAAM,cAAc,OAAO,KAAK,MAAM,UAAU,MAAM,EACxD,MAAM,MAAM,YAAY,IAAI,MAAM,SAAS;AAG3C,YAAQ,MAAM,WAAW;AAAA,MACvB,KAAK,WAAW;AACd,gBAAQ,IAAIA,MAAI;AAChB;AAAA,MACF;AAAA,MACA,KAAK,YAAY;AACf,gBAAQ,IAAI,GAAGA,MAAI,OAAO,MAAM,YAAY,IAAI;AAChD;AAAA,MACF;AAAA,IACF;AAAA,EACF;AACF;AAEA,IAAM,yBAA2C;AAAA,EAC/C,gBAAgB,OAAsB;AAEpC,QACE,MAAM,cAAc,cACpB,MAAM,UAAU,QAChB,iBAAiB,MAAM,UACvB,MAAM,QAAQ,eAAe,MAC7B;AACA,cAAQ;AAAA,QACN,GAAG;AAAA,QACH,QAAQ,OAAO;AAAA,UACb,OAAO,QAAQ,MAAM,MAAM,EAAE,OAAO,CAAC,CAAC,CAAC,MAAM,MAAM,aAAa;AAAA;AAAA,QAElE;AAAA,MACF;AAAA,IACF;AAIA,aAAS,YAAY,KAAe;AAClC,UAAI,eAAe,QAAQ,OAAO,QAAQ,UAAU;AAClD,eAAO;AAAA,MACT;AAEA,UAAI,MAAM,QAAQ,GAAG,GAAG;AACtB,eAAO,IAAI,IAAI,CAAC,SAAS,YAAY,IAAI,CAAC;AAAA,MAC5C;AAEA,UAAI,QAAQ,QAAQ,OAAO,QAAQ,UAAU;AAC3C,eAAO,OAAO;AAAA,UACZ,OAAO,QAAQ,GAAG,EAEf,IAAI,CAAC,CAAC,GAAG,CAAC,MAAM;AACf,gBAAI,MAAM,QAAW;AACnB,qBAAO,CAAC,GAAG,MAAS;AAAA,YACtB,WAAW,aAAa,YAAY;AAClC,qBAAO,CAAC,GAAG,qBAAqB;AAAA,YAClC,WACE,MAAM,QAAQ,CAAC,KACf,EAAE,SAAS,MACX,EAAE,MAAM,CAACC,OAAM,OAAOA,OAAM,QAAQ,GACpC;AACA,qBAAO,CAAC,GAAG,wBAAwB;AAAA,YACrC,OAAO;AACL,qBAAO,CAAC,GAAG,YAAY,CAAC,CAAC;AAAA,YAC3B;AAAA,UACF,CAAC,EAEA,OAAO,CAAC,CAAC,GAAG,CAAC,MAAM,MAAM,MAAS;AAAA,QACvC;AAAA,MACF;AACA,aAAO;AAAA,IACT;AAGA,UAAM,eAAe,YAAY,KAAK;AAEtC,YAAQ,IAAI,YAAY;AAAA,EAC1B;AACF;AAEA,IAAM,uBAAyC;AAAA,EAC7C,gBAAgB,OAAsB;AAEpC,QACE,MAAM,cAAc,cACpB,MAAM,UAAU,QAChB,iBAAiB,MAAM,UACvB,MAAM,QAAQ,eAAe,MAC7B;AACA,cAAQ;AAAA,QACN,GAAG;AAAA,QACH,QAAQ,OAAO;AAAA,UACb,OAAO,QAAQ,MAAM,MAAM,EAAE,OAAO,CAAC,CAAC,CAAC,MAAM,MAAM,aAAa;AAAA;AAAA,QAElE;AAAA,MACF;AAAA,IACF;AAGA,YAAQ,OAAO;AAAA;AAAA,MAEb,OAAO,QAAQ,KAAK,EAAE,OAAO,CAAC,CAAC,GAAG,CAAC,MAAM,MAAM,MAAS;AAAA,IAC1D;AAEA,YAAQ,IAAI,KAAK,UAAU,KAAK,CAAC;AAAA,EACnC;AACF;;;ACnIO,SAAS,gBAAgB;AAE9B,QAAM,gBAAgB;AAEtB,MAAI,cAAc,aAAa;AAC7B,WAAO;AAAA,EACT;AAEA,MAAI,WAAW,WAAW,cAAc,sBAAsB;AAC5D,WAAO;AAAA,EACT;AAEA,MAAI,WAAW,SAAS,SAAS,SAAS,QAAQ;AAChD,WAAO;AAAA,EACT;AAEA,MAAI,WAAW,QAAQ;AACrB,WAAO;AAAA,EACT;AAEA,SAAO;AACT;;;ACbA,IAAI;AAEJ,eAAe,eAAe;AAC5B,MAAI,cAAc,MAAM,UAAU,CAAC,YAAY;AAK7C,QAAI;AAEJ,QAAI;AACF,2BAAqB,MAAM,OAAO,aAAa,GAAG;AAAA,IACpD,SAAS,OAAO;AACd,UAAI;AAEF,4BAAoB,UAAQ,aAAa,EAAE;AAAA,MAC7C,SAASC,QAAO;AACd,cAAM,IAAI,MAAM,kDAAkD;AAAA,MACpE;AAAA,IACF;AAEA,iBAAa,IAAI,kBAAkB;AAAA,EACrC;AAEA,SAAO,QAAQ,QAAQ;AACzB;AAKA,eAAsB,OAAO,KAAqC;AAChE,QAAM,aAAa;AACnB,SAAO,OAAO,YAAY,SAAS;AACrC;AAKA,eAAsB,QACpB,KACA,UACA;AACA,QAAM,aAAa;AACnB,MAAI,cAAc,MAAM;AACtB,UAAM,WAAW,IAAI,KAAK,MAAM,SAAS,GAAG,CAAC;AAAA,EAC/C,OAAO;AACL,UAAM,SAAS,GAAG;AAAA,EACpB;AACF;;;ACxDO,SAAS,2BAAgD;AAE9D,SAAO,WAAW,eAAe,OAC7B,IAAI,kCAAkC,IACtC,IAAI,wBAAwB;AAClC;AAQA,IAAM,oCAAN,MAAuE;AAAA,EACpD,YAAY,WAAW,YAAY,IAAI;AAAA,EAExD,IAAI,oBAAoB;AACtB,WAAO,KAAK;AAAA,OACT,WAAW,YAAY,aAAa,KAAK,aAAa;AAAA,IACzD;AAAA,EACF;AAAA,EAEA,IAAI,YAAY;AACd,WAAO,IAAI,KAAK,KAAK,oBAAoB,GAAI;AAAA,EAC/C;AAAA,EAEA,IAAI,eAAe;AACjB,WAAO,KAAK,KAAK,WAAW,YAAY,IAAI,IAAI,KAAK,SAAS;AAAA,EAChE;AACF;AAEA,IAAM,0BAAN,MAA6D;AAAA,EAC1C,YAAY,KAAK,IAAI;AAAA,EAEtC,IAAI,oBAAoB;AACtB,WAAO,KAAK,MAAM,KAAK,YAAY,GAAI;AAAA,EACzC;AAAA,EAEA,IAAI,YAAY;AACd,WAAO,IAAI,KAAK,KAAK,SAAS;AAAA,EAChC;AAAA,EAEA,IAAI,eAAe;AACjB,WAAO,KAAK,IAAI,IAAI,KAAK;AAAA,EAC3B;AACF;;;AC3CO,IAAM,UAAU,OACrB,MACgC;AAChC,MAAI;AACF,WAAO,EAAE,IAAI,MAAM,OAAO,MAAM,EAAE,EAAE;AAAA,EACtC,SAAS,OAAO;AACd,QAAI,iBAAiB,SAAS,MAAM,SAAS,cAAc;AACzD,aAAO,EAAE,IAAI,OAAO,WAAW,KAAK;AAAA,IACtC;AAEA,WAAO,EAAE,IAAI,OAAO,MAAM;AAAA,EAC5B;AACF;;;ALFA,eAAsB,oBAA2B;AAAA,EAC/C;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA,oBAAoB;AAAA,EACpB,qBAAqB;AACvB,GAOmB;AACjB,QAAM,MAAM,MAAM,OAAO,SAAS,GAAG;AAErC,QAAM,cAAc,IAAI,oBAAoB;AAAA,IAC1C,WAAW;AAAA,MACT,GAAG,sBAAsB,SAAS,WAAW,aAAa,CAAC;AAAA,MAC3D,GAAG,qBAAqB;AAAA,MACxB,GAAI,KAAK,oBAAoB,OAAO,CAAC,IAAI,gBAAgB,IAAI,CAAC;AAAA,MAC9D,GAAI,SAAS,aAAa,CAAC;AAAA,IAC7B;AAAA,IACA,cAAc,KAAK;AAAA,EACrB,CAAC;AAED,QAAM,sBAAsB,yBAAyB;AAErD,QAAM,gBAAgB;AAAA,IACpB;AAAA,IAEA,QAAQ,QAAQC,UAAS,CAAC;AAAA,IAC1B,cAAc,SAAS;AAAA,IACvB,OAAO,KAAK;AAAA,IACZ,WAAW,KAAK;AAAA,IAChB,QAAQ,KAAK;AAAA,IACb,YAAY,SAAS;AAAA,IAErB,CAAC,iBAAiB,GAAG;AAAA,IAErB,WAAW,oBAAoB;AAAA,IAC/B,gBAAgB,oBAAoB;AAAA,EACtC;AAEA,cAAY,OAAO;AAAA,IACjB,WAAW;AAAA,IACX,GAAG;AAAA,EACL,CAAkB;AAElB,QAAM,SAAS,MAAM;AAAA,IAAQ,MAC3B,QAAQ;AAAA,MACN;AAAA,MACA,YAAY,SAAS;AAAA,MACrB,QAAQ,cAAc;AAAA,MACtB,SAAS,SAAS;AAAA,MAClB,WAAW,SAAS;AAAA,MACpB;AAAA,IACF,CAAC;AAAA,EACH;AAEA,QAAM,iBAAiB;AAAA,IACrB,WAAW;AAAA,IACX,GAAG;AAAA,IACH,iBAAiB,oBAAI,KAAK;AAAA,IAC1B,cAAc,oBAAoB;AAAA,EACpC;AAEA,MAAI,CAAC,OAAO,IAAI;AACd,QAAI,OAAO,WAAW;AACpB,kBAAY,OAAO;AAAA,QACjB,GAAG;AAAA,QACH,WAAW;AAAA,QACX,QAAQ;AAAA,UACN,QAAQ;AAAA,QACV;AAAA,MACF,CAAkB;AAClB,YAAM,IAAI,WAAW;AAAA,IACvB;AAEA,gBAAY,OAAO;AAAA,MACjB,GAAG;AAAA,MACH,WAAW;AAAA,MACX,QAAQ;AAAA,QACN,QAAQ;AAAA,QACR,OAAO,OAAO;AAAA,MAChB;AAAA,IACF,CAAkB;AAElB,UAAM,OAAO;AAAA,EACf;AAEA,cAAY,OAAO;AAAA,IACjB,GAAG;AAAA,IACH,WAAW;AAAA,IACX,QAAQ;AAAA,MACN,QAAQ;AAAA,MACR,CAAC,kBAAkB,GAAG,OAAO;AAAA,IAC/B;AAAA,EACF,CAAkB;AAElB,SAAO,OAAO;AAChB;;;AM/GA,eAAsB,gBACpB,IACA,OACA,SACiB;AACjB,SAAO,oBAAoB;AAAA,IACzB;AAAA,IACA;AAAA,IACA,cAAc;AAAA,IACd,SAAS,OAAOC,aAAY,GAAG,OAAOA,QAAO;AAAA,EAC/C,CAAC;AACH;;;ACZO,IAAM,iBAAN,cAA6B,MAAM;AAAA;AAAA,EAE/B;AAAA,EACA;AAAA,EAET,YAAY,EAAE,MAAAC,QAAM,MAAM,GAAqC;AAC7D;AAAA,MACE,8BACWA,MAAI;AAAA,iBACK,gBAAgB,KAAK,CAAC;AAAA,IAC5C;AAEA,SAAK,OAAO;AAEZ,SAAK,QAAQ;AACb,SAAK,OAAOA;AAAA,EACd;AAAA,EAEA,SAAS;AACP,WAAO;AAAA,MACL,MAAM,KAAK;AAAA,MACX,SAAS,KAAK;AAAA,MACd,OAAO,KAAK;AAAA,MACZ,OAAO,KAAK;AAAA,MAEZ,WAAW,KAAK;AAAA,IAClB;AAAA,EACF;AACF;;;AC5BO,IAAM,sBAAN,cAAkC,MAAM;AAAA,EACpC;AAAA,EACA;AAAA,EAET,YAAY,EAAE,OAAO,MAAM,GAAuC;AAChE;AAAA,MACE,kCACY,KAAK,UAAU,KAAK,CAAC;AAAA,iBACb,gBAAgB,KAAK,CAAC;AAAA,IAC5C;AAEA,SAAK,OAAO;AAEZ,SAAK,QAAQ;AACb,SAAK,QAAQ;AAAA,EACf;AAAA,EAEA,SAAS;AACP,WAAO;AAAA,MACL,MAAM,KAAK;AAAA,MACX,SAAS,KAAK;AAAA,MACd,OAAO,KAAK;AAAA,MACZ,OAAO,KAAK;AAAA,MAEZ,OAAO,KAAK;AAAA,IACd;AAAA,EACF;AACF;;;AC1BO,SAAS,gBAAwB,YAAsB;AAC5D,SAAO,IAAI,gBAAwB,UAAU;AAC/C;AAEO,IAAM,kBAAN,MAEP;AAAA,EACE,YAA6B,YAAsB;AAAtB;AAAA,EAAuB;AAAA,EAEpD,SACE,OACuE;AACvE,WAAO,EAAE,SAAS,MAAM,MAAuB;AAAA,EACjD;AAAA,EAEA,gBAAyB;AACvB,WAAO,KAAK;AAAA,EACd;AAAA,EAES;AACX;;;ACrBA,SAAS,uBAAuB;AAIzB,SAAS,UAAkBC,YAA6B;AAC7D,SAAO,IAAI,UAAUA,UAAS;AAChC;AAEO,IAAM,YAAN,MAAsE;AAAA,EAClE;AAAA,EAET,YAAYA,YAA6B;AACvC,SAAK,YAAYA;AAAA,EACnB;AAAA,EAEA,SACE,OACuE;AACvE,UAAM,SAAS,KAAK,UAAU,UAAU,KAAK;AAE7C,WAAO,OAAO,UACV,EAAE,SAAS,MAAM,OAAO,OAAO,KAAK,IACpC,EAAE,SAAS,OAAO,OAAO,OAAO,MAAM;AAAA,EAC5C;AAAA,EAEA,gBAAyB;AACvB,WAAO,gBAAgB,KAAK,SAAS;AAAA,EACvC;AAAA;AAAA;AAAA;AAAA,EAKS;AAAA;AAAA;AAAA;AAAA,EAKA;AACX;;;ACxCA,OAAO,gBAAgB;;;ACYhB,SAAS,cAAiB;AAAA,EAC/B;AAAA,EACA;AACF,GAGM;AACJ,MAAI;AACF,UAAM,mBAAmB,OAAO,SAAS,KAAK;AAE9C,QAAI,CAAC,iBAAiB,SAAS;AAC7B,YAAM,IAAI,oBAAoB;AAAA,QAC5B;AAAA,QACA,OAAO,iBAAiB;AAAA,MAC1B,CAAC;AAAA,IACH;AAEA,WAAO,iBAAiB;AAAA,EAC1B,SAAS,OAAO;AACd,QAAI,iBAAiB,qBAAqB;AACxC,YAAM;AAAA,IACR;AAEA,UAAM,IAAI,oBAAoB,EAAE,OAAO,OAAO,MAAM,CAAC;AAAA,EACvD;AACF;AAWO,SAAS,kBAAqB;AAAA,EACnC;AAAA,EACA;AACF,GAKmD;AACjD,MAAI;AACF,UAAM,mBAAmB,OAAO,SAAS,KAAK;AAE9C,QAAI,iBAAiB,SAAS;AAC5B,aAAO;AAAA,IACT;AAEA,WAAO;AAAA,MACL,SAAS;AAAA,MACT,OAAO,IAAI,oBAAoB;AAAA,QAC7B;AAAA,QACA,OAAO,iBAAiB;AAAA,MAC1B,CAAC;AAAA,IACH;AAAA,EACF,SAAS,OAAO;AACd,WAAO;AAAA,MACL,SAAS;AAAA,MACT,OACE,iBAAiB,sBACb,QACA,IAAI,oBAAoB,EAAE,OAAO,OAAO,MAAM,CAAC;AAAA,IACvD;AAAA,EACF;AACF;;;ADpDO,SAAS,UAAa;AAAA,EAC3B,MAAAC;AAAA,EACA;AACF,GAGM;AACJ,MAAI;AACF,UAAM,QAAQ,WAAW,MAAMA,MAAI;AAEnC,QAAI,UAAU,MAAM;AAClB,aAAO;AAAA,IACT;AAEA,WAAO,cAAc,EAAE,OAAO,OAAO,CAAC;AAAA,EACxC,SAAS,OAAO;AACd,QACE,iBAAiB,kBACjB,iBAAiB,qBACjB;AACA,YAAM;AAAA,IACR;AAEA,UAAM,IAAI,eAAe,EAAE,MAAAA,QAAM,OAAO,MAAM,CAAC;AAAA,EACjD;AACF;AAgCO,SAAS,cAAiB;AAAA,EAC/B,MAAAA;AAAA,EACA;AACF,GAKoE;AAClE,MAAI;AACF,UAAM,QAAQ,WAAW,MAAMA,MAAI;AAEnC,QAAI,UAAU,MAAM;AAClB,aAAO;AAAA,QACL,SAAS;AAAA,QACT;AAAA,MACF;AAAA,IACF;AAEA,WAAO,kBAAkB,EAAE,OAAO,OAAO,CAAC;AAAA,EAC5C,SAAS,OAAO;AACd,WAAO;AAAA,MACL,SAAS;AAAA,MACT,OACE,iBAAiB,iBACb,QACA,IAAI,eAAe,EAAE,MAAAA,QAAM,OAAO,MAAM,CAAC;AAAA,IACjD;AAAA,EACF;AACF;;;AExGO,SAAS,iBAAiB,GAAa,GAAa;AACzD,MAAI,EAAE,WAAW,EAAE,QAAQ;AACzB,UAAM,IAAI;AAAA,MACR,yCAAyC,EAAE,MAAM,QAAQ,EAAE,MAAM;AAAA,IACnE;AAAA,EACF;AAEA,SAAO,WAAW,GAAG,CAAC,KAAK,UAAU,CAAC,IAAI,UAAU,CAAC;AACvD;AAEA,SAAS,WAAW,GAAa,GAAa;AAC5C,SAAO,EAAE;AAAA,IACP,CAAC,KAAa,KAAa,MAAc,MAAM,MAAM,EAAE,CAAC;AAAA,IACxD;AAAA,EACF;AACF;AAEA,SAAS,UAAU,GAAa;AAC9B,SAAO,KAAK,KAAK,WAAW,GAAG,CAAC,CAAC;AACnC;;;AC7BA,SAAS,UAAUC,iBAAgB;AAiBnC,eAAsB,oBAGpB;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AACF,GAcG;AACD,QAAM,MAAM,MAAM,OAAO,SAAS,GAAG;AACrC,QAAM,WAAW,MAAM;AAEvB,QAAM,cAAc,IAAI,oBAAoB;AAAA,IAC1C,WAAW;AAAA,MACT,GAAG,sBAAsB,SAAS,WAAW,aAAa,CAAC;AAAA,MAC3D,GAAG,qBAAqB;AAAA,MACxB,GAAI,SAAS,aAAa,CAAC;AAAA,MAC3B,GAAI,KAAK,oBAAoB,OAAO,CAAC,IAAI,gBAAgB,IAAI,CAAC;AAAA,MAC9D,GAAI,SAAS,aAAa,CAAC;AAAA,IAC7B;AAAA,IACA,cAAc,KAAK;AAAA,EACrB,CAAC;AAED,QAAM,sBAAsB,yBAAyB;AAErD,QAAM,gBAAgB;AAAA,IACpB;AAAA,IAEA,QAAQ,QAAQC,UAAS,CAAC;AAAA,IAC1B,cAAc,SAAS;AAAA,IACvB,OAAO,KAAK;AAAA,IACZ,WAAW,KAAK;AAAA,IAChB,QAAQ,KAAK;AAAA,IACb,YAAY,SAAS;AAAA,IAErB,OAAO,MAAM;AAAA,IACb,UAAU,MAAM;AAAA,IAChB;AAAA,IAEA,WAAW,oBAAoB;AAAA,IAC/B,gBAAgB,oBAAoB;AAAA,EACtC;AAEA,cAAY,OAAO;AAAA,IACjB,WAAW;AAAA,IACX,GAAG;AAAA,EACL,CAA0B;AAE1B,QAAM,SAAS,MAAM;AAAA,IAAQ,MAC3B,iBAAiB;AAAA,MACf;AAAA,MACA,YAAY,SAAS;AAAA,MACrB,QAAQ,cAAc;AAAA,MACtB,SAAS,SAAS;AAAA,MAClB,WAAW,SAAS;AAAA,MACpB,OAAO,SAAS;AAAA,MAChB;AAAA,IACF,CAAC;AAAA,EACH;AAEA,QAAM,iBAAiB;AAAA,IACrB,WAAW;AAAA,IACX,GAAG;AAAA,IACH,iBAAiB,oBAAI,KAAK;AAAA,IAC1B,cAAc,oBAAoB;AAAA,EACpC;AAEA,MAAI,CAAC,OAAO,IAAI;AACd,QAAI,OAAO,WAAW;AACpB,kBAAY,OAAO;AAAA,QACjB,GAAG;AAAA,QACH,WAAW;AAAA,QACX,QAAQ;AAAA,UACN,QAAQ;AAAA,QACV;AAAA,MACF,CAA2B;AAC3B,YAAM,IAAI,WAAW;AAAA,IACvB;AAEA,gBAAY,OAAO;AAAA,MACjB,GAAG;AAAA,MACH,WAAW;AAAA,MACX,QAAQ;AAAA,QACN,QAAQ;AAAA,QACR,OAAO,OAAO;AAAA,MAChB;AAAA,IACF,CAA2B;AAE3B,UAAM,OAAO;AAAA,EACf;AAEA,QAAM,cAAc,OAAO,MAAM;AACjC,QAAM,QAAQ,OAAO,MAAM;AAC3B,QAAM,QAAQ,OAAO,MAAM;AAE3B,cAAY,OAAO;AAAA,IACjB,GAAG;AAAA,IACH,WAAW;AAAA,IACX,QAAQ;AAAA,MACN,QAAQ;AAAA,MACR;AAAA,MACA;AAAA,MACA;AAAA,IACF;AAAA,EACF,CAA2B;AAE3B,SAAO;AAAA,IACL;AAAA,IACA;AAAA,IACA,UAAU;AAAA,MACR,OAAO,MAAM;AAAA,MAEb,QAAQ,eAAe;AAAA,MACvB,OAAO,eAAe;AAAA,MACtB,WAAW,eAAe;AAAA,MAC1B,QAAQ,eAAe;AAAA,MACvB,YAAY,eAAe;AAAA,MAE3B,gBAAgB,cAAc;AAAA,MAC9B,iBAAiB,eAAe;AAAA,MAChC,cAAc,eAAe;AAAA,MAE7B;AAAA,IACF;AAAA,EACF;AACF;;;AClHA,eAAsB,UAAiB;AAAA,EACrC;AAAA,EACA;AAAA,EACA;AAAA,EACA,GAAG;AACL,GAWE;AACA,QAAM,eAAe,MAAM,oBAAoB;AAAA,IAC7C,cAAc;AAAA,IACd,OAAO;AAAA,IACP;AAAA,IACA;AAAA,IACA,kBAAkB,OAAOC,aAAY;AAEnC,YAAM,mBAAmB,MAAM;AAC/B,YAAM,cAAyB,CAAC;AAEhC,UAAI,oBAAoB,MAAM;AAC5B,oBAAY,KAAK,MAAM;AAAA,MACzB,OAAO;AACL,iBAAS,IAAI,GAAG,IAAI,OAAO,QAAQ,KAAK,kBAAkB;AACxD,sBAAY,KAAK,OAAO,MAAM,GAAG,IAAI,gBAAgB,CAAC;AAAA,QACxD;AAAA,MACF;AAGA,UAAI;AACJ,UAAI,MAAM,kBAAkB;AAC1B,oBAAY,MAAM,QAAQ;AAAA,UACxB,YAAY;AAAA,YAAI,CAAC,eACf,MAAM,cAAc,YAAYA,QAAO;AAAA,UACzC;AAAA,QACF;AAAA,MACF,OAAO;AACL,oBAAY,CAAC;AACb,mBAAW,cAAc,aAAa;AACpC,gBAAM,WAAW,MAAM,MAAM,cAAc,YAAYA,QAAO;AAC9D,oBAAU,KAAK,QAAQ;AAAA,QACzB;AAAA,MACF;AAEA,YAAM,eAAe,UAAU,IAAI,CAAC,aAAa,SAAS,WAAW;AACrE,YAAM,aAA4B,CAAC;AACnC,iBAAW,YAAY,WAAW;AAChC,mBAAW,KAAK,GAAG,SAAS,UAAU;AAAA,MACxC;AAEA,aAAO;AAAA,QACL,aAAa;AAAA,QACb,gBAAgB;AAAA,MAClB;AAAA,IACF;AAAA,EACF,CAAC;AAED,SAAO,eACH;AAAA,IACE,YAAY,aAAa;AAAA,IACzB,aAAa,aAAa;AAAA,IAC1B,UAAU,aAAa;AAAA,EACzB,IACA,aAAa;AACnB;AAoCA,eAAsB,MAAa;AAAA,EACjC;AAAA,EACA;AAAA,EACA;AAAA,EACA,GAAG;AACL,GAOE;AACA,QAAM,eAAe,MAAM,oBAAoB;AAAA,IAC7C,cAAc;AAAA,IACd,OAAO;AAAA,IACP;AAAA,IACA;AAAA,IACA,kBAAkB,OAAOA,aAAY;AACnC,YAAM,SAAS,MAAM,MAAM,cAAc,CAAC,KAAK,GAAGA,QAAO;AACzD,aAAO;AAAA,QACL,aAAa,OAAO;AAAA,QACpB,gBAAgB,OAAO,WAAW,CAAC;AAAA,MACrC;AAAA,IACF;AAAA,EACF,CAAC;AAED,SAAO,eACH;AAAA,IACE,WAAW,aAAa;AAAA,IACxB,aAAa,aAAa;AAAA,IAC1B,UAAU,aAAa;AAAA,EACzB,IACA,aAAa;AACnB;;;AC9JO,IAAM,gCAAN,MAAM,+BASb;AAAA,EACW;AAAA,EAEA,mBAAmB;AAAA,IAC1B,UAAU;AAAA,IACV,WAAW;AAAA,EACb;AAAA,EAEQ;AAAA,EAQR,YACE,UACA;AACA,SAAK,WAAW;AAAA,EAClB;AAAA,EAEA,MAAM,cAAc,SAA8B;AAChD,QAAI,KAAK,cAAc,MAAM;AAC3B,aAAO,KAAK;AAAA,IACd;AAEA,UAAM,aAID,CAAC;AAEN,eAAW,WAAW,KAAK,SAAS,UAAU;AAC5C,YAAM,oBAAoB,MAAM,UAAU;AAAA,QACxC,OAAO,KAAK,SAAS;AAAA,QACrB,QAAQ,QAAQ;AAAA,QAChB,GAAG;AAAA,MACL,CAAC;AAED,eAAS,IAAI,GAAG,IAAI,kBAAkB,QAAQ,KAAK;AACjD,mBAAW,KAAK;AAAA,UACd,WAAW,kBAAkB,CAAC;AAAA,UAC9B,cAAc,QAAQ,OAAO,CAAC;AAAA,UAC9B,aAAa,QAAQ;AAAA,QACvB,CAAC;AAAA,MACH;AAAA,IACF;AAEA,SAAK,aAAa;AAElB,WAAO;AAAA,EACT;AAAA,EAEA,MAAM,WAAW,OAAc,SAA8B;AAC3D,UAAM,iBAAiB,MAAM,MAAM;AAAA,MACjC,OAAO,KAAK,SAAS;AAAA,MACrB;AAAA,MACA,GAAG;AAAA,IACL,CAAC;AAED,UAAM,oBAAoB,MAAM,KAAK,cAAc,OAAO;AAE1D,UAAM,aAID,CAAC;AAEN,eAAW,aAAa,mBAAmB;AACzC,YAAM,aAAa,iBAAiB,gBAAgB,UAAU,SAAS;AAEvE,UAAI,cAAc,KAAK,SAAS,qBAAqB;AACnD,mBAAW,KAAK;AAAA,UACd;AAAA,UACA,cAAc,UAAU;AAAA,UACxB,aAAa,UAAU;AAAA,QACzB,CAAC;AAAA,MACH;AAAA,IACF;AAGA,eAAW,KAAK,CAAC,GAAG,MAAM,EAAE,aAAa,EAAE,UAAU;AAErD,WAAO;AAAA,MACL,OACE,WAAW,SAAS,IACf,WAAW,CAAC,EAAE,cACf;AAAA,MACN,aAAa;AAAA,IACf;AAAA,EACF;AAAA,EAEA,IAAI,mBAEF;AACA,UAAM,yBAAwC;AAAA,MAC5C;AAAA,MACA;AAAA,MACA;AAAA,IACF;AAKA,WAAO,OAAO;AAAA,MACZ,OAAO,QAAQ,KAAK,QAAQ,EAAE;AAAA,QAAO,CAAC,CAAC,GAAG,MACxC,uBAAuB,SAAS,GAAG;AAAA,MACrC;AAAA,IACF;AAAA,EACF;AAAA,EAEA,aACE,oBAGA;AACA,WAAO,IAAI;AAAA,MACT,OAAO,OAAO,CAAC,GAAG,KAAK,UAAU,kBAAkB;AAAA,IACrD;AAAA,EACF;AACF;;;ACrIA,eAAsB,SAA6C;AAAA,EACjE;AAAA,EACA;AAAA,EACA;AAAA,EACA,GAAG;AACL,GAME;AACA,QAAM,eAAe,MAAM,oBAAoB;AAAA,IAC7C,cAAc;AAAA,IACd,OAAO;AAAA,IACP;AAAA,IACA;AAAA,IACA,kBAAkB,OAAOC,aAAY;AACnC,YAAM,SAAS,MAAM,MAAM,WAAW,OAAOA,QAAO;AACpD,aAAO;AAAA,QACL,aAAa,OAAO;AAAA,QACpB,gBAAgB,OAAO;AAAA,MACzB;AAAA,IACF;AAAA,EACF,CAAC;AAED,SAAO,eACH;AAAA,IACE,OAAO,aAAa;AAAA,IACpB,aAAa,aAAa;AAAA,IAC1B,UAAU,aAAa;AAAA,EACzB,IACA,aAAa;AACnB;;;ACjDO,IAAM,qCAAN,MAAM,oCAMb;AAAA,EACW;AAAA,EACA;AAAA,EAET,YAAY;AAAA,IACV;AAAA,IACA;AAAA,EACF,GAGG;AACD,SAAK,QAAQ;AACb,SAAK,iBAAiB;AAAA,EACxB;AAAA,EAEA,IAAI,mBAAmB;AACrB,WAAO,KAAK,MAAM;AAAA,EACpB;AAAA,EAEA,IAAI,WAAW;AACb,WAAO,KAAK,MAAM;AAAA,EACpB;AAAA,EAEA,iBAAiB,QAAgB,SAA8B;AAC7D,UAAM,eAAe,KAAK,eAAe,OAAO,MAAM;AACtD,WAAO,KAAK,MAAM,iBAAiB,cAAc,OAAO;AAAA,EAC1D;AAAA,EAEA,IAAI,mBAAsC;AACxC,WAAO,KAAK,MAAM;AAAA,EACpB;AAAA,EAEA,mBACE,gBAC0E;AAC1E,WAAO,IAAI,oCAKT,EAAE,OAAO,MAAM,eAAe,CAAC;AAAA,EACnC;AAAA,EAEA,aAAa,oBAA6C;AACxD,WAAO,IAAI,oCAAmC;AAAA,MAC5C,OAAO,KAAK,MAAM,aAAa,kBAAkB;AAAA,MACjD,gBAAgB,KAAK;AAAA,IACvB,CAAC;AAAA,EACH;AACF;;;AChDO,SAAS,mBAAmB,cAAsB;AACvD,SAAO,WAAW;AAAA,IAChB,WAAW,KAAK,kBAAkB,YAAY,CAAC;AAAA,IAC/C,CAAC,MAAM,EAAE,YAAY,CAAC;AAAA,EACxB;AACF;AAGA,IAAM,iBAAiB;AAEhB,SAAS,mBAAmB,OAAmB;AACpD,MAAI;AAEJ,MAAI,MAAM,SAAS,gBAAgB;AAEjC,aAAS,WAAW,KAAK,OAAO,cAAc,GAAG,KAAK,CAAC;AAAA,EACzD,OAAO;AACL,aAAS;AACT,eAAW,SAAS,OAAO;AACzB,gBAAU,OAAO,cAAc,KAAK;AAAA,IACtC;AAEA,aAAS,WAAW,KAAK,MAAM;AAAA,EACjC;AAEA,SAAO;AACT;AAEA,SAAS,kBAAkB,WAAmB;AAC5C,SAAO,UAAU,WAAW,KAAK,GAAG,EAAE,WAAW,KAAK,GAAG;AAC3D;;;ACSA,eAAsB,cAAsB;AAAA,EAC1C;AAAA,EACA;AAAA,EACA;AAAA,EACA,GAAG;AACL,GAeE;AACA,QAAM,eAAe,MAAM,oBAAoB;AAAA,IAC7C,cAAc;AAAA,IACd,OAAO;AAAA,IACP;AAAA,IACA;AAAA,IACA,kBAAkB,OAAOC,aAAY;AACnC,YAAM,SAAS,MAAM,MAAM,iBAAiB,QAAQA,QAAO;AAE3D,aAAO;AAAA,QACL,aAAa,OAAO;AAAA,QACpB,gBAAgB,OAAO;AAAA,MACzB;AAAA,IACF;AAAA,EACF,CAAC;AAED,QAAM,eAAe,aAAa;AAClC,QAAM,SAAS,aAAa,IAAI,kBAAkB;AAElD,SAAO,eACH;AAAA,IACE,OAAO,OAAO,CAAC;AAAA,IACf,aAAa,aAAa,CAAC;AAAA,IAC3B;AAAA,IACA;AAAA,IACA,aAAa,aAAa;AAAA,IAC1B,UAAU,aAAa;AAAA,EACzB,IACA,OAAO,CAAC;AACd;;;AC3DA,eAAsB,eAAe;AAAA,EACnC;AAAA,EACA,MAAAC;AAAA,EACA;AAAA,EACA,GAAG;AACL,GAOE;AACA,QAAM,eAAe,MAAM,oBAAoB;AAAA,IAC7C,cAAc;AAAA,IACd,OAAOA;AAAA,IACP;AAAA,IACA;AAAA,IACA,kBAAkB,OAAOC,aAAY;AACnC,YAAM,WAAW,MAAM,MAAM,yBAAyBD,QAAMC,QAAO;AAEnE,aAAO;AAAA,QACL,aAAa;AAAA,QACb,gBAAgB;AAAA,MAClB;AAAA,IACF;AAAA,EACF,CAAC;AAED,SAAO,eACH;AAAA,IACE,QAAQ,aAAa;AAAA,IACrB,aAAa,aAAa;AAAA,IAC1B,UAAU,aAAa;AAAA,EACzB,IACA,aAAa;AACnB;;;AC/DO,IAAM,aAAN,MAAgD;AAAA,EAC7C,SAAS,MAEf;AAAA,EACM,mBAAsC,CAAC;AAAA,EACvC,SAAkB;AAAA,EAElB,0BAAgC;AACtC,WAAO,KAAK,iBAAiB,SAAS,GAAG;AACvC,WAAK,iBAAiB,MAAM,IAAI;AAAA,IAClC;AAAA,EACF;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAUA,KAAK,OAAgB;AACnB,QAAI,KAAK,QAAQ;AACf,YAAM,IAAI,MAAM,oCAAoC;AAAA,IACtD;AAEA,SAAK,OAAO,KAAK,EAAE,MAAM,SAAS,MAAM,CAAC;AACzC,SAAK,wBAAwB;AAAA,EAC/B;AAAA,EAEA,MAAM,OAAsB;AAC1B,QAAI,KAAK,QAAQ;AACf,YAAM,IAAI,MAAM,oCAAoC;AAAA,IACtD;AAEA,SAAK,OAAO,KAAK,EAAE,MAAM,SAAS,MAAM,CAAC;AACzC,SAAK,wBAAwB;AAAA,EAC/B;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAQA,QAAc;AACZ,SAAK,SAAS;AACd,SAAK,wBAAwB;AAAA,EAC/B;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAcA,CAAC,OAAO,aAAa,IAAsB;AACzC,QAAI,WAAW;AAEf,WAAO;AAAA,MACL,MAAM,MACJ,IAAI,QAAQ,CAAC,SAAS,WAAW;AAC/B,cAAM,iBAAiB,MAAM;AAC3B,cAAI,WAAW,KAAK,OAAO,QAAQ;AACjC,kBAAM,QAAQ,KAAK,OAAO,UAAU;AACpC,oBAAQ,MAAM,MAAM;AAAA,cAClB,KAAK;AACH,wBAAQ,EAAE,OAAO,MAAM,OAAO,MAAM,MAAM,CAAC;AAC3C;AAAA,cACF,KAAK;AACH,uBAAO,MAAM,KAAK;AAClB;AAAA,YACJ;AAAA,UACF,WAAW,KAAK,QAAQ;AAEtB,oBAAQ,EAAE,OAAO,QAAkB,MAAM,KAAK,CAAC;AAAA,UACjD,OAAO;AAIL,iBAAK,iBAAiB,KAAK,cAAc;AAAA,UAC3C;AAAA,QACF;AAEA,uBAAe;AAAA,MACjB,CAAC;AAAA,IACL;AAAA,EACF;AACF;;;AC7GA,SAAS,UAAUC,iBAAgB;AAmBnC,eAAsB,kBAIpB;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AACF,GAgBG;AACD,QAAM,MAAM,MAAM,OAAO,SAAS,GAAG;AACrC,QAAM,WAAW,MAAM;AAEvB,QAAM,cAAc,IAAI,oBAAoB;AAAA,IAC1C,WAAW;AAAA,MACT,GAAG,sBAAsB,SAAS,WAAW,aAAa,CAAC;AAAA,MAC3D,GAAG,qBAAqB;AAAA,MACxB,GAAI,SAAS,aAAa,CAAC;AAAA,MAC3B,GAAI,KAAK,oBAAoB,OAAO,CAAC,IAAI,gBAAgB,IAAI,CAAC;AAAA,MAC9D,GAAI,SAAS,aAAa,CAAC;AAAA,IAC7B;AAAA,IACA,cAAc,KAAK;AAAA,EACrB,CAAC;AAED,QAAM,sBAAsB,yBAAyB;AAErD,QAAM,gBAAgB;AAAA,IACpB;AAAA,IAEA,QAAQ,QAAQC,UAAS,CAAC;AAAA,IAC1B,cAAc,SAAS;AAAA,IACvB,OAAO,KAAK;AAAA,IACZ,WAAW,KAAK;AAAA,IAChB,QAAQ,KAAK;AAAA,IACb,YAAY,SAAS;AAAA,IAErB,OAAO,MAAM;AAAA,IACb,UAAU,MAAM;AAAA,IAChB;AAAA,IAEA,WAAW,oBAAoB;AAAA,IAC/B,gBAAgB,oBAAoB;AAAA,EACtC;AAEA,cAAY,OAAO;AAAA,IACjB,WAAW;AAAA,IACX,GAAG;AAAA,EACL,CAA0B;AAE1B,QAAM,SAAS,MAAM,QAAQ,YAAY;AACvC,UAAM,gBAAgB,MAAM,YAAY;AAAA,MACtC;AAAA,MACA,YAAY,SAAS;AAAA,MACrB,QAAQ,cAAc;AAAA,MACtB,SAAS,SAAS;AAAA,MAClB,WAAW,SAAS;AAAA,MACpB;AAAA,IACF,CAAC;AAGD,UAAM,gBAAgB,IAAI,WAAkB;AAG5C,KAAC,iBAAkB;AACjB,UAAI;AACF,cAAM,aAAa,MAAM,QAAQ,YAAY;AAC3C,2BAAiB,SAAS,eAAe;AACvC,gBAAI,OAAO,SAAS,SAAS;AAC3B,oBAAM,QAAQ,MAAM;AAEpB,oBAAMC,kBAAiB;AAAA,gBACrB,WAAW;AAAA,gBACX,GAAG;AAAA,gBACH,iBAAiB,oBAAI,KAAK;AAAA,gBAC1B,cAAc,oBAAoB;AAAA,cACpC;AAEA,0BAAY;AAAA,gBACV,iBAAiB,aACZ;AAAA,kBACC,GAAGA;AAAA,kBACH,QAAQ,EAAE,QAAQ,QAAQ;AAAA,gBAC5B,IACC;AAAA,kBACC,GAAGA;AAAA,kBACH,QAAQ,EAAE,QAAQ,SAAS,MAAM;AAAA,gBACnC;AAAA,cACN;AAEA,oBAAM;AAAA,YACR;AAEA,gBAAI,OAAO,SAAS,SAAS;AAC3B,oBAAM,QAAQ,aAAa,KAAK;AAChC,kBAAI,UAAU,QAAW;AACvB,8BAAc,KAAK,KAAK;AAAA,cAC1B;AAAA,YACF;AAAA,UACF;AAEA,cAAI,mBAAmB,MAAM;AAC3B,kBAAM,QAAQ,gBAAgB;AAE9B,gBAAI,UAAU,QAAW;AACvB,4BAAc,KAAK,KAAK;AAAA,YAC1B;AAAA,UACF;AAAA,QACF,CAAC;AAGD,YAAI,CAAC,WAAW,IAAI;AAClB,gBAAMA,kBAAiB;AAAA,YACrB,WAAW;AAAA,YACX,GAAG;AAAA,YACH,iBAAiB,oBAAI,KAAK;AAAA,YAC1B,cAAc,oBAAoB;AAAA,UACpC;AAEA,cAAI,WAAW,WAAW;AACxB,wBAAY,OAAO;AAAA,cACjB,GAAGA;AAAA,cACH,WAAW;AAAA,cACX,QAAQ;AAAA,gBACN,QAAQ;AAAA,cACV;AAAA,YACF,CAA2B;AAE3B,0BAAc,MAAM,IAAI,WAAW,CAAC;AAEpC;AAAA,UACF;AAEA,sBAAY,OAAO;AAAA,YACjB,GAAGA;AAAA,YACH,WAAW;AAAA,YACX,QAAQ;AAAA,cACN,QAAQ;AAAA,cACR,OAAO,WAAW;AAAA,YACpB;AAAA,UACF,CAA2B;AAE3B,wBAAc,MAAM,WAAW,KAAK;AAEpC;AAAA,QACF;AAEA,iBAAS;AAET,cAAM,iBAAiB;AAAA,UACrB,WAAW;AAAA,UACX,GAAG;AAAA,UACH,iBAAiB,oBAAI,KAAK;AAAA,UAC1B,cAAc,oBAAoB;AAAA,QACpC;AAEA,oBAAY,OAAO;AAAA,UACjB,GAAG;AAAA,UACH,QAAQ;AAAA,YACN,QAAQ;AAAA,UACV;AAAA,QACF,CAA2B;AAAA,MAC7B,UAAE;AAEA,sBAAc,MAAM;AAAA,MACtB;AAAA,IACF,GAAG;AAEH,WAAO;AAAA,MACL,QAAQ;AAAA,IACV;AAAA,EACF,CAAC;AAED,MAAI,CAAC,OAAO,IAAI;AACd,UAAM,iBAAiB;AAAA,MACrB,WAAW;AAAA,MACX,GAAG;AAAA,MACH,iBAAiB,oBAAI,KAAK;AAAA,MAC1B,cAAc,oBAAoB;AAAA,IACpC;AAEA,QAAI,OAAO,WAAW;AACpB,kBAAY,OAAO;AAAA,QACjB,GAAG;AAAA,QACH,WAAW;AAAA,QACX,QAAQ;AAAA,UACN,QAAQ;AAAA,QACV;AAAA,MACF,CAA2B;AAE3B,YAAM,IAAI,WAAW;AAAA,IACvB;AAEA,gBAAY,OAAO;AAAA,MACjB,GAAG;AAAA,MACH,WAAW;AAAA,MACX,QAAQ;AAAA,QACN,QAAQ;AAAA,QACR,OAAO,OAAO;AAAA,MAChB;AAAA,IACF,CAA2B;AAE3B,UAAM,OAAO;AAAA,EACf;AAEA,SAAO;AAAA,IACL,OAAO,OAAO,MAAM;AAAA,IACpB,UAAU;AAAA,EACZ;AACF;;;ACrMA,eAAsB,aAAa;AAAA,EACjC;AAAA,EACA,MAAAC;AAAA,EACA;AAAA,EACA,GAAG;AACL,GAUE;AACA,MAAI;AAGJ,MAAI,OAAOA,WAAS,UAAU;AAC5B,UAAM,QAAQ,IAAI,WAAmB;AACrC,UAAM,KAAKA,MAAI;AACf,UAAM,MAAM;AACZ,iBAAa;AAAA,EACf,OAAO;AACL,iBAAaA;AAAA,EACf;AAEA,QAAM,eAAe,MAAM,kBAAkB;AAAA,IAC3C,cAAc;AAAA,IACd,OAAOA;AAAA,IACP;AAAA,IACA;AAAA,IACA,aAAa,OAAOC,aAClB,MAAM,6BAA6B,YAAYA,QAAO;AAAA,IACxD,cAAc,CAAC,UAAU,MAAM;AAAA,EACjC,CAAC;AAED,SAAO,eACH;AAAA,IACE,cAAc,aAAa;AAAA,IAC3B,UAAU,aAAa;AAAA,EACzB,IACA,aAAa;AACnB;;;ACxCA,eAAsB,aAAqB;AAAA,EACzC;AAAA,EACA;AAAA,EACA;AAAA,EACA,GAAG;AACL,GAcE;AACA,QAAM,iBAAiB,MAAM,aAAa,MAAM;AAEhD,QAAM,eAAe,MAAM,oBAAoB;AAAA,IAC7C,cAAc;AAAA,IACd,OAAO;AAAA,IACP;AAAA,IACA;AAAA,IACA,kBAAkB,OAAOC,aAAY;AACnC,qBAAe,oBAAoB;AACjC,YAAIA,UAAS,SAAS,MAAM;AAC1B,iBAAO;AAAA,YACL,GAAI,MAAM,MAAM,gBAAgB,eAAe,QAAQA,QAAO;AAAA,YAC9D,OAAO;AAAA,UACT;AAAA,QACF;AAEA,YAAI,cAAqC;AAEzC,cAAM,WAAW;AAAA,UACf,cAAc;AAAA,UACd,YAAYA,UAAS;AAAA,UACrB,OAAO;AAAA,YACL;AAAA,YACA,UAAU,MAAM;AAAA;AAAA,YAChB,QAAQ,eAAe;AAAA,UACzB;AAAA,QACF;AAEA,YAAI;AACF,gBAAM,oBAAoB,MAAMA,SAAQ,MAAM,YAAY,QAAQ;AAElE,cAAI,qBAAqB,MAAM;AAC7B,mBAAO;AAAA,cACL,GAAG,MAAM,sBAAsB,iBAAiB;AAAA,cAChD,OAAO,EAAE,QAAQ,MAAM;AAAA,YACzB;AAAA,UACF;AAAA,QACF,SAAS,KAAK;AACZ,wBAAc,CAAC,GAAG;AAAA,QACpB;AAEA,cAAMC,UAAS,MAAM,MAAM;AAAA,UACzB,eAAe;AAAA,UACfD;AAAA,QACF;AAEA,YAAI;AACF,gBAAMA,SAAQ,MAAM,WAAW,UAAUC,QAAO,WAAW;AAAA,QAC7D,SAAS,KAAK;AACZ,wBAAc,CAAC,GAAI,eAAe,CAAC,GAAI,GAAG;AAAA,QAC5C;AAEA,eAAO;AAAA,UACL,GAAGA;AAAA,UACH,OAAO,EAAE,QAAQ,QAAQ,QAAQ,YAAY;AAAA,QAC/C;AAAA,MACF;AAEA,YAAM,SAAS,MAAM,kBAAkB;AAEvC,YAAM,uBAAuB,MAAM,SAAS,kBAAkB;AAE9D,YAAMC,yBAAwB,uBAC1B,OAAO,sBAAsB,IAAI,CAAC,oBAAoB;AAAA,QACpD,MAAM,eAAe,KAAK,KAAK;AAAA,QAC/B,cAAc,eAAe;AAAA,MAC/B,EAAE,IACF,OAAO;AAGX,aAAO;AAAA,QACL,aAAa,OAAO;AAAA,QACpB,gBAAgBA;AAAA,QAChB,OAAO,OAAO;AAAA,MAChB;AAAA,IACF;AAAA,EACF,CAAC;AAED,QAAM,wBAAwB,aAAa;AAC3C,QAAM,cAAc,sBAAsB,CAAC;AAE3C,SAAO,eACH;AAAA,IACE,MAAM,YAAY;AAAA,IAClB,cAAc,YAAY;AAAA,IAC1B,OAAO,sBAAsB;AAAA,MAC3B,CAAC,mBAAmB,eAAe;AAAA,IACrC;AAAA,IACA;AAAA,IACA,aAAa,aAAa;AAAA,IAC1B,UAAU,aAAa;AAAA,EACzB,IACA,YAAY;AAClB;;;ACrKO,IAAM,mBAAN,cAA+B,MAAM;AAAA,EACjC;AAAA,EACA;AAAA,EAET,YAAY,EAAE,WAAW,MAAM,GAA0C;AACvE;AAAA,MACE,iCACY,SAAS;AAAA,iBACD,gBAAgB,KAAK,CAAC;AAAA,IAC5C;AAEA,SAAK,OAAO;AAEZ,SAAK,QAAQ;AACb,SAAK,YAAY;AAAA,EACnB;AAAA,EAEA,SAAS;AACP,WAAO;AAAA,MACL,MAAM,KAAK;AAAA,MACX,OAAO,KAAK;AAAA,MACZ,SAAS,KAAK;AAAA,MACd,OAAO,KAAK;AAAA,MAEZ,WAAW,KAAK;AAAA,IAClB;AAAA,EACF;AACF;;;ACjBO,IAAM,gCAAN,MAAM,+BAKb;AAAA,EACqB;AAAA,EACA;AAAA,EAKnB,YAAY;AAAA,IACV;AAAA,IACA;AAAA,EACF,GAGG;AACD,SAAK,QAAQ;AACb,SAAK,WAAW;AAAA,EAClB;AAAA,EAEA,IAAI,mBAAmB;AACrB,WAAO,KAAK,MAAM;AAAA,EACpB;AAAA,EAEA,IAAI,WAAW;AACb,WAAO,KAAK,MAAM;AAAA,EACpB;AAAA,EAEA,IAAI,mBAA+C;AACjD,WAAO,KAAK,MAAM;AAAA,EACpB;AAAA,EAEA,uBAAuB,QAA8C;AACnE,QAAI,KAAK,SAAS,kBAAkB,MAAM;AACxC,aAAO,KAAK,SAAS,eAAe;AAAA,QAClC,OAAO,KAAK;AAAA,QACZ;AAAA,MACF,CAAC;AAAA,IACH;AAEA,WAAO,KAAK;AAAA,EACd;AAAA,EAEA,MAAM,iBACJ,QACA,QACA,SACA;AACA,UAAM,EAAE,aAAa,MAAAC,OAAK,IAAI,MAAM,aAAa;AAAA,MAC/C,OAAO,KAAK,uBAAuB,MAAM;AAAA,MACzC,QAAQ,KAAK,SAAS,aAAa,QAAQ,MAAM;AAAA,MACjD,cAAc;AAAA,MACd,GAAG;AAAA,IACL,CAAC;AAED,QAAI;AACF,aAAO;AAAA,QACL;AAAA,QACA,OAAO,KAAK,SAAS,cAAcA,MAAI;AAAA,QACvC,WAAWA;AAAA,MACb;AAAA,IACF,SAAS,OAAO;AACd,YAAM,IAAI,iBAAiB;AAAA,QACzB,WAAWA;AAAA,QACX,OAAO;AAAA,MACT,CAAC;AAAA,IACH;AAAA,EACF;AAAA,EAEA,aAAa,oBAAsD;AACjE,WAAO,IAAI,+BAA8B;AAAA,MACvC,OAAO,KAAK,MAAM,aAAa,kBAAkB;AAAA,MACjD,UAAU,KAAK;AAAA,IACjB,CAAC;AAAA,EACH;AACF;;;ACxCA,eAAsB,WAAmB;AAAA,EACvC;AAAA,EACA;AAAA,EACA;AAAA,EACA,GAAG;AACL,GAWE;AACA,QAAM,uBAAuB,MAAM,SAAS,kBAAkB;AAE9D,MAAI,kBAAkB;AACtB,MAAI,eAAe;AACnB,MAAI,qBAAqB;AAEzB,MAAI;AACJ,QAAM,cAAc,IAAI,QAAgB,CAAC,YAAY;AACnD,kBAAc;AAAA,EAChB,CAAC;AAED,QAAM,iBAAiB,MAAM,aAAa,MAAM;AAEhD,QAAM,eAAe,MAAM,kBAAkB;AAAA,IAC3C,cAAc;AAAA,IACd,OAAO;AAAA,IACP;AAAA,IACA;AAAA,IACA,aAAa,OAAOC,aAClB,MAAM,aAAa,eAAe,QAAQA,QAAO;AAAA,IACnD,cAAc,CAAC,UAAU;AACvB,UAAI,YAAY,MAAM,iBAAiB,MAAM,UAAU;AAEvD,UAAI,aAAa,QAAQ,UAAU,WAAW,GAAG;AAC/C,eAAO;AAAA,MACT;AAEA,UAAI,sBAAsB;AACxB,oBAAY;AAAA;AAAA,UAER,UAAU,UAAU;AAAA;AAAA;AAAA,UAEpB,qBAAqB;AAAA;AAGzB,cAAM,0BAA0B,UAAU,MAAM,MAAM;AACtD,6BAAqB,0BACjB,wBAAwB,CAAC,IACzB;AACJ,oBAAY,UAAU,QAAQ;AAAA,MAChC;AAEA,qBAAe;AACf,yBAAmB;AAEnB,aAAO;AAAA,IACT;AAAA,IACA,QAAQ,MAAM;AACZ,kBAAY,eAAe;AAAA,IAC7B;AAAA,EACF,CAAC;AAED,SAAO,eACH;AAAA,IACE,YAAY,aAAa;AAAA,IACzB;AAAA,IACA,UAAU,aAAa;AAAA,EACzB,IACA,aAAa;AACnB;;;AC9HA,OAAOC,iBAAgB;;;AC0BhB,SAAS,QAAQ,OAAuB;AAC7C,QAAM,QAAiB,CAAC,MAAM;AAC9B,MAAI,iBAAiB;AACrB,MAAI,eAA8B;AAElC,WAAS,kBAAkB,MAAc,GAAW,WAAkB;AACpE;AACE,cAAQ,MAAM;AAAA,QACZ,KAAK,KAAK;AACR,2BAAiB;AACjB,gBAAM,IAAI;AACV,gBAAM,KAAK,SAAS;AACpB,gBAAM,KAAK,eAAe;AAC1B;AAAA,QACF;AAAA,QAEA,KAAK;AAAA,QACL,KAAK;AAAA,QACL,KAAK,KAAK;AACR,2BAAiB;AACjB,yBAAe;AACf,gBAAM,IAAI;AACV,gBAAM,KAAK,SAAS;AACpB,gBAAM,KAAK,gBAAgB;AAC3B;AAAA,QACF;AAAA,QAEA,KAAK,KAAK;AACR,gBAAM,IAAI;AACV,gBAAM,KAAK,SAAS;AACpB,gBAAM,KAAK,eAAe;AAC1B;AAAA,QACF;AAAA,QACA,KAAK;AAAA,QACL,KAAK;AAAA,QACL,KAAK;AAAA,QACL,KAAK;AAAA,QACL,KAAK;AAAA,QACL,KAAK;AAAA,QACL,KAAK;AAAA,QACL,KAAK;AAAA,QACL,KAAK;AAAA,QACL,KAAK,KAAK;AACR,2BAAiB;AACjB,gBAAM,IAAI;AACV,gBAAM,KAAK,SAAS;AACpB,gBAAM,KAAK,eAAe;AAC1B;AAAA,QACF;AAAA,QAEA,KAAK,KAAK;AACR,2BAAiB;AACjB,gBAAM,IAAI;AACV,gBAAM,KAAK,SAAS;AACpB,gBAAM,KAAK,qBAAqB;AAChC;AAAA,QACF;AAAA,QAEA,KAAK,KAAK;AACR,2BAAiB;AACjB,gBAAM,IAAI;AACV,gBAAM,KAAK,SAAS;AACpB,gBAAM,KAAK,oBAAoB;AAC/B;AAAA,QACF;AAAA,MACF;AAAA,IACF;AAAA,EACF;AAEA,WAAS,wBAAwB,MAAc,GAAW;AACxD,YAAQ,MAAM;AAAA,MACZ,KAAK,KAAK;AACR,cAAM,IAAI;AACV,cAAM,KAAK,2BAA2B;AACtC;AAAA,MACF;AAAA,MACA,KAAK,KAAK;AACR,yBAAiB;AACjB,cAAM,IAAI;AACV;AAAA,MACF;AAAA,IACF;AAAA,EACF;AAEA,WAAS,uBAAuB,MAAc,GAAW;AACvD,YAAQ,MAAM;AAAA,MACZ,KAAK,KAAK;AACR,cAAM,IAAI;AACV,cAAM,KAAK,0BAA0B;AACrC;AAAA,MACF;AAAA,MACA,KAAK,KAAK;AACR,yBAAiB;AACjB,cAAM,IAAI;AACV;AAAA,MACF;AAAA,IACF;AAAA,EACF;AAEA,WAAS,IAAI,GAAG,IAAI,MAAM,QAAQ,KAAK;AACrC,UAAM,OAAO,MAAM,CAAC;AACpB,UAAM,eAAe,MAAM,MAAM,SAAS,CAAC;AAE3C,YAAQ,cAAc;AAAA,MACpB,KAAK;AACH,0BAAkB,MAAM,GAAG,QAAQ;AACnC;AAAA,MAEF,KAAK,uBAAuB;AAC1B,gBAAQ,MAAM;AAAA,UACZ,KAAK,KAAK;AACR,kBAAM,IAAI;AACV,kBAAM,KAAK,mBAAmB;AAC9B;AAAA,UACF;AAAA,UACA,KAAK,KAAK;AACR,kBAAM,IAAI;AACV;AAAA,UACF;AAAA,QACF;AACA;AAAA,MACF;AAAA,MAEA,KAAK,6BAA6B;AAChC,gBAAQ,MAAM;AAAA,UACZ,KAAK,KAAK;AACR,kBAAM,IAAI;AACV,kBAAM,KAAK,mBAAmB;AAC9B;AAAA,UACF;AAAA,QACF;AACA;AAAA,MACF;AAAA,MAEA,KAAK,qBAAqB;AACxB,gBAAQ,MAAM;AAAA,UACZ,KAAK,KAAK;AACR,kBAAM,IAAI;AACV,kBAAM,KAAK,yBAAyB;AACpC;AAAA,UACF;AAAA,QACF;AACA;AAAA,MACF;AAAA,MAEA,KAAK,2BAA2B;AAC9B,gBAAQ,MAAM;AAAA,UACZ,KAAK,KAAK;AACR,kBAAM,IAAI;AACV,kBAAM,KAAK,4BAA4B;AAEvC;AAAA,UACF;AAAA,QACF;AACA;AAAA,MACF;AAAA,MAEA,KAAK,8BAA8B;AACjC,0BAAkB,MAAM,GAAG,2BAA2B;AACtD;AAAA,MACF;AAAA,MAEA,KAAK,6BAA6B;AAChC,gCAAwB,MAAM,CAAC;AAC/B;AAAA,MACF;AAAA,MAEA,KAAK,iBAAiB;AACpB,gBAAQ,MAAM;AAAA,UACZ,KAAK,KAAK;AACR,kBAAM,IAAI;AACV,6BAAiB;AACjB;AAAA,UACF;AAAA,UAEA,KAAK,MAAM;AACT,kBAAM,KAAK,sBAAsB;AACjC;AAAA,UACF;AAAA,UAEA,SAAS;AACP,6BAAiB;AAAA,UACnB;AAAA,QACF;AAEA;AAAA,MACF;AAAA,MAEA,KAAK,sBAAsB;AACzB,gBAAQ,MAAM;AAAA,UACZ,KAAK,KAAK;AACR,6BAAiB;AACjB,kBAAM,IAAI;AACV;AAAA,UACF;AAAA,UAEA,SAAS;AACP,6BAAiB;AACjB,8BAAkB,MAAM,GAAG,0BAA0B;AACrD;AAAA,UACF;AAAA,QACF;AACA;AAAA,MACF;AAAA,MAEA,KAAK,4BAA4B;AAC/B,gBAAQ,MAAM;AAAA,UACZ,KAAK,KAAK;AACR,kBAAM,IAAI;AACV,kBAAM,KAAK,0BAA0B;AACrC;AAAA,UACF;AAAA,UAEA,KAAK,KAAK;AACR,6BAAiB;AACjB,kBAAM,IAAI;AACV;AAAA,UACF;AAAA,UAEA,SAAS;AACP,6BAAiB;AACjB;AAAA,UACF;AAAA,QACF;AAEA;AAAA,MACF;AAAA,MAEA,KAAK,4BAA4B;AAC/B,0BAAkB,MAAM,GAAG,0BAA0B;AACrD;AAAA,MACF;AAAA,MAEA,KAAK,wBAAwB;AAC3B,cAAM,IAAI;AACV,yBAAiB;AAEjB;AAAA,MACF;AAAA,MAEA,KAAK,iBAAiB;AACpB,gBAAQ,MAAM;AAAA,UACZ,KAAK;AAAA,UACL,KAAK;AAAA,UACL,KAAK;AAAA,UACL,KAAK;AAAA,UACL,KAAK;AAAA,UACL,KAAK;AAAA,UACL,KAAK;AAAA,UACL,KAAK;AAAA,UACL,KAAK;AAAA,UACL,KAAK,KAAK;AACR,6BAAiB;AACjB;AAAA,UACF;AAAA,UAEA,KAAK;AAAA,UACL,KAAK;AAAA,UACL,KAAK;AAAA,UACL,KAAK,KAAK;AACR;AAAA,UACF;AAAA,UAEA,KAAK,KAAK;AACR,kBAAM,IAAI;AAEV,gBAAI,MAAM,MAAM,SAAS,CAAC,MAAM,4BAA4B;AAC1D,qCAAuB,MAAM,CAAC;AAAA,YAChC;AAEA,gBAAI,MAAM,MAAM,SAAS,CAAC,MAAM,6BAA6B;AAC3D,sCAAwB,MAAM,CAAC;AAAA,YACjC;AAEA;AAAA,UACF;AAAA,UAEA,KAAK,KAAK;AACR,kBAAM,IAAI;AAEV,gBAAI,MAAM,MAAM,SAAS,CAAC,MAAM,6BAA6B;AAC3D,sCAAwB,MAAM,CAAC;AAAA,YACjC;AAEA;AAAA,UACF;AAAA,UAEA,KAAK,KAAK;AACR,kBAAM,IAAI;AAEV,gBAAI,MAAM,MAAM,SAAS,CAAC,MAAM,4BAA4B;AAC1D,qCAAuB,MAAM,CAAC;AAAA,YAChC;AAEA;AAAA,UACF;AAAA,UAEA,SAAS;AACP,kBAAM,IAAI;AACV;AAAA,UACF;AAAA,QACF;AAEA;AAAA,MACF;AAAA,MAEA,KAAK,kBAAkB;AACrB,cAAM,iBAAiB,MAAM,UAAU,cAAe,IAAI,CAAC;AAE3D,YACE,CAAC,QAAQ,WAAW,cAAc,KAClC,CAAC,OAAO,WAAW,cAAc,KACjC,CAAC,OAAO,WAAW,cAAc,GACjC;AACA,gBAAM,IAAI;AAEV,cAAI,MAAM,MAAM,SAAS,CAAC,MAAM,6BAA6B;AAC3D,oCAAwB,MAAM,CAAC;AAAA,UACjC,WAAW,MAAM,MAAM,SAAS,CAAC,MAAM,4BAA4B;AACjE,mCAAuB,MAAM,CAAC;AAAA,UAChC;AAAA,QACF,OAAO;AACL,2BAAiB;AAAA,QACnB;AAEA;AAAA,MACF;AAAA,IACF;AAAA,EACF;AAEA,MAAI,SAAS,MAAM,MAAM,GAAG,iBAAiB,CAAC;AAE9C,WAAS,IAAI,MAAM,SAAS,GAAG,KAAK,GAAG,KAAK;AAC1C,UAAM,QAAQ,MAAM,CAAC;AAErB,YAAQ,OAAO;AAAA,MACb,KAAK,iBAAiB;AACpB,kBAAU;AACV;AAAA,MACF;AAAA,MAEA,KAAK;AAAA,MACL,KAAK;AAAA,MACL,KAAK;AAAA,MACL,KAAK;AAAA,MACL,KAAK;AAAA,MACL,KAAK,6BAA6B;AAChC,kBAAU;AACV;AAAA,MACF;AAAA,MAEA,KAAK;AAAA,MACL,KAAK;AAAA,MACL,KAAK,4BAA4B;AAC/B,kBAAU;AACV;AAAA,MACF;AAAA,MAEA,KAAK,kBAAkB;AACrB,cAAM,iBAAiB,MAAM,UAAU,cAAe,MAAM,MAAM;AAElE,YAAI,OAAO,WAAW,cAAc,GAAG;AACrC,oBAAU,OAAO,MAAM,eAAe,MAAM;AAAA,QAC9C,WAAW,QAAQ,WAAW,cAAc,GAAG;AAC7C,oBAAU,QAAQ,MAAM,eAAe,MAAM;AAAA,QAC/C,WAAW,OAAO,WAAW,cAAc,GAAG;AAC5C,oBAAU,OAAO,MAAM,eAAe,MAAM;AAAA,QAC9C;AAAA,MACF;AAAA,IACF;AAAA,EACF;AAEA,SAAO;AACT;;;AD5YO,SAAS,iBACd,UACqB;AACrB,MAAI,YAAY,MAAM;AACpB,WAAO;AAAA,EACT;AAEA,MAAI;AAEF,WAAOC,YAAW,MAAM,QAAQ;AAAA,EAClC,SAAS,SAAS;AAChB,QAAI;AAEF,YAAM,gBAAgB,QAAQ,QAAQ;AACtC,aAAOA,YAAW,MAAM,aAAa;AAAA,IACvC,SAASC,UAAS;AAAA,IAElB;AAAA,EACF;AAEA,SAAO;AACT;;;AETO,IAAM,+BAAN,MAAM,sCAQH,8BAEV;AAAA,EACE,YAAY,SAGT;AACD,UAAM,OAAO;AAAA,EACf;AAAA,EAEA,MAAM,eACJ,QACA,QACA,SACA;AACA,UAAM,aAAa,MAAM,WAAW;AAAA,MAClC,OAAO,KAAK,uBAAuB,MAAM;AAAA,MACzC,QAAQ,KAAK,SAAS,aAAa,QAAQ,MAAM;AAAA,MACjD,GAAG;AAAA,IACL,CAAC;AAED,UAAM,QAAQ,IAAI,WAA0B;AAG5C,KAAC,YAAY;AACX,UAAI;AACF,yBAAiB,aAAa,YAAY;AACxC,gBAAM,KAAK,EAAE,MAAM,SAAS,YAAY,UAAU,CAAC;AAAA,QACrD;AAAA,MACF,SAAS,OAAO;AACd,cAAM,KAAK,EAAE,MAAM,SAAS,MAAM,CAAC;AAAA,MACrC,UAAE;AACA,cAAM,MAAM;AAAA,MACd;AAAA,IACF,GAAG;AAEH,WAAO;AAAA,EACT;AAAA,EAEA,uBAAuB,OAAwB;AAC7C,WAAO;AAAA,EACT;AAAA,EAEA,2BAA2B,iBAAkC;AAC3D,WAAO,iBAAiB,eAAe;AAAA,EACzC;AAAA,EAEA,aAAa,oBAAsD;AACjE,WAAO,IAAI,8BAA6B;AAAA,MACtC,OAAO,KAAK,MAAM,aAAa,kBAAkB;AAAA,MACjD,UAAU,KAAK;AAAA,IACjB,CAAC;AAAA,EACH;AACF;;;AC1DO,IAAM,uBAAN,cAAmC,SAAS;AAAA,EACjD,YAAY,QAA+B,MAAqB;AAC9D,UAAM,yBAAyB,MAAM,GAAG;AAAA,MACtC,GAAG;AAAA,MACH,QAAQ;AAAA,MACR,SAAS,EAAE,gBAAgB,4BAA4B;AAAA,IACzD,CAAC;AAAA,EACH;AACF;AAQA,gBAAuB,yBAAiC;AAAA,EACtD;AACF,GAGG;AACD,MAAIC,SAAO;AAEX,QAAM,SAAS,SAAS,KAAM,UAAU;AAGxC,SAAO,MAAM;AACX,UAAM,EAAE,MAAM,MAAM,IAAI,MAAM,OAAO,KAAK;AAE1C,QAAI;AAAM;AAEV,IAAAA,UAAQ,IAAI,YAAY,EAAE,OAAO,KAAK;AAEtC,UAAM,gBAAgB,iBAAiBA,MAAI;AAE3C,UAAM,EAAE,cAAc;AAAA,EACxB;AACF;AAEA,SAAS,yBAAyB,QAA+B;AAC/D,QAAMC,eAAc,IAAI,YAAY;AACpC,SAAO,IAAI,eAAe;AAAA,IACxB,MAAM,MAAM,YAAY;AACtB,UAAI;AACF,yBAAiB,EAAE,UAAU,KAAK,QAAQ;AACxC,qBAAW,QAAQA,aAAY,OAAO,SAAS,CAAC;AAAA,QAClD;AAAA,MACF,UAAE;AACA,mBAAW,MAAM;AAAA,MACnB;AAAA,IACF;AAAA,EACF,CAAC;AACH;;;ACrEO,IAAM,wBAAN,cAAoC,MAAM;AAAA,EACtC;AAAA,EACA;AAAA,EACA;AAAA,EAET,YAAY;AAAA,IACV;AAAA,IACA;AAAA,IACA;AAAA,EACF,GAIG;AACD;AAAA,MACE,oCACY,SAAS;AAAA,iBACD,gBAAgB,KAAK,CAAC;AAAA,IAC5C;AAEA,SAAK,OAAO;AAEZ,SAAK,QAAQ;AACb,SAAK,QAAQ;AACb,SAAK,YAAY;AAAA,EACnB;AAAA,EAEA,SAAS;AACP,WAAO;AAAA,MACL,MAAM,KAAK;AAAA,MACX,SAAS,KAAK;AAAA,MACd,OAAO,KAAK;AAAA,MACZ,OAAO,KAAK;AAAA,MAEZ,OAAO,KAAK;AAAA,MACZ,WAAW,KAAK;AAAA,IAClB;AAAA,EACF;AACF;;;AC6CA,eAAsB,eAIpB;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA,GAAG;AACL,GAeE;AAEA,QAAM,iBACJ,OAAO,WAAW,cAAc,CAAC,iBAAiB,MAAM,IACnD,OAA8C,MAAM,IACrD;AAEN,QAAM,iBAAiB,MAAM,aAAa,cAAc;AAExD,QAAM,eAAe,MAAM,oBAAoB;AAAA,IAC7C,cAAc;AAAA,IACd,OAAO;AAAA,MACL;AAAA,MACA,GAAG;AAAA,IACL;AAAA,IACA;AAAA,IACA;AAAA,IACA,kBAAkB,OAAOC,aAAY;AACnC,YAAM,SAAS,MAAM,MAAM;AAAA,QACzB;AAAA,QACA,eAAe;AAAA,QACfA;AAAA,MACF;AAEA,YAAM,cAAc,OAAO,SAAS,OAAO,KAAK;AAEhD,UAAI,CAAC,YAAY,SAAS;AACxB,cAAM,IAAI,sBAAsB;AAAA,UAC9B,WAAW,OAAO;AAAA,UAClB,OAAO,OAAO;AAAA,UACd,OAAO,YAAY;AAAA,QACrB,CAAC;AAAA,MACH;AAEA,YAAM,QAAQ,YAAY;AAE1B,aAAO;AAAA,QACL,aAAa,OAAO;AAAA,QACpB,gBAAgB;AAAA,QAChB,OAAO,OAAO;AAAA,MAChB;AAAA,IACF;AAAA,EACF,CAAC;AAED,SAAO,eACH;AAAA,IACE,OAAO,aAAa;AAAA,IACpB,aAAa,aAAa;AAAA,IAC1B,UAAU,aAAa;AAAA,EACzB,IACA,aAAa;AACnB;;;ACxJA,IAAM,wBAAwB;AAC9B,IAAM,wBACJ;AAEK,IAAM,mBAAmB;AAAA,EAC9B,OACE,cAI4D;AAC5D,WAAO,EAAE,cAAc,cAAc;AAAA,EACvC;AAAA,EAEA,KAAK;AAAA,IACH;AAAA,IACA;AAAA,EACF,IAGI,CAAC,GAAoE;AACvE,WAAO;AAAA,MACL,cAAc,CACZ,QACA,YACI;AAAA,QACJ,QAAQ,mBAAmB,EAAE,QAAQ,cAAc,aAAa,CAAC;AAAA,QACjE,aAAa;AAAA,MACf;AAAA,MACA;AAAA,MACA,YAAY,CAAC,UAAU,MAAM,sBAAsB;AAAA,MACnD,gBAAgB,CAAC,EAAE,OAAO,OAAO,MAAM,MAAM,eAAe,MAAM;AAAA,IACpE;AAAA,EACF;AAAA,EAEA,YAAY;AAAA,IACV;AAAA,IACA;AAAA,EACF,IAGI,CAAC,GAGH;AACA,WAAO;AAAA,MACL,cAAc,CACZ,QACA,YACI;AAAA,QACJ,QAAQ,mBAAmB;AAAA,UACzB,sBAAsB,OAAO;AAAA,UAC7B;AAAA,UACA;AAAA,UACA;AAAA,QACF,CAAC;AAAA,QACD,aAAa,OAAO;AAAA,MACtB;AAAA,MACA;AAAA,MACA,YAAY,CAAC,UAAU,MAAM,sBAAsB;AAAA,MACnD,gBAAgB,CAAC,EAAE,OAAO,OAAO,MAAM,MAAM,eAAe,MAAM;AAAA,IACpE;AAAA,EACF;AACF;AAEA,SAAS,mBAAmB;AAAA,EAC1B;AAAA,EACA;AAAA,EACA,eAAe;AAAA,EACf,eAAe;AACjB,GAKG;AACD,SAAO;AAAA,IACL;AAAA,IACA,wBAAwB,OAAO,KAAK;AAAA,IACpC;AAAA,IACA,KAAK,UAAU,OAAO,cAAc,CAAC;AAAA,IACrC;AAAA,EACF,EACG,OAAO,OAAO,EACd,KAAK,IAAI;AACd;AAEA,SAAS,cAAc,UAA2B;AAChD,SAAO,UAAU,EAAE,MAAM,SAAS,CAAC;AACrC;;;AC1FO,SAAS,gBAAgB,MAAW,MAAoB;AAE7D,MAAI,SAAS;AAAM,WAAO;AAG1B,MAAI,QAAQ,QAAQ,QAAQ;AAAM,WAAO;AAGzC,MAAI,OAAO,SAAS,YAAY,OAAO,SAAS;AAC9C,WAAO,SAAS;AAGlB,MAAI,KAAK,gBAAgB,KAAK;AAAa,WAAO;AAGlD,MAAI,gBAAgB,QAAQ,gBAAgB,MAAM;AAChD,WAAO,KAAK,QAAQ,MAAM,KAAK,QAAQ;AAAA,EACzC;AAGA,MAAI,MAAM,QAAQ,IAAI,GAAG;AACvB,QAAI,KAAK,WAAW,KAAK;AAAQ,aAAO;AACxC,aAAS,IAAI,GAAG,IAAI,KAAK,QAAQ,KAAK;AACpC,UAAI,CAAC,gBAAgB,KAAK,CAAC,GAAG,KAAK,CAAC,CAAC;AAAG,eAAO;AAAA,IACjD;AACA,WAAO;AAAA,EACT;AAGA,QAAM,QAAQ,OAAO,KAAK,IAAI;AAC9B,QAAM,QAAQ,OAAO,KAAK,IAAI;AAC9B,MAAI,MAAM,WAAW,MAAM;AAAQ,WAAO;AAG1C,aAAW,OAAO,OAAO;AACvB,QAAI,CAAC,MAAM,SAAS,GAAG;AAAG,aAAO;AACjC,QAAI,CAAC,gBAAgB,KAAK,GAAG,GAAG,KAAK,GAAG,CAAC;AAAG,aAAO;AAAA,EACrD;AAEA,SAAO;AACT;;;AC4BA,eAAsB,aAA6B;AAAA,EACjD;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA,GAAG;AACL,GAeE;AAEA,QAAM,iBACJ,OAAO,WAAW,cAAc,CAAC,iBAAiB,MAAM,IACnD,OAA8C,MAAM,IACrD;AAEN,QAAM,iBAAiB,MAAM,aAAa,cAAc;AAExD,MAAI,kBAAkB;AACtB,MAAI,uBAAuB;AAC3B,MAAI;AAEJ,MAAI;AACJ,MAAI;AACJ,QAAM,gBAAgB,IAAI,QAAgB,CAAC,SAAS,WAAW;AAC7D,oBAAgB;AAChB,mBAAe;AAAA,EACjB,CAAC;AAED,QAAM,eAAe,MAAM,kBAQzB;AAAA,IACA,cAAc;AAAA,IACd,OAAO;AAAA,MACL;AAAA,MACA,GAAG;AAAA,IACL;AAAA,IACA;AAAA,IACA;AAAA,IACA,aAAa,OAAOC,aAClB,MAAM,eAAe,QAAQ,eAAe,QAAQA,QAAO;AAAA,IAE7D,aAAa,OAAO;AAClB,YAAM,YAAY,MAAM,uBAAuB,MAAM,UAAU;AAE/D,UAAI,aAAa,MAAM;AACrB,eAAO;AAAA,MACT;AAEA,yBAAmB;AACnB,8BAAwB;AAExB,YAAM,gBAAgB,MAAM,2BAA2B,eAAe;AAGtE,UAAI,CAAC,gBAAgB,cAAc,aAAa,GAAG;AACjD,uBAAe;AAGf,cAAM,8BAA8B;AACpC,+BAAuB;AAGvB,eAAO;AAAA,UACL,eAAe;AAAA,UAIf,aAAa;AAAA,UACb,WAAW;AAAA,QACb;AAAA,MACF;AAEA,aAAO;AAAA,IACT;AAAA;AAAA;AAAA,IAIA,kBAAkB;AAChB,aAAO;AAAA,QACL,eAAe;AAAA,QAIf,aAAa;AAAA,QACb,WAAW;AAAA,MACb;AAAA,IACF;AAAA,IAEA,SAAS;AAEP,YAAM,cAAc,OAAO,SAAS,YAAY;AAEhD,UAAI,YAAY,SAAS;AACvB,sBAAc,YAAY,KAAK;AAAA,MACjC,OAAO;AACL,qBAAa,YAAY,KAAK;AAAA,MAChC;AAAA,IACF;AAAA,EACF,CAAC;AAED,SAAO,eACH;AAAA,IACE,cAAc,aAAa;AAAA,IAC3B;AAAA,IACA,UAAU,aAAa;AAAA,EACzB,IACA,aAAa;AACnB;;;ACxMO,IAAM,qBAAN,cAAiC,MAAM;AAAA,EACnC;AAAA,EACA;AAAA,EACA;AAAA,EAET,YAAY;AAAA,IACV;AAAA,IACA;AAAA,IACA;AAAA,EACF,GAIG;AACD;AAAA,MACE,iCAAiC,QAAQ,aAC7B,SAAS;AAAA,iBACD,gBAAgB,KAAK,CAAC;AAAA,IAC5C;AAEA,SAAK,OAAO;AAEZ,SAAK,WAAW;AAChB,SAAK,QAAQ;AACb,SAAK,YAAY;AAAA,EACnB;AAAA,EAEA,SAAS;AACP,WAAO;AAAA,MACL,MAAM,KAAK;AAAA,MACX,OAAO,KAAK;AAAA,MACZ,SAAS,KAAK;AAAA,MACd,OAAO,KAAK;AAAA,MAEZ,UAAU,KAAK;AAAA,MACf,WAAW,KAAK;AAAA,IAClB;AAAA,EACF;AACF;;;AC3BO,IAAM,8BAAN,MAAM,6BAKb;AAAA,EACmB;AAAA,EACA;AAAA,EAKjB,YAAY;AAAA,IACV;AAAA,IACA;AAAA,EACF,GAGG;AACD,SAAK,QAAQ;AACb,SAAK,WAAW;AAAA,EAClB;AAAA,EAEA,IAAI,mBAAmB;AACrB,WAAO,KAAK,MAAM;AAAA,EACpB;AAAA,EAEA,IAAI,WAAW;AACb,WAAO,KAAK,MAAM;AAAA,EACpB;AAAA,EAEA,IAAI,mBAA+C;AACjD,WAAO,KAAK,MAAM;AAAA,EACpB;AAAA,EAEA,uBAAuB,QAA8C;AACnE,QAAI,KAAK,SAAS,kBAAkB,MAAM;AACxC,aAAO,KAAK,SAAS,eAAe;AAAA,QAClC,OAAO,KAAK;AAAA,QACZ;AAAA,MACF,CAAC;AAAA,IACH;AAEA,WAAO,KAAK;AAAA,EACd;AAAA,EAEA,MAAM,mBACJ,MACA,QACA,SACA;AACA,UAAM,EAAE,aAAa,MAAAC,QAAM,SAAS,IAAI,MAAM,aAAa;AAAA,MACzD,OAAO,KAAK,uBAAuB,KAAK,UAAU;AAAA,MAClD,QAAQ,KAAK,SAAS,aAAa,QAAQ,IAAI;AAAA,MAC/C,cAAc;AAAA,MACd,GAAG;AAAA,IACL,CAAC;AAED,QAAI;AACF,aAAO;AAAA,QACL;AAAA,QACA,UAAU,KAAK,SAAS,gBAAgBA,QAAM,IAAI;AAAA,QAClD,OAAO,UAAU;AAAA,MAOnB;AAAA,IACF,SAAS,OAAO;AACd,YAAM,IAAI,mBAAmB;AAAA,QAC3B,UAAU,KAAK;AAAA,QACf,WAAWA;AAAA,QACX,OAAO;AAAA,MACT,CAAC;AAAA,IACH;AAAA,EACF;AAAA,EAEA,aAAa,oBAAsD;AACjE,WAAO,IAAI,6BAA4B;AAAA,MACrC,OAAO,KAAK,MAAM,aAAa,kBAAkB;AAAA,MACjD,UAAU,KAAK;AAAA,IACjB,CAAC;AAAA,EACH;AACF;;;AChGO,IAAM,sBAAN,cAAkC,MAAM;AAAA,EACpC;AAAA,EACA;AAAA,EAET,YAAY,EAAE,WAAW,MAAM,GAA0C;AACvE;AAAA,MACE,qCACY,SAAS;AAAA,iBACD,gBAAgB,KAAK,CAAC;AAAA,IAC5C;AAEA,SAAK,OAAO;AAEZ,SAAK,QAAQ;AACb,SAAK,YAAY;AAAA,EACnB;AAAA,EAEA,SAAS;AACP,WAAO;AAAA,MACL,MAAM,KAAK;AAAA,MACX,OAAO,KAAK;AAAA,MACZ,SAAS,KAAK;AAAA,MACd,OAAO,KAAK;AAAA,MAEZ,WAAW,KAAK;AAAA,IAClB;AAAA,EACF;AACF;;;AClBO,IAAM,+BAAN,MAAM,8BAQb;AAAA,EACmB;AAAA,EACA;AAAA,EAKjB,YAAY;AAAA,IACV;AAAA,IACA;AAAA,EACF,GAGG;AACD,SAAK,QAAQ;AACb,SAAK,WAAW;AAAA,EAClB;AAAA,EAEA,IAAI,mBAAmB;AACrB,WAAO,KAAK,MAAM;AAAA,EACpB;AAAA,EAEA,IAAI,WAAW;AACb,WAAO,KAAK,MAAM;AAAA,EACpB;AAAA,EAEA,IAAI,mBAA+C;AACjD,WAAO,KAAK,MAAM;AAAA,EACpB;AAAA,EAEA,MAAM,oBACJ,OACA,QACA,SACA;AACA,UAAM;AAAA,MACJ;AAAA,MACA,MAAM;AAAA,MACN;AAAA,IACF,IAAI,MAAM,aAAa;AAAA,MACrB,OAAO,KAAK;AAAA,MACZ,QAAQ,KAAK,SAAS,aAAa,QAAQ,KAAK;AAAA,MAChD,cAAc;AAAA,MACd,GAAG;AAAA,IACL,CAAC;AAED,QAAI;AACF,YAAM,EAAE,MAAAC,QAAM,UAAU,IACtB,KAAK,SAAS,wBAAwB,aAAa;AAErD,aAAO;AAAA,QACL;AAAA,QACA,MAAAA;AAAA,QACA;AAAA,QACA,OAAO,UAAU;AAAA,MAOnB;AAAA,IACF,SAAS,OAAO;AACd,YAAM,IAAI,oBAAoB;AAAA,QAC5B,WAAW;AAAA,QACX,OAAO;AAAA,MACT,CAAC;AAAA,IACH;AAAA,EACF;AAAA,EAEA,aAAa,oBAAsD;AACjE,WAAO,IAAI,8BAA6B;AAAA,MACtC,OAAO,KAAK,MAAM,aAAa,kBAAkB;AAAA,MACjD,UAAU,KAAK;AAAA,IACjB,CAAC;AAAA,EACH;AACF;;;AChFO,IAAM,oCAAN,MAAM,mCAMb;AAAA,EACW;AAAA,EACA;AAAA,EAET,YAAY;AAAA,IACV;AAAA,IACA;AAAA,EACF,GAGG;AACD,SAAK,QAAQ;AACb,SAAK,iBAAiB;AAAA,EACxB;AAAA,EAEA,IAAI,mBAAmB;AACrB,WAAO,KAAK,MAAM;AAAA,EACpB;AAAA,EAEA,IAAI,WAAW;AACb,WAAO,KAAK,MAAM;AAAA,EACpB;AAAA,EAEA,IAAI,YAAgC;AAClC,WAAO,KAAK,MAAM;AAAA,EACpB;AAAA,EAEA,IAAI,oBAAgD;AAClD,WAAO,KAAK,MAAM;AAAA,EACpB;AAAA,EAEA,IAAI,oBAEwC;AAC1C,UAAM,4BAA4B,KAAK,MAAM,mBAAmB;AAAA,MAC9D,KAAK;AAAA,IACP;AAEA,QAAI,8BAA8B,QAAW;AAC3C,aAAO;AAAA,IAGT;AAEA,WAAQ,CAAC,WACP;AAAA,MACE,KAAK,eAAe,OAAO,MAAM;AAAA,IACnC;AAAA,EAGJ;AAAA,EAEA,gBAAgB,QAAgB,SAA+B;AAC7D,UAAM,eAAe,KAAK,eAAe,OAAO,MAAM;AACtD,WAAO,KAAK,MAAM,gBAAgB,cAAc,OAAO;AAAA,EACzD;AAAA,EAEA,sBAAsB,aAAsB;AAC1C,WAAO,KAAK,MAAM,sBAAsB,WAAW;AAAA,EACrD;AAAA,EAEA,IAAI,mBAAsC;AACxC,WAAO,KAAK,MAAM;AAAA,EACpB;AAAA,EAEA,0BACE,gBACA;AACA,WAAO,IAAI,4BAA4B;AAAA,MACrC,OAAO;AAAA,MACP,UAAU;AAAA,IACZ,CAAC;AAAA,EACH;AAAA,EAEA,iCACE,gBACA;AACA,WAAO,IAAI,6BAA6B;AAAA,MACtC,OAAO;AAAA,MACP,UAAU;AAAA,IACZ,CAAC;AAAA,EACH;AAAA,EAEA,wBACE,gBACA;AACA,WAAO,IAAI,8BAA8B;AAAA,MACvC,OAAO;AAAA,MACP,UAAU;AAAA,IACZ,CAAC;AAAA,EACH;AAAA,EAEA,eAAe,QAAoD;AACjE,WAAO,IAAI,mCAAkC;AAAA,MAC3C,OAAO,KAAK,MAAM,eAAe,MAAM;AAAA,MACvC,gBAAgB,KAAK;AAAA,IACvB,CAAC;AAAA,EACH;AAAA,EAEA,aAAa,oBAA6C;AACxD,WAAO,IAAI,mCAAkC;AAAA,MAC3C,OAAO,KAAK,MAAM,aAAa,kBAAkB;AAAA,MACjD,gBAAgB,KAAK;AAAA,IACvB,CAAC;AAAA,EACH;AACF;;;AClHO,IAAM,mCAAN,MAAM,0CAMH,kCAOV;AAAA,EACE,YAAY,SAGT;AACD,UAAM,OAAO;AAAA,EACf;AAAA,EAEA,aAAa,QAAgB,SAA+B;AAC1D,UAAM,eAAe,KAAK,eAAe,OAAO,MAAM;AACtD,WAAO,KAAK,MAAM,aAAa,cAAc,OAAO;AAAA,EACtD;AAAA,EAEA,iBAAiB,OAAgB;AAC/B,WAAO,KAAK,MAAM,iBAAiB,KAAK;AAAA,EAC1C;AAAA,EAEA,wBACE,gBACA;AACA,WAAO,IAAI,6BAA6B;AAAA,MACtC,OAAO;AAAA,MACP,UAAU;AAAA,IACZ,CAAC;AAAA,EACH;AAAA,EAEA,eAAe,QAAoD;AACjE,WAAO,IAAI,kCAAiC;AAAA,MAC1C,OAAO,KAAK,MAAM,eAAe,MAAM;AAAA,MACvC,gBAAgB,KAAK;AAAA,IACvB,CAAC;AAAA,EACH;AAAA,EAEA,aAAa,oBAA6C;AACxD,WAAO,IAAI,kCAAiC;AAAA,MAC1C,OAAO,KAAK,MAAM,aAAa,kBAAkB;AAAA,MACjD,gBAAgB,KAAK;AAAA,IACvB,CAAC;AAAA,EACH;AACF;;;ACrDO,IAAM,8BAAN,MAAM,qCAQH,iCAUV;AAAA,EACE,YAAY,SAGT;AACD,UAAM,OAAO;AAAA,EACf;AAAA,EAEA,mBACE,MACA,QACA,SAOC;AACD,UAAM,eAAe,KAAK,eAAe,OAAO,MAAM;AACtD,WAAO,KAAK,MAAM,mBAAmB,MAAM,cAAc,OAAO;AAAA,EAClE;AAAA,EAEA,oBACE,OACA,QACA,SAQC;AACD,UAAM,eAAe,KAAK,eAAe,OAAO,MAAM;AACtD,WAAO,KAAK,MAAM,oBAAoB,OAAO,cAAc,OAAO;AAAA,EACpE;AAAA,EAEA,aAAa,oBAA6C;AACxD,WAAO,IAAI,6BAA4B;AAAA,MACrC,OAAO,KAAK,MAAM,aAAa,kBAAkB;AAAA,MACjD,gBAAgB,KAAK;AAAA,IACvB,CAAC;AAAA,EACH;AACF;;;AC/DO,IAAM,gCAAgC;AAAA,EAC3C;AAAA,EACA;AAAA,EACA;AAAA,EACA;AACF;;;AChBA;AAAA;AAAA;AAAA;AAAA;AAAA;;;ACGO,IAAM,qBAAN,cAAiC,MAAM;AAAA,EACnC;AAAA,EAET,YAAY,SAAiB,QAAiB;AAC5C,UAAM,OAAO;AAEb,SAAK,OAAO;AACZ,SAAK,SAAS;AAAA,EAChB;AAAA,EAEA,SAAS;AACP,WAAO;AAAA,MACL,MAAM,KAAK;AAAA,MACX,SAAS,KAAK;AAAA,MACd,OAAO,KAAK;AAAA,MAEZ,QAAQ,KAAK;AAAA,IACf;AAAA,EACF;AACF;;;ACmBO,SAAS,wBACd,SACA,QACQ;AACR,MAAI,OAAO,YAAY,UAAU;AAC/B,UAAM,IAAI;AAAA,MACR;AAAA,MACA;AAAA,IACF;AAAA,EACF;AAEA,SAAO;AACT;;;AFhDA,IAAM,8BACJ;AACF,IAAM,iCACJ;AAKK,SAAS,OAAqD;AACnE,SAAO;AAAA,IACL,eAAe,CAAC;AAAA,IAChB,OAAO,QAAQ;AACb,UAAIC,SAAO;AACX,MAAAA,UAAQ;AACR,MAAAA,UAAQ;AACR,MAAAA,UAAQ;AAER,aAAOA;AAAA,IACT;AAAA,EACF;AACF;AAsCO,SAAS,cAGd;AACA,SAAO;AAAA,IACL,eAAe,CAAC;AAAA,IAChB,OAAO,QAAQ;AACb,UAAIA,SACF,OAAO,WACN,OAAO,SAAS,OACb,8BACA;AAEN,MAAAA,UAAQ;AAER,UAAI,OAAO,UAAU,MAAM;AACzB,QAAAA,UAAQ,GAAG,OAAO,MAAM;AAAA;AAAA,MAC1B;AAEA,MAAAA,UAAQ,wBAAwB,OAAO,aAAa,MAAM;AAE1D,UAAI,OAAO,SAAS,MAAM;AACxB,QAAAA,UAAQ;AAAA;AAAA;AAAA,EAAmB,OAAO,KAAK;AAAA,MACzC;AAEA,MAAAA,UAAQ;AAER,UAAI,OAAO,kBAAkB,MAAM;AACjC,QAAAA,UAAQ,GAAG,OAAO,cAAc;AAAA;AAAA,MAClC;AAEA,aAAOA;AAAA,IACT;AAAA,EACF;AACF;AAKO,SAAS,OAAyD;AACvE,QAAM,IAAI,MAAM,sDAAsD;AACxE;;;AGxGA;AAAA;AAAA,cAAAC;AAAA,EAAA,mBAAAC;AAAA,EAAA,YAAAC;AAAA;AAMA,IAAM,gBAAgB;AACtB,IAAM,cAAc;AAEpB,SAAS,aAAa,MAAuC;AAC3D,SAAO,GAAG,aAAa,GAAG,IAAI;AAAA;AAChC;AAEA,SAAS,QACP,MACAC,QACA;AACA,SAAOA,UAAQ,OAAO,KAAK,GAAG,aAAa,IAAI,CAAC,GAAGA,MAAI,GAAG,WAAW;AAAA;AACvE;AAKO,SAASA,QAAqD;AACnE,SAAO;AAAA,IACL,eAAe,CAAC,WAAW;AAAA,IAC3B,OAAO,QAAQ;AAEb,aAAO,QAAQ,QAAQ,MAAM,IAAI,aAAa,WAAW;AAAA,IAC3D;AAAA,EACF;AACF;AAeO,SAASC,eAGd;AACA,SAAO;AAAA,IACL,eAAe,CAAC,WAAW;AAAA,IAC3B,OAAO,QAAQ;AACb,YAAMA,gBAAc,wBAAwB,OAAO,aAAa,MAAM;AAEtE,aACE,QAAQ,UAAU,OAAO,MAAM,IAC/B,QAAQ,QAAQA,aAAW,IAC3B,aAAa,WAAW,KACvB,OAAO,kBAAkB;AAAA,IAE9B;AAAA,EACF;AACF;AAeO,SAASC,QAAyD;AACvE,SAAO;AAAA,IACL,OAAO,QAAQ;AACb,UAAIF,SAAO,OAAO,UAAU,OAAO,QAAQ,UAAU,OAAO,MAAM,IAAI;AAEtE,iBAAW,EAAE,MAAM,QAAQ,KAAK,OAAO,UAAU;AAC/C,gBAAQ,MAAM;AAAA,UACZ,KAAK,QAAQ;AACX,YAAAA,UAAQ,QAAQ,QAAQ,wBAAwB,SAAS,MAAM,CAAC;AAChE;AAAA,UACF;AAAA,UACA,KAAK,aAAa;AAChB,YAAAA,UAAQ;AAAA,cACN;AAAA,cACA,wBAAwB,SAAS,MAAM;AAAA,YACzC;AACA;AAAA,UACF;AAAA,UACA,KAAK,QAAQ;AACX,kBAAM,IAAI;AAAA,cACR;AAAA,cACA;AAAA,YACF;AAAA,UACF;AAAA,UACA,SAAS;AACP,kBAAM,mBAA0B;AAChC,kBAAM,IAAI,MAAM,qBAAqB,gBAAgB,EAAE;AAAA,UACzD;AAAA,QACF;AAAA,MACF;AAGA,MAAAA,UAAQ,aAAa,WAAW;AAEhC,aAAOA;AAAA,IACT;AAAA,IACA,eAAe,CAAC,WAAW;AAAA,EAC7B;AACF;;;AC/DO,IAAM,cAAc;AAAA,EACzB,KAAK,EAAE,MAAAG,OAAK,GAAkC;AAC5C,WAAO;AAAA,MACL,MAAM;AAAA,MACN,SAASA;AAAA,IACX;AAAA,EACF;AAAA,EAEA,KAAK;AAAA,IACH;AAAA,EACF,GAEgB;AACd,WAAO;AAAA,MACL,MAAM;AAAA,MACN,SAAS,kBAAkB,EAAE,YAAY,CAAC;AAAA,IAC5C;AAAA,EACF;AAAA,EAEA,UAAU;AAAA,IACR,MAAAA;AAAA,IACA;AAAA,EACF,GAGgB;AACd,WAAO;AAAA,MACL,MAAM;AAAA,MACN,SAAS,uBAAuB,EAAE,MAAAA,QAAM,YAAY,CAAC;AAAA,IACvD;AAAA,EACF;AACF;AAEA,SAAS,kBAAkB;AAAA,EACzB;AACF,GAEG;AACD,QAAM,cAA2B,CAAC;AAElC,aAAW,EAAE,QAAQ,SAAS,KAAK,eAAe,CAAC,GAAG;AACpD,gBAAY,KAAK;AAAA,MACf,MAAM;AAAA,MACN,IAAI,SAAS;AAAA,MACb,UAAU;AAAA,IACZ,CAAC;AAAA,EACH;AAEA,SAAO;AACT;AAEA,SAAS,uBAAuB;AAAA,EAC9B,MAAAA;AAAA,EACA;AACF,GAGG;AACD,QAAM,UAA4B,CAAC;AAEnC,MAAIA,UAAQ,MAAM;AAChB,YAAQ,KAAK,EAAE,MAAM,QAAQ,MAAAA,OAAK,CAAC;AAAA,EACrC;AAEA,aAAW,EAAE,SAAS,KAAK,eAAe,CAAC,GAAG;AAC5C,YAAQ,KAAK,EAAE,MAAM,aAAa,GAAG,SAAS,CAAC;AAAA,EACjD;AAEA,SAAO;AACT;AAEO,SAAS,iBACd,gBACqD;AACrD,SAAO,CAAC,UACN,qBAAqB,aAAa;AAAA,IAChC;AAAA,IACA,QAAQ,MAAM,eAAe,KAAK;AAAA,EACpC,EAAE;AACN;;;AC3FO,SAAS,wBACd,gBAC4D;AAC5D,SAAO,CAAC,UACN,qBAAqB,aAAa;AAAA,IAChC;AAAA,IACA,QAAQ,MAAM,eAAe,KAAK;AAAA,EACpC,EAAE;AACN;;;ACjDA;AAAA;AAAA,cAAAC;AAAA,EAAA,mBAAAC;AAAA,EAAA,YAAAC;AAAA,EAAA;AAAA;AAOA,IAAM,gBAAgB;AACtB,IAAMC,eAAc;AACpB,IAAM,oBAAoB;AAC1B,IAAM,kBAAkB;AACxB,IAAM,eAAe;AACrB,IAAM,aAAa;AAYZ,SAASC,QAAqD;AACnE,SAAO;AAAA,IACL,eAAe,CAACD,YAAW;AAAA,IAC3B,OAAO,QAAQ;AACb,aAAO,GAAG,aAAa,GAAG,iBAAiB,GAAG,MAAM,GAAG,eAAe;AAAA,IACxE;AAAA,EACF;AACF;AAiBO,SAASE,eAGd;AACA,SAAO;AAAA,IACL,eAAe,CAACF,YAAW;AAAA,IAC3B,OAAO,QAAQ;AACb,YAAME,gBAAc,wBAAwB,OAAO,aAAa,MAAM;AAEtE,aAAO,GAAG,aAAa,GAAG,iBAAiB,GACzC,OAAO,UAAU,OACb,GAAG,YAAY,GAAG,OAAO,MAAM,GAAG,UAAU,KAC5C,EACN,GAAGA,aAAW,GAAG,eAAe,GAAG,OAAO,kBAAkB,EAAE;AAAA,IAChE;AAAA,EACF;AACF;AAcO,SAASC,QAAyD;AACvE,SAAO;AAAA,IACL,OAAO,QAAQ;AACb,2BAAqB,MAAM;AAG3B,YAAM,UAAU,OAAO,SAAS,CAAC,EAAE;AAEnC,UAAIF,SAAO,GAAG,aAAa,GAAG,iBAAiB,GAC7C,OAAO,UAAU,OACb,GAAG,YAAY,GAAG,OAAO,MAAM,GAAG,UAAU,KAC5C,EACN,GAAG,OAAO,GAAG,eAAe;AAG5B,eAAS,IAAI,GAAG,IAAI,OAAO,SAAS,QAAQ,KAAK;AAC/C,cAAM,EAAE,MAAM,SAAAG,SAAQ,IAAI,OAAO,SAAS,CAAC;AAC3C,gBAAQ,MAAM;AAAA,UACZ,KAAK,QAAQ;AACX,kBAAM,cAAc,wBAAwBA,UAAS,MAAM;AAC3D,YAAAH,UAAQ,GAAG,aAAa,GAAG,iBAAiB,GAAG,WAAW,GAAG,eAAe;AAC5E;AAAA,UACF;AAAA,UACA,KAAK,aAAa;AAChB,YAAAA,UAAQ,GAAG,wBAAwBG,UAAS,MAAM,CAAC,GAAGJ,YAAW;AACjE;AAAA,UACF;AAAA,UACA,KAAK,QAAQ;AACX,kBAAM,IAAI;AAAA,cACR;AAAA,cACA;AAAA,YACF;AAAA,UACF;AAAA,UACA,SAAS;AACP,kBAAM,mBAA0B;AAChC,kBAAM,IAAI,MAAM,qBAAqB,gBAAgB,EAAE;AAAA,UACzD;AAAA,QACF;AAAA,MACF;AAEA,aAAOC;AAAA,IACT;AAAA,IACA,eAAe,CAACD,YAAW;AAAA,EAC7B;AACF;AAaO,SAAS,qBAAqB,YAAwB;AAC3D,QAAM,WAAW,WAAW;AAE5B,MAAI,SAAS,SAAS,GAAG;AACvB,UAAM,IAAI;AAAA,MACR;AAAA,MACA;AAAA,IACF;AAAA,EACF;AAEA,WAAS,IAAI,GAAG,IAAI,SAAS,QAAQ,KAAK;AACxC,UAAM,eAAe,IAAI,MAAM,IAAI,SAAS;AAC5C,UAAM,OAAO,SAAS,CAAC,EAAE;AAEzB,QAAI,SAAS,cAAc;AACzB,YAAM,IAAI;AAAA,QACR,oBAAoB,CAAC,sBAAsB,YAAY,oBAAoB,IAAI;AAAA,QAC/E;AAAA,MACF;AAAA,IACF;AAAA,EACF;AAEA,MAAI,SAAS,SAAS,MAAM,GAAG;AAC7B,UAAM,IAAI;AAAA,MACR;AAAA,MACA;AAAA,IACF;AAAA,EACF;AACF;;;ACnKA;AAAA;AAAA,cAAAK;AAAA,EAAA,mBAAAC;AAAA,EAAA,YAAAC;AAAA,EAAA;AAAA;AAMA,IAAMC,iBAAgB;AACtB,IAAMC,eAAc;AACpB,IAAMC,qBAAoB;AAC1B,IAAMC,mBAAkB;AAWjB,SAASC,QAAqD;AACnE,SAAO;AAAA,IACL,eAAe,CAACH,YAAW;AAAA,IAC3B,OAAO,QAAQ;AACb,aAAO,GAAGD,cAAa,GAAGE,kBAAiB,GAAG,MAAM,GAAGC,gBAAe;AAAA,IACxE;AAAA,EACF;AACF;AAmBO,SAASE,eAGd;AACA,SAAO;AAAA,IACL,eAAe,CAACJ,YAAW;AAAA,IAC3B,OAAO,QAAQ;AACb,YAAMI,gBAAc,wBAAwB,OAAO,aAAa,MAAM;AAEtE,UAAI,OAAO,UAAU,MAAM;AACzB,eAAO,GAAGL,cAAa,GAAGE,kBAAiB,GACzC,OAAO,MACT,GAAGC,gBAAe,GAAGF,YAAW,GAAGC,kBAAiB,GAAGG,aAAW,GAAGF,gBAAe,GAClF,OAAO,kBAAkB,EAC3B;AAAA,MACF;AAEA,aAAO,GAAGH,cAAa,GAAGE,kBAAiB,GAAGG,aAAW,GAAGF,gBAAe,GACzE,OAAO,kBAAkB,EAC3B;AAAA,IACF;AAAA,EACF;AACF;AAmBO,SAASG,QAAyD;AACvE,SAAO;AAAA,IACL,OAAO,QAAQ;AACb,4BAAsB,MAAM;AAE5B,UAAIF,SAAO;AACX,UAAI,IAAI;AAGR,UAAI,OAAO,UAAU,MAAM;AACzB,QAAAA,UAAQ,GAAGJ,cAAa,GAAGE,kBAAiB,GAAG,OAAO,MAAM,GAAGC,gBAAe,GAAGF,YAAW;AAAA,MAC9F,OAAO;AAEL,QAAAG,SAAO,GAAGJ,cAAa,GAAGE,kBAAiB,GAAG,OAAO,SAAS,CAAC,EAAE,OAAO,GAAGC,gBAAe;AAG1F,YAAI,OAAO,SAAS,SAAS,GAAG;AAC9B,UAAAC,UAAQ,GAAG,OAAO,SAAS,CAAC,EAAE,OAAO,GAAGH,YAAW;AAAA,QACrD;AAEA,YAAI;AAAA,MACN;AAGA,aAAO,IAAI,OAAO,SAAS,QAAQ,KAAK;AACtC,cAAM,EAAE,MAAM,QAAQ,IAAI,OAAO,SAAS,CAAC;AAC3C,gBAAQ,MAAM;AAAA,UACZ,KAAK,QAAQ;AACX,kBAAM,cAAc,wBAAwB,SAAS,MAAM;AAC3D,YAAAG,UAAQ,GAAGF,kBAAiB,GAAG,WAAW,GAAGC,gBAAe;AAC5D;AAAA,UACF;AAAA,UACA,KAAK,aAAa;AAChB,YAAAC,UAAQ,wBAAwB,SAAS,MAAM;AAC/C;AAAA,UACF;AAAA,UACA,KAAK,QAAQ;AACX,kBAAM,IAAI;AAAA,cACR;AAAA,cACA;AAAA,YACF;AAAA,UACF;AAAA,UACA,SAAS;AACP,kBAAM,mBAA0B;AAChC,kBAAM,IAAI,MAAM,qBAAqB,gBAAgB,EAAE;AAAA,UACzD;AAAA,QACF;AAAA,MACF;AAEA,aAAOA;AAAA,IACT;AAAA,IACA,eAAe,CAACH,YAAW;AAAA,EAC7B;AACF;AAaO,SAAS,sBAAsB,YAAwB;AAC5D,QAAM,WAAW,WAAW;AAE5B,MAAI,SAAS,SAAS,GAAG;AACvB,UAAM,IAAI;AAAA,MACR;AAAA,MACA;AAAA,IACF;AAAA,EACF;AAEA,WAAS,IAAI,GAAG,IAAI,SAAS,QAAQ,KAAK;AACxC,UAAM,eAAe,IAAI,MAAM,IAAI,SAAS;AAC5C,UAAM,OAAO,SAAS,CAAC,EAAE;AAEzB,QAAI,SAAS,cAAc;AACzB,YAAM,IAAI;AAAA,QACR,oBAAoB,CAAC,sBAAsB,YAAY,oBAAoB,IAAI;AAAA,QAC/E;AAAA,MACF;AAAA,IACF;AAAA,EACF;AAEA,MAAI,SAAS,SAAS,MAAM,GAAG;AAC7B,UAAM,IAAI;AAAA,MACR;AAAA,MACA;AAAA,IACF;AAAA,EACF;AACF;;;ACrLA;AAAA;AAAA,cAAAM;AAAA,EAAA,mBAAAC;AAAA,EAAA,YAAAC;AAAA;AAMA,IAAM,YAAY;AAAA,EAChB,QAAQ;AAAA,EACR,MAAM;AAAA,EACN,WAAW;AACb;AAEA,SAASC,cAAa,MAAuC;AAC3D,SAAO,OAAO,UAAU,IAAI,CAAC;AAAA;AAC/B;AAEA,SAASC,SACP,MACAC,QACA;AACA,SAAOA,UAAQ,OAAO,KAAK,GAAGF,cAAa,IAAI,CAAC,GAAGE,MAAI;AAAA;AACzD;AAOO,SAASA,QAAqD;AACnE,SAAO;AAAA,IACL,eAAe,CAAC;AAAA,IAChB,OAAO,QAAQ;AAEb,aAAOD,SAAQ,QAAQ,MAAM,IAAID,cAAa,WAAW;AAAA,IAC3D;AAAA,EACF;AACF;AAOO,IAAMG,eAGT,OAAO;AAAA,EACT,eAAe,CAAC;AAAA,EAChB,OAAO,QAAQ;AACb,UAAMA,gBAAc,wBAAwB,OAAO,aAAa,MAAM;AAEtE,WACEF,SAAQ,UAAU,OAAO,MAAM,IAC/BA,SAAQ,QAAQE,aAAW,IAC3BH,cAAa,WAAW,KACvB,OAAO,kBAAkB;AAAA,EAE9B;AACF;AASO,SAASI,QAAyD;AACvE,SAAO;AAAA,IACL,OAAO,QAAQ;AACb,UAAIF,SAAO,OAAO,UAAU,OAAOD,SAAQ,UAAU,OAAO,MAAM,IAAI;AAEtE,iBAAW,EAAE,MAAM,QAAQ,KAAK,OAAO,UAAU;AAC/C,gBAAQ,MAAM;AAAA,UACZ,KAAK,QAAQ;AACX,kBAAM,cAAc,wBAAwB,SAAS,MAAM;AAC3D,YAAAC,UAAQD,SAAQ,QAAQ,WAAW;AACnC;AAAA,UACF;AAAA,UACA,KAAK,aAAa;AAChB,YAAAC,UAAQD;AAAA,cACN;AAAA,cACA,wBAAwB,SAAS,MAAM;AAAA,YACzC;AACA;AAAA,UACF;AAAA,UACA,KAAK,QAAQ;AACX,kBAAM,IAAI;AAAA,cACR;AAAA,cACA;AAAA,YACF;AAAA,UACF;AAAA,UACA,SAAS;AACP,kBAAM,mBAA0B;AAChC,kBAAM,IAAI,MAAM,qBAAqB,gBAAgB,EAAE;AAAA,UACzD;AAAA,QACF;AAAA,MACF;AAGA,MAAAC,UAAQF,cAAa,WAAW;AAEhC,aAAOE;AAAA,IACT;AAAA,IACA,eAAe,CAAC;AAAA,EAAK,UAAU,IAAI,GAAG;AAAA,EACxC;AACF;;;AC1GA;AAAA;AAAA,cAAAG;AAAA,EAAA,mBAAAC;AAAA,EAAA,YAAAC;AAAA;AAeO,IAAMC,QAA2D,OAAO;AAAA,EAC7E,eAAe,CAAC;AAAA,EAChB,QAAQ,CAAC,WAAW,SAAS,MAAM;AAAA;AACrC;AAYO,IAAMC,eAAc,OAGrB;AAAA,EACJ,eAAe,CAAC;AAAA,MAAS;AAAA,EACzB,OAAO,QAAQ;AACb,QAAID,SAAO,OAAO,UAAU,OAAO,WAAW,OAAO,MAAM;AAAA,IAAO;AAElE,IAAAA,UAAQ,SAAS,wBAAwB,OAAO,aAAa,MAAM,CAAC;AAAA;AACpE,IAAAA,UAAQ,cAAc,OAAO,kBAAkB,EAAE;AAEjD,WAAOA;AAAA,EACT;AACF;AAYO,IAAME,QAGT,OAAO;AAAA,EACT,OAAO,QAAQ;AACb,QAAIF,SAAO,OAAO,UAAU,OAAO,WAAW,OAAO,MAAM;AAAA,IAAO;AAElE,eAAW,EAAE,MAAM,QAAQ,KAAK,OAAO,UAAU;AAC/C,cAAQ,MAAM;AAAA,QACZ,KAAK,QAAQ;AACX,UAAAA,UAAQ,SAAS,wBAAwB,SAAS,MAAM,CAAC;AAAA;AACzD;AAAA,QACF;AAAA,QACA,KAAK,aAAa;AAChB,UAAAA,UAAQ,cAAc,wBAAwB,SAAS,MAAM,CAAC;AAAA;AAC9D;AAAA,QACF;AAAA,QACA,KAAK,QAAQ;AACX,gBAAM,IAAI;AAAA,YACR;AAAA,YACA;AAAA,UACF;AAAA,QACF;AAAA,QACA,SAAS;AACP,gBAAM,mBAA0B;AAChC,gBAAM,IAAI,MAAM,qBAAqB,gBAAgB,EAAE;AAAA,QACzD;AAAA,MACF;AAAA,IACF;AAGA,IAAAA,UAAQ;AAER,WAAOA;AAAA,EACT;AAAA,EACA,eAAe,CAAC;AAAA,MAAS;AAC3B;;;ACtFO,SAAS,iBACd,gBACiD;AACjD,SAAO,CAAC,UACN,qBAAqB,aAAa;AAAA,IAChC;AAAA,IACA,QAAQ,MAAM,eAAe,KAAK;AAAA,EACpC,EAAE;AACN;;;ACbA;AAAA;AAAA,cAAAG;AAAA,EAAA,mBAAAC;AAAA,EAAA,YAAAC;AAAA;AASO,IAAMC,QAA2D,OAAO;AAAA,EAC7E,eAAe,CAAC;AAAA,EAChB,QAAQ,CAAC,WAAW;AACtB;AAKO,IAAMC,eAGT,OAAO;AAAA,EACT,eAAe,CAAC;AAAA,EAChB,OAAO,QAAQ;AACb,QAAID,SAAO;AAEX,QAAI,OAAO,UAAU,MAAM;AACzB,MAAAA,UAAQ,GAAG,OAAO,MAAM;AAAA;AAAA;AAAA,IAC1B;AAEA,IAAAA,UAAQ,GAAG,wBAAwB,OAAO,aAAa,MAAM,CAAC;AAAA;AAAA;AAE9D,QAAI,OAAO,kBAAkB,MAAM;AACjC,MAAAA,UAAQ,OAAO;AAAA,IACjB;AAEA,WAAOA;AAAA,EACT;AACF;AASO,IAAME,QAI4C,CAAC;AAAA,EACxD,OAAO;AAAA,EACP,YAAY;AAAA,EACZ;AACF,IAAI,CAAC,OAAO;AAAA,EACV,OAAO,QAAQ;AACb,QAAIF,SACF,OAAO,UAAU,OACb,GAAG,UAAU,OAAO,GAAG,MAAM,MAAM,EAAE,GAAG,OAAO,MAAM;AAAA;AAAA,IACrD;AAEN,eAAW,EAAE,MAAM,QAAQ,KAAK,OAAO,UAAU;AAC/C,cAAQ,MAAM;AAAA,QACZ,KAAK,QAAQ;AACX,UAAAA,UAAQ,GAAG,IAAI;AAAA,EAAM,wBAAwB,SAAS,MAAM,CAAC;AAAA;AAAA;AAC7D;AAAA,QACF;AAAA,QACA,KAAK,aAAa;AAChB,UAAAA,UAAQ,GAAG,SAAS;AAAA,EAAM;AAAA,YACxB;AAAA,YACA;AAAA,UACF,CAAC;AAAA;AAAA;AACD;AAAA,QACF;AAAA,QACA,KAAK,QAAQ;AACX,gBAAM,IAAI;AAAA,YACR;AAAA,YACA;AAAA,UACF;AAAA,QACF;AAAA,QACA,SAAS;AACP,gBAAM,mBAA0B;AAChC,gBAAM,IAAI,MAAM,qBAAqB,gBAAgB,EAAE;AAAA,QACzD;AAAA,MACF;AAAA,IACF;AAGA,IAAAA,UAAQ,GAAG,SAAS;AAAA;AAEpB,WAAOA;AAAA,EACT;AAAA,EACA,eAAe,CAAC;AAAA,EAAK,IAAI,GAAG;AAC9B;;;AC7FA;AAAA;AAAA,cAAAG;AAAA,EAAA,mBAAAC;AAAA,EAAA,YAAAC;AAAA;AAOA,IAAM,yBACJ;AAMK,SAASC,QAAqD;AACnE,SAAO;AAAA,IACL,eAAe,CAAC;AAAA,IAChB,OAAO,QAAQ;AACb,UAAIA,SAAO;AACX,MAAAA,UAAQ;AACR,MAAAA,UAAQ;AACR,MAAAA,UAAQ;AACR,aAAOA;AAAA,IACT;AAAA,EACF;AACF;AAKO,IAAMC,eAAc,OAGrB;AAAA,EACJ,eAAe,CAAC;AAAA,MAAS;AAAA,EACzB,OAAO,QAAQ;AACb,QAAID,SACF,OAAO,UAAU,OACb,GAAG,OAAO,MAAM;AAAA;AAAA,IAChB,GAAG,sBAAsB;AAAA;AAAA;AAE/B,IAAAA,UAAQ,SAAS,wBAAwB,OAAO,aAAa,MAAM,CAAC;AAAA;AACpE,IAAAA,UAAQ;AAER,WAAOA;AAAA,EACT;AACF;AAeO,SAASE,QAAyD;AACvE,SAAO;AAAA,IACL,OAAO,QAAQ;AACb,UAAIF,SACF,OAAO,UAAU,OACb,GAAG,OAAO,MAAM;AAAA;AAAA,IAChB,GAAG,sBAAsB;AAAA;AAAA;AAE/B,iBAAW,EAAE,MAAM,QAAQ,KAAK,OAAO,UAAU;AAC/C,gBAAQ,MAAM;AAAA,UACZ,KAAK,QAAQ;AACX,kBAAM,cAAc,wBAAwB,SAAS,MAAM;AAC3D,YAAAA,UAAQ,SAAS,WAAW;AAAA;AAC5B;AAAA,UACF;AAAA,UACA,KAAK,aAAa;AAChB,YAAAA,UAAQ,cAAc,wBAAwB,SAAS,MAAM,CAAC;AAAA;AAC9D;AAAA,UACF;AAAA,UACA,KAAK,QAAQ;AACX,kBAAM,IAAI;AAAA,cACR;AAAA,cACA;AAAA,YACF;AAAA,UACF;AAAA,UACA,SAAS;AACP,kBAAM,mBAA0B;AAChC,kBAAM,IAAI,MAAM,qBAAqB,gBAAgB,EAAE;AAAA,UACzD;AAAA,QACF;AAAA,MACF;AAGA,MAAAA,UAAQ;AAER,aAAOA;AAAA,IACT;AAAA,IACA,eAAe,CAAC;AAAA,MAAS;AAAA,EAC3B;AACF;;;AClFA,eAAsB,eAAe;AAAA,EACnC;AAAA,EACA;AAAA,EACA,aAAa,MAAM,qBAChB,MAAM,SAAS,uBAAuB,MAAM,oBAAoB;AACrE,GAMwB;AACtB,MAAI,gBAAgB;AAAA,IAClB,QAAQ,OAAO;AAAA,IACf,UAAU,CAAC,OAAO,SAAS,OAAO,SAAS,SAAS,CAAC,CAAC;AAAA;AAAA,EACxD;AAGA,QAAM,mBAAmB,MAAM,MAAM,kBAAkB,aAAa;AAGpE,MAAI,mBAAmB,YAAY;AACjC,WAAO;AAAA,EACT;AAGA,QAAM,gBAAgB,OAAO,SAAS,MAAM,GAAG,EAAE;AAGjD,WAAS,IAAI,cAAc,SAAS,GAAG,KAAK,GAAG,KAAK,GAAG;AACrD,UAAM,mBAAmB,cAAc,CAAC;AACxC,UAAM,cAAc,cAAc,IAAI,CAAC;AAGvC,UAAM,kBAAkB;AAAA,MACtB,QAAQ,OAAO;AAAA,MACf,UAAU,CAAC,aAAa,kBAAkB,GAAG,cAAc,QAAQ;AAAA,IACrE;AACA,UAAM,aAAa,MAAM,MAAM,kBAAkB,eAAe;AAEhE,QAAI,aAAa,YAAY;AAC3B;AAAA,IACF;AAGA,oBAAgB;AAAA,EAClB;AAEA,SAAO;AACT;;;ACjBA,eAAsB,sBAAsB;AAAA,EAC1C;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA,GAAG;AACL,GAOE;AACA,QAAM,QAAQ,EAAE,UAAU,UAAU;AAEpC,QAAM,eAAe,MAAM,oBAAoB;AAAA,IAC7C,cAAc;AAAA,IACd;AAAA,IACA;AAAA,IACA;AAAA,IACA,kBAAkB,OAAOG,aAAY;AACnC,YAAM,SAAS,MAAM,MAAM,aAAa,OAAOA,QAAO;AACtD,aAAO;AAAA,QACL,aAAa,OAAO;AAAA,QACpB,gBAAgB,OAAO;AAAA,MACzB;AAAA,IACF;AAAA,EACF,CAAC;AAED,SAAO,eAAe,eAAe,aAAa;AACpD;;;AC5EA,eAAsB,YAAY,WAA2BC,QAAc;AACzE,UAAQ,MAAM,UAAU,SAASA,MAAI,GAAG;AAC1C;;;ACEO,IAAM,gCAAN,cAA4C,oCAAoC;AAAA,EACrF,YAAY,WAAuD,CAAC,GAAG;AACrE,UAAM;AAAA,MACJ,GAAG;AAAA,MACH,iBAAiB;AAAA,QACf,UAAU;AAAA,QACV,MAAM;AAAA,QACN,MAAM;AAAA,QACN,MAAM;AAAA,MACR;AAAA,IACF,CAAC;AAAA,EACH;AACF;;;ACrBA,SAAS,SAAS;;;ACOX,SAAS,iCAAiC,SAA8B;AAC7E,MAAI,OAAO,YAAY,UAAU;AAC/B,WAAO;AAAA,EACT;AAEA,MAAI,mBAAmB,aAAa;AAClC,WAAO,mBAAmB,IAAI,WAAW,OAAO,CAAC;AAAA,EACnD;AAEA,SAAO,mBAAmB,OAAO;AACnC;AAEO,SAAS,+BACd,SACY;AACZ,MAAI,mBAAmB,YAAY;AACjC,WAAO;AAAA,EACT;AAEA,MAAI,OAAO,YAAY,UAAU;AAC/B,WAAO,mBAAmB,OAAO;AAAA,EACnC;AAEA,MAAI,mBAAmB,aAAa;AAClC,WAAO,IAAI,WAAW,OAAO;AAAA,EAC/B;AAEA,QAAM,IAAI;AAAA,IACR,wFAAwF,OAAO,OAAO;AAAA,EACxG;AACF;;;AC1BO,IAAM,iCACX,CAAI;AAAA,EACF;AAAA,EACA;AAAA,EACA;AACF,MAKA,OAAO,EAAE,UAAU,KAAK,kBAAkB,MAAM;AAC9C,QAAM,eAAe,MAAM,SAAS,KAAK;AAGzC,MAAI,aAAa,KAAK,MAAM,IAAI;AAC9B,WAAO,IAAI,aAAa;AAAA,MACtB,SAAS,SAAS;AAAA,MAClB;AAAA,MACA;AAAA,MACA,YAAY,SAAS;AAAA,MACrB;AAAA,MACA,aAAa,cAAc,QAAQ;AAAA,IACrC,CAAC;AAAA,EACH;AAGA,MAAI;AACF,UAAM,cAAc,UAAU;AAAA,MAC5B,MAAM;AAAA,MACN,QAAQ;AAAA,IACV,CAAC;AAED,WAAO,IAAI,aAAa;AAAA,MACtB,SAAS,eAAe,WAAW;AAAA,MACnC;AAAA,MACA;AAAA,MACA,YAAY,SAAS;AAAA,MACrB;AAAA,MACA,MAAM;AAAA,MACN,aAAa,cAAc,UAAU,WAAW;AAAA,IAClD,CAAC;AAAA,EACH,SAAS,YAAY;AACnB,WAAO,IAAI,aAAa;AAAA,MACtB,SAAS,SAAS;AAAA,MAClB;AAAA,MACA;AAAA,MACA,YAAY,SAAS;AAAA,MACrB;AAAA,MACA,aAAa,cAAc,QAAQ;AAAA,IACrC,CAAC;AAAA,EACH;AACF;AAEK,IAAM,iCACX,CAAC;AAAA,EACC;AACF,IAEI,CAAC,MACL,OAAO,EAAE,UAAU,KAAK,kBAAkB,MAAM;AAC9C,QAAM,eAAe,MAAM,SAAS,KAAK;AAEzC,SAAO,IAAI,aAAa;AAAA,IACtB,SAAS,aAAa,KAAK,MAAM,KAAK,eAAe,SAAS;AAAA,IAC9D;AAAA,IACA;AAAA,IACA,YAAY,SAAS;AAAA,IACrB;AAAA,IACA,aAAa,cAAc,QAAQ;AAAA,EACrC,CAAC;AACH;AAEK,IAAM,4BACX,CAAI,mBACJ,OAAO,EAAE,UAAU,KAAK,kBAAkB,MAAM;AAC9C,QAAM,eAAe,MAAM,SAAS,KAAK;AAEzC,QAAM,eAAe,cAAc;AAAA,IACjC,MAAM;AAAA,IACN,QAAQ;AAAA,EACV,CAAC;AAED,MAAI,CAAC,aAAa,SAAS;AACzB,UAAM,IAAI,aAAa;AAAA,MACrB,SAAS;AAAA,MACT,OAAO,aAAa;AAAA,MACpB,YAAY,SAAS;AAAA,MACrB;AAAA,MACA;AAAA,MACA;AAAA,IACF,CAAC;AAAA,EACH;AAEA,SAAO,aAAa;AACtB;AAEK,IAAM,4BACX,MACA,OAAO,EAAE,SAAS,MAChB,SAAS,KAAK;AAEX,IAAM,iCACX,MACA,OAAO,EAAE,UAAU,KAAK,kBAAkB,MAAM;AAC9C,MAAI,SAAS,QAAQ,IAAI,cAAc,MAAM,cAAc;AACzD,UAAM,IAAI,aAAa;AAAA,MACrB,SAAS;AAAA,MACT,YAAY,SAAS;AAAA,MACrB;AAAA,MACA;AAAA,IACF,CAAC;AAAA,EACH;AAEA,SAAO,+BAA+B,MAAM,SAAS,YAAY,CAAC;AACpE;AAEK,IAAM,gBAAgB,OAAU;AAAA,EACrC;AAAA,EACA;AAAA,EACA;AAAA,EACA,uBAAAC;AAAA,EACA,2BAAAC;AAAA,EACA;AACF,MAQE,UAAU;AAAA,EACR;AAAA,EACA,SAAS;AAAA,IACP,GAAG;AAAA,IACH,gBAAgB;AAAA,EAClB;AAAA,EACA,MAAM;AAAA,IACJ,SAAS,KAAK,UAAU,IAAI;AAAA,IAC5B,QAAQ;AAAA,EACV;AAAA,EACA,uBAAAD;AAAA,EACA,2BAAAC;AAAA,EACA;AACF,CAAC;AAEI,IAAM,YAAY,OAAU;AAAA,EACjC;AAAA,EACA,UAAU,CAAC;AAAA,EACX;AAAA,EACA,2BAAAA;AAAA,EACA,uBAAAD;AAAA,EACA;AACF,MAUM;AACJ,MAAI;AACF,UAAM,WAAW,MAAM,MAAM,KAAK;AAAA,MAChC,QAAQ;AAAA,MACR;AAAA,MACA,MAAM,KAAK;AAAA,MACX,QAAQ;AAAA,IACV,CAAC;AAED,QAAI,CAAC,SAAS,IAAI;AAChB,UAAI;AACF,cAAM,MAAMA,uBAAsB;AAAA,UAChC;AAAA,UACA;AAAA,UACA,mBAAmB,KAAK;AAAA,QAC1B,CAAC;AAAA,MACH,SAAS,OAAO;AACd,YAAI,iBAAiB,OAAO;AAC1B,cAAI,MAAM,SAAS,gBAAgB,iBAAiB,cAAc;AAChE,kBAAM;AAAA,UACR;AAAA,QACF;AAEA,cAAM,IAAI,aAAa;AAAA,UACrB,SAAS;AAAA,UACT,OAAO;AAAA,UACP,YAAY,SAAS;AAAA,UACrB;AAAA,UACA,mBAAmB,KAAK;AAAA,QAC1B,CAAC;AAAA,MACH;AAAA,IACF;AAEA,QAAI;AACF,aAAO,MAAMC,2BAA0B;AAAA,QACrC;AAAA,QACA;AAAA,QACA,mBAAmB,KAAK;AAAA,MAC1B,CAAC;AAAA,IACH,SAAS,OAAO;AACd,UAAI,iBAAiB,OAAO;AAC1B,YAAI,MAAM,SAAS,gBAAgB,iBAAiB,cAAc;AAChE,gBAAM;AAAA,QACR;AAAA,MACF;AAEA,YAAM,IAAI,aAAa;AAAA,QACrB,SAAS;AAAA,QACT,OAAO;AAAA,QACP,YAAY,SAAS;AAAA,QACrB;AAAA,QACA,mBAAmB,KAAK;AAAA,MAC1B,CAAC;AAAA,IACH;AAAA,EACF,SAAS,OAAO;AACd,QAAI,iBAAiB,OAAO;AAC1B,UAAI,MAAM,SAAS,cAAc;AAC/B,cAAM;AAAA,MACR;AAAA,IACF;AAGA,QAAI,iBAAiB,aAAa,MAAM,YAAY,gBAAgB;AAElE,YAAM,QAAS,MAAc;AAE7B,UAAI,SAAS,MAAM;AAEjB,cAAM,IAAI,aAAa;AAAA,UACrB,SAAS,0BAA0B,MAAM,OAAO;AAAA,UAChD;AAAA,UACA;AAAA,UACA,mBAAmB,KAAK;AAAA,UACxB,aAAa;AAAA,QACf,CAAC;AAAA,MACH;AAAA,IACF;AAEA,UAAM;AAAA,EACR;AACF;;;AFtPA,IAAM,+BAA+B,EAAE,OAAO;AAAA,EAC5C,OAAO,EAAE,OAAO;AAAA,EAChB,QAAQ,EAAE,OAAO;AAAA,EACjB,MAAM,EAAE,OAAO;AAAA,EACf,QAAQ,EAAE,OAAO;AACnB,CAAC;AAMM,IAAM,yCACX,+BAA+B;AAAA,EAC7B,aAAa,UAAU,4BAA4B;AAAA,EACnD,gBAAgB,CAAC,UAAU,MAAM;AACnC,CAAC;;;AGvBH;AAAA;AAAA;AAAA;AAAA;;;ACAA,SAAS,KAAAC,UAAS;;;ACKX,IAAM,2BAA2B,OAAe;AAAA,EACrD,QAAQ,WAAW;AAAA,EACnB,WAAW,YAAY;AAAA,EACvB;AACF,MAIuB,MAAM,YAAY,SAAS,IAAI,CAAC;;;ACVhD,IAAe,gBAAf,MAEP;AAAA,EACW;AAAA,EAET,YAAY,EAAE,SAAS,GAA2B;AAChD,SAAK,WAAW;AAAA,EAClB;AAAA;AAAA,EAMA,IAAI,mBAAqC;AACvC,WAAO;AAAA,MACL,UAAU,KAAK;AAAA,MACf,WAAW,KAAK;AAAA,IAClB;AAAA,EACF;AAKF;;;AChBO,SAAS,sCAGd;AACA,SAAO;AAAA,IACL,QAAQ,CAAC,iBAAiB,EAAE,QAAQ,YAAY;AAAA,EAClD;AACF;;;AHyCO,IAAM,oCAAN,MAAM,2CACH,cAMV;AAAA,EACE,YAAY,UAAgD;AAC1D,UAAM,EAAE,SAAS,CAAC;AAAA,EACpB;AAAA,EAES,WAAW;AAAA,EAEpB,IAAI,YAAY;AACd,WAAO,KAAK,SAAS;AAAA,EACvB;AAAA,EAEA,MAAM,QACJ,OACA,aAC+C;AAC/C,UAAM,MAAM,KAAK,SAAS,OAAO,IAAI,8BAA8B;AACnE,UAAM,cAAc,YAAY,KAAK;AAErC,WAAO,yBAAyB;AAAA,MAC9B,OAAO,IAAI;AAAA,MACX,UAAU,IAAI;AAAA,MACd,MAAM,YACJ,cAAc;AAAA,QACZ,KAAK,IAAI,YAAY,UAAU;AAAA,QAC/B,SAAS,IAAI,QAAQ;AAAA,UACnB,cAAc,YAAY;AAAA,UAC1B,YAAY,YAAY;AAAA,UACxB,KAAK,YAAY;AAAA,UACjB,QAAQ,YAAY;AAAA,QACtB,CAAC;AAAA,QACD,MAAM;AAAA,UACJ,QAAQ,MAAM;AAAA,UACd,iBAAiB,MAAM;AAAA,UACvB,MAAM,KAAK,SAAS;AAAA,UACpB,YAAY,KAAK,SAAS;AAAA,UAC1B,QAAQ,KAAK,SAAS;AAAA,UACtB,OAAO,KAAK,SAAS;AAAA,UACrB,WAAW,KAAK,SAAS;AAAA,UACzB,eAAe,KAAK,SAAS;AAAA,UAC7B,OAAO,KAAK,SAAS;AAAA,UACrB,mBAAmB;AAAA,YACjB,qBAAqB,KAAK,SAAS;AAAA,UACrC;AAAA,QACF;AAAA,QACA,uBAAuB;AAAA,QACvB,2BAA2B;AAAA,UACzB,UAAU,0CAA0C;AAAA,QACtD;AAAA,QACA;AAAA,MACF,CAAC;AAAA,IACL,CAAC;AAAA,EACH;AAAA,EAEA,IAAI,mBAAkE;AACpE,UAAM,yBAAwC;AAAA,MAC5C;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,IACF;AAEA,WAAO,OAAO;AAAA,MACZ,OAAO,QAAQ,KAAK,QAAQ,EAAE;AAAA,QAAO,CAAC,CAAC,GAAG,MACxC,uBAAuB,SAAS,GAAG;AAAA,MACrC;AAAA,IACF;AAAA,EACF;AAAA,EAEA,MAAM,iBACJ,QACA,SACA;AACA,UAAM,cAAc,MAAM,KAAK,QAAQ,QAAQ,OAAO;AAEtD,WAAO;AAAA,MACL;AAAA,MACA,cAAc,YAAY;AAAA,IAC5B;AAAA,EACF;AAAA,EAEA,iBAAiB;AACf,WAAO,KAAK,mBAAmB,oCAAoC,CAAC;AAAA,EACtE;AAAA,EAEA,mBACE,gBASA;AACA,WAAO,IAAI,mCAAmC;AAAA,MAC5C,OAAO;AAAA,MACP;AAAA,IACF,CAAC;AAAA,EACH;AAAA,EAEA,aACE,oBACA;AACA,WAAO,IAAI;AAAA,MACT,OAAO,OAAO,CAAC,GAAG,KAAK,UAAU,kBAAkB;AAAA,IACrD;AAAA,EACF;AACF;AAEA,IAAM,6CAA6CC,GAAE,OAAO;AAAA,EAC1D,QAAQA,GAAE,MAAMA,GAAE,OAAO,CAAC;AAAA,EAC1B,YAAYA,GAAE,OAAO,CAAC,CAAC;AAAA,EACvB,MAAMA,GAAE,OAAO;AACjB,CAAC;;;AD1KM,SAAS,IAAI,UAAsD;AACxE,SAAO,IAAI,8BAA8B,QAAQ;AACnD;AASO,SAAS,eAAe,UAAgD;AAC7E,SAAO,IAAI,kCAAkC,QAAQ;AACvD;;;AKxBO,IAAM,kBAAN,cAA8B,MAAM;AAAA,EACzC,YAAY,EAAE,QAAQ,GAAwB;AAC5C,UAAM,OAAO;AAEb,SAAK,OAAO;AAAA,EACd;AAAA,EAEA,SAAS;AACP,WAAO;AAAA,MACL,MAAM,KAAK;AAAA,MACX,SAAS,KAAK;AAAA,IAChB;AAAA,EACF;AACF;;;ACXO,SAAS,WAAW;AAAA,EACzB;AAAA,EACA;AAAA,EACA,sBAAsB;AAAA,EACtB;AACF,GAKW;AACT,MAAI,UAAU,MAAM;AAClB,WAAO;AAAA,EACT;AAEA,MAAI,OAAO,YAAY,aAAa;AAClC,UAAM,IAAI,gBAAgB;AAAA,MACxB,SAAS,GAAG,WAAW,2CAA2C,mBAAmB;AAAA,IACvF,CAAC;AAAA,EACH;AAEA,WAAS,QAAQ,IAAI,uBAAuB;AAE5C,MAAI,UAAU,MAAM;AAClB,UAAM,IAAI,gBAAgB;AAAA,MACxB,SAAS,GAAG,WAAW,2CAA2C,mBAAmB,qFAAqF,uBAAuB;AAAA,IACnM,CAAC;AAAA,EACH;AAEA,SAAO;AACT;;;ACtBO,IAAM,yBAAN,cAAqC,oCAAoC;AAAA,EAC9E,YACE,WAEI,CAAC,GACL;AACA,UAAM;AAAA,MACJ,GAAG;AAAA,MACH,SAAS;AAAA,QACP,eAAe,UAAU,WAAW;AAAA,UAClC,QAAQ,SAAS;AAAA,UACjB,yBAAyB;AAAA,UACzB,aAAa;AAAA,QACf,CAAC,CAAC;AAAA,MACJ;AAAA,MACA,iBAAiB;AAAA,QACf,UAAU;AAAA,QACV,MAAM;AAAA,QACN,MAAM;AAAA,QACN,MAAM;AAAA,MACR;AAAA,IACF,CAAC;AAAA,EACH;AACF;;;ACjCA,SAAS,KAAAC,UAAS;AAIlB,IAAM,wBAAwBC,GAAE,OAAO;AAAA,EACrC,SAASA,GAAE,OAAO;AACpB,CAAC;AAIM,IAAM,kCAAkC,+BAA+B;AAAA,EAC5E,aAAa,UAAU,qBAAqB;AAAA,EAC5C,gBAAgB,CAAC,UAAU,MAAM;AACnC,CAAC;;;ACbD;AAAA;AAAA,aAAAC;AAAA,EAAA;AAAA;AAAA;AAAA;;;ACAA,SAAS,KAAAC,UAAS;;;ACAlB,SAAS,KAAAC,UAAS;AAwCX,IAAM,kBAAN,MAA+C;AAAA,EAC3C;AAAA,EAET,YAAY,UAAmC;AAC7C,SAAK,WAAW;AAAA,EAClB;AAAA,EAEA,MAAM,gBACJC,QACA,aACqC;AACrC,UAAM,MAAM,KAAK,SAAS,OAAO,IAAI,uBAAuB;AAC5D,UAAM,cAAc,aAAa,KAAK;AAEtC,WAAO,yBAAyB;AAAA,MAC9B,OAAO,IAAI;AAAA,MACX,UAAU,IAAI;AAAA,MACd,MAAM,YACJ,cAAc;AAAA,QACZ,KAAK,IAAI,YAAY,WAAW;AAAA,QAChC,SAAS,IAAI,QAAQ;AAAA,UACnB,cAAc;AAAA,UACd,YAAY,aAAa;AAAA,UACzB,KAAK,aAAa;AAAA,UAClB,QAAQ;AAAA,QACV,CAAC;AAAA,QACD,MAAM;AAAA,UACJ,OAAO,KAAK,SAAS;AAAA,UACrB,MAAAA;AAAA,QACF;AAAA,QACA,uBAAuB;AAAA,QACvB,2BAA2B;AAAA,UACzB,UAAU,gCAAgC;AAAA,QAC5C;AAAA,QACA;AAAA,MACF,CAAC;AAAA,IACL,CAAC;AAAA,EACH;AAAA,EAEA,MAAM,kBACJ,QACA,aACuC;AACvC,UAAM,MAAM,KAAK,SAAS,OAAO,IAAI,uBAAuB;AAC5D,UAAM,cAAc,aAAa,KAAK;AAEtC,WAAO,yBAAyB;AAAA,MAC9B,OAAO,IAAI;AAAA,MACX,UAAU,IAAI;AAAA,MACd,MAAM,YACJ,cAAc;AAAA,QACZ,KAAK,IAAI,YAAY,aAAa;AAAA,QAClC,SAAS,IAAI,QAAQ;AAAA,UACnB,cAAc;AAAA,UACd,YAAY,aAAa;AAAA,UACzB,KAAK,aAAa;AAAA,UAClB,QAAQ;AAAA,QACV,CAAC;AAAA,QACD,MAAM;AAAA,UACJ,OAAO,KAAK,SAAS;AAAA,UACrB;AAAA,QACF;AAAA,QACA,uBAAuB;AAAA,QACvB,2BAA2B;AAAA,UACzB,UAAU,kCAAkC;AAAA,QAC9C;AAAA,QACA;AAAA,MACF,CAAC;AAAA,IACL,CAAC;AAAA,EACH;AAAA,EAEA,MAAM,SAASA,QAAc;AAC3B,YAAQ,MAAM,KAAK,kBAAkBA,MAAI,GAAG;AAAA,EAC9C;AAAA,EAEA,MAAM,kBAAkBA,QAAc;AACpC,UAAM,WAAW,MAAM,KAAK,gBAAgBA,MAAI;AAEhD,WAAO;AAAA,MACL,QAAQ,SAAS;AAAA,MACjB,YAAY,SAAS;AAAA,IACvB;AAAA,EACF;AAAA,EAEA,MAAM,WAAW,QAAkB;AACjC,UAAM,WAAW,MAAM,KAAK,kBAAkB,MAAM;AAEpD,WAAO,SAAS;AAAA,EAClB;AACF;AAEA,IAAM,qCAAqCC,GAAE,OAAO;AAAA,EAClD,MAAMA,GAAE,OAAO;AAAA,EACf,MAAMA,GAAE,OAAO;AAAA,IACb,aAAaA,GAAE,OAAO;AAAA,MACpB,SAASA,GAAE,OAAO;AAAA,IACpB,CAAC;AAAA,EACH,CAAC;AACH,CAAC;AAMD,IAAM,mCAAmCA,GAAE,OAAO;AAAA,EAChD,QAAQA,GAAE,MAAMA,GAAE,OAAO,CAAC;AAAA,EAC1B,eAAeA,GAAE,MAAMA,GAAE,OAAO,CAAC;AAAA,EACjC,MAAMA,GAAE,OAAO;AAAA,IACb,aAAaA,GAAE,OAAO;AAAA,MACpB,SAASA,GAAE,OAAO;AAAA,IACpB,CAAC;AAAA,EACH,CAAC;AACH,CAAC;;;ADrIM,IAAM,+BAA+B;AAAA,EAC1C,4BAA4B;AAAA,IAC1B,mBAAmB;AAAA,IACnB,YAAY;AAAA,EACd;AAAA,EACA,sBAAsB;AAAA,IACpB,mBAAmB;AAAA,IACnB,YAAY;AAAA,EACd;AAAA,EACA,2BAA2B;AAAA,IACzB,mBAAmB;AAAA,IACnB,YAAY;AAAA,EACd;AAAA,EACA,sBAAsB;AAAA,IACpB,mBAAmB;AAAA,IACnB,YAAY;AAAA,EACd;AAAA,EACA,4BAA4B;AAAA,IAC1B,mBAAmB;AAAA,IACnB,YAAY;AAAA,EACd;AAAA,EACA,2BAA2B;AAAA,IACzB,mBAAmB;AAAA,IACnB,YAAY;AAAA,EACd;AAAA,EACA,iCAAiC;AAAA,IAC/B,mBAAmB;AAAA,IACnB,YAAY;AAAA,EACd;AACF;AA+BO,IAAM,2BAAN,MAAM,kCACH,cAIV;AAAA,EACE,YAAY,UAA4C;AACtD,UAAM,EAAE,SAAS,CAAC;AAElB,SAAK,oBACH,6BAA6B,KAAK,SAAS,EAAE;AAE/C,SAAK,YAAY,IAAI,gBAAgB;AAAA,MACnC,KAAK,KAAK,SAAS;AAAA,MACnB,OAAO,KAAK,SAAS;AAAA,IACvB,CAAC;AAED,SAAK,aAAa,6BAA6B,KAAK,SAAS,EAAE;AAAA,EACjE;AAAA,EAES,WAAW;AAAA,EACpB,IAAI,YAAY;AACd,WAAO,KAAK,SAAS;AAAA,EACvB;AAAA,EAES,mBAAmB;AAAA,EACnB,mBAAmB;AAAA,EACnB;AAAA,EAEA;AAAA,EACQ;AAAA,EAEjB,MAAM,SAASC,QAAc;AAC3B,WAAO,KAAK,UAAU,SAASA,MAAI;AAAA,EACrC;AAAA,EAEA,MAAM,kBAAkBA,QAAc;AACpC,WAAO,KAAK,UAAU,kBAAkBA,MAAI;AAAA,EAC9C;AAAA,EAEA,MAAM,WAAW,QAAkB;AACjC,WAAO,KAAK,UAAU,WAAW,MAAM;AAAA,EACzC;AAAA,EAEA,MAAM,QACJ,OACA,aACsC;AACtC,QAAI,MAAM,SAAS,KAAK,kBAAkB;AACxC,YAAM,IAAI;AAAA,QACR,0CAA0C,KAAK,gBAAgB;AAAA,MACjE;AAAA,IACF;AAEA,UAAM,MAAM,KAAK,SAAS,OAAO,IAAI,uBAAuB;AAC5D,UAAM,cAAc,YAAY,KAAK;AAErC,WAAO,yBAAyB;AAAA,MAC9B,OAAO,IAAI;AAAA,MACX,UAAU,IAAI;AAAA,MACd,MAAM,YACJ,cAAc;AAAA,QACZ,KAAK,IAAI,YAAY,QAAQ;AAAA,QAC7B,SAAS,IAAI,QAAQ;AAAA,UACnB,cAAc,YAAY;AAAA,UAC1B,YAAY,YAAY;AAAA,UACxB,KAAK,YAAY;AAAA,UACjB,QAAQ,YAAY;AAAA,QACtB,CAAC;AAAA,QACD,MAAM;AAAA,UACJ,OAAO,KAAK,SAAS;AAAA,UACrB;AAAA,UACA,YAAY,KAAK,SAAS;AAAA,UAC1B,UAAU,KAAK,SAAS;AAAA,QAC1B;AAAA,QACA,uBAAuB;AAAA,QACvB,2BAA2B;AAAA,UACzB,UAAU,iCAAiC;AAAA,QAC7C;AAAA,QACA;AAAA,MACF,CAAC;AAAA,IACL,CAAC;AAAA,EACH;AAAA,EAEA,IAAI,mBAA8D;AAChE,WAAO;AAAA,MACL,UAAU,KAAK,SAAS;AAAA,IAC1B;AAAA,EACF;AAAA,EAEA,MAAM,cAAc,OAAiB,SAA8B;AACjE,UAAM,cAAc,MAAM,KAAK,QAAQ,OAAO,OAAO;AACrD,WAAO;AAAA,MACL;AAAA,MACA,YAAY,YAAY;AAAA,IAC1B;AAAA,EACF;AAAA,EAEA,aAAa,oBAA+D;AAC1E,WAAO,IAAI;AAAA,MACT,OAAO,OAAO,CAAC,GAAG,KAAK,UAAU,kBAAkB;AAAA,IACrD;AAAA,EACF;AACF;AAEA,IAAM,oCAAoCC,GAAE,OAAO;AAAA,EACjD,IAAIA,GAAE,OAAO;AAAA,EACb,OAAOA,GAAE,MAAMA,GAAE,OAAO,CAAC;AAAA,EACzB,YAAYA,GAAE,MAAMA,GAAE,MAAMA,GAAE,OAAO,CAAC,CAAC;AAAA,EACvC,MAAMA,GAAE,OAAO;AAAA,IACb,aAAaA,GAAE,OAAO;AAAA,MACpB,SAASA,GAAE,OAAO;AAAA,IACpB,CAAC;AAAA,EACH,CAAC;AACH,CAAC;;;AEjMD,SAAS,KAAAC,UAAS;;;ACGX,SAAS,gBAAmB;AAAA,EACjC;AAAA,EACA;AAAA,EACA,SAAAC;AAAA,EACA;AACF,GAKG;AACD,WAAS,YAAY,MAAc;AACjC,IAAAA,SAAQ,UAAU,EAAE,MAAM,MAAM,OAAO,CAAC,CAAC;AAAA,EAC3C;AAEA,UAAQ,YAAY;AAClB,QAAI;AACF,YAAM,SAAS,IAAI,4BAA4B,MAAM;AACrD,YAAM,cAAc,IAAI,YAAY,OAAO;AAE3C,UAAI,kBAAkB;AAGtB,aAAO,MAAM;AACX,cAAM,EAAE,OAAO,OAAO,KAAK,IAAI,MAAM,OAAO,KAAK;AAEjD,YAAI,MAAM;AACR;AAAA,QACF;AAEA,2BAAmB,YAAY,OAAO,OAAO,EAAE,QAAQ,KAAK,CAAC;AAE7D,cAAM,mBAAmB,gBAAgB,MAAM,IAAI;AAEnD,0BAAkB,iBAAiB,IAAI,KAAK;AAE5C,yBAAiB,QAAQ,WAAW;AAAA,MACtC;AAGA,UAAI,iBAAiB;AACnB,oBAAY,eAAe;AAAA,MAC7B;AAAA,IACF,UAAE;AACA,eAAS;AAAA,IACX;AAAA,EACF,GAAG;AACL;;;AC7CA,eAAsB,+BAAkC;AAAA,EACtD;AAAA,EACA;AACF,GAGqC;AACnC,QAAM,QAAQ,IAAI,WAAqB;AAGvC,kBAAgB;AAAA,IACd;AAAA,IACA;AAAA,IACA,QAAQ,OAAO;AACb,YAAM,KAAK,EAAE,MAAM,SAAS,YAAY,MAAM,CAAC;AAAA,IACjD;AAAA,IACA,SAAS;AACP,YAAM,MAAM;AAAA,IACd;AAAA,EACF,CAAC;AAED,SAAO;AACT;;;ACxBO,IAAM,kCACX,CAAI,WACJ,CAAC,EAAE,SAAS,MACV,+BAA+B;AAAA,EAC7B,QAAQ,SAAS;AAAA,EACjB;AACF,CAAC;;;AHsBE,IAAM,gCAAgC;AAAA,EAC3C,SAAS;AAAA,IACP,mBAAmB;AAAA,EACrB;AAAA,EACA,iBAAiB;AAAA,IACf,mBAAmB;AAAA,EACrB;AACF;AAwCO,IAAM,4BAAN,MAAM,mCACH,cAEV;AAAA,EACE,YAAY,UAA6C;AACvD,UAAM,EAAE,SAAS,CAAC;AAElB,SAAK,oBACH,8BAA8B,KAAK,SAAS,KAAK,EAAE;AAErD,SAAK,YAAY,IAAI,gBAAgB;AAAA,MACnC,KAAK,KAAK,SAAS;AAAA,MACnB,OAAO,KAAK,SAAS;AAAA,IACvB,CAAC;AAAA,EACH;AAAA,EAES,WAAW;AAAA,EACpB,IAAI,YAAY;AACd,WAAO,KAAK,SAAS;AAAA,EACvB;AAAA,EAES;AAAA,EACA;AAAA,EAET,MAAM,kBAAkB,OAAe;AACrC,WAAO,YAAY,KAAK,WAAW,KAAK;AAAA,EAC1C;AAAA,EAEA,MAAM,QACJ,QACA,aACA,SAGmB;AACnB,UAAM,MAAM,KAAK,SAAS,OAAO,IAAI,uBAAuB;AAC5D,UAAM,iBAAiB,QAAQ;AAC/B,UAAM,cAAc,YAAY,KAAK;AAErC,WAAO,yBAAyB;AAAA,MAC9B,OAAO,IAAI;AAAA,MACX,UAAU,IAAI;AAAA,MACd,MAAM,YACJ,cAAc;AAAA,QACZ,KAAK,IAAI,YAAY,WAAW;AAAA,QAChC,SAAS,IAAI,QAAQ;AAAA,UACnB,cAAc,YAAY;AAAA,UAC1B,YAAY,YAAY;AAAA,UACxB,KAAK,YAAY;AAAA,UACjB,QAAQ,YAAY;AAAA,QACtB,CAAC;AAAA,QACD,MAAM;AAAA,UACJ,QAAQ,eAAe;AAAA,UACvB,OAAO,KAAK,SAAS;AAAA,UACrB;AAAA,UACA,iBAAiB,KAAK,SAAS;AAAA,UAC/B,YAAY,KAAK,SAAS;AAAA,UAC1B,aAAa,KAAK,SAAS;AAAA,UAC3B,GAAG,KAAK,SAAS;AAAA,UACjB,GAAG,KAAK,SAAS;AAAA,UACjB,mBAAmB,KAAK,SAAS;AAAA,UACjC,kBAAkB,KAAK,SAAS;AAAA,UAChC,eAAe,KAAK,SAAS;AAAA,UAC7B,gBAAgB,KAAK,SAAS;AAAA,UAC9B,oBAAoB,KAAK,SAAS;AAAA,UAClC,YAAY,KAAK,SAAS;AAAA,UAC1B,UAAU,KAAK,SAAS;AAAA,QAC1B;AAAA,QACA,uBAAuB;AAAA,QACvB,2BAA2B,eAAe;AAAA,QAC1C;AAAA,MACF,CAAC;AAAA,IACL,CAAC;AAAA,EACH;AAAA,EAEA,IAAI,mBAA+D;AACjE,UAAM,yBAAwC;AAAA,MAC5C,GAAG;AAAA,MAEH;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,IACF;AAEA,WAAO,OAAO;AAAA,MACZ,OAAO,QAAQ,KAAK,QAAQ,EAAE;AAAA,QAAO,CAAC,CAAC,GAAG,MACxC,uBAAuB,SAAS,GAAG;AAAA,MACrC;AAAA,IACF;AAAA,EACF;AAAA,EAEA,MAAM,gBAAgB,QAAgB,SAA8B;AAClE,WAAO,KAAK;AAAA,MACV,MAAM,KAAK,QAAQ,QAAQ,SAAS;AAAA,QAClC,gBAAgB,mCAAmC;AAAA,MACrD,CAAC;AAAA,IACH;AAAA,EACF;AAAA,EAEA,sBAAsB,aAAsB;AAC1C,WAAO,KAAK;AAAA,MACV,cAAc;AAAA,QACZ,OAAO;AAAA,QACP,QAAQ,UAAU,kCAAkC;AAAA,MACtD,CAAC;AAAA,IACH;AAAA,EACF;AAAA,EAEA,8BAA8B,aAA2C;AACvE,WAAO;AAAA,MACL;AAAA,MACA,uBAAuB,YAAY,YAAY,IAAI,CAAC,gBAAgB;AAAA,QAClE,MAAM,WAAW;AAAA,QACjB,cAAc,KAAK,sBAAsB,WAAW,aAAa;AAAA,MACnE,EAAE;AAAA,IACJ;AAAA,EACF;AAAA,EAEQ,sBACN,cAC4B;AAC5B,YAAQ,cAAc;AAAA,MACpB,KAAK;AACH,eAAO;AAAA,MACT,KAAK;AACH,eAAO;AAAA,MACT,KAAK;AACH,eAAO;AAAA,MACT,KAAK;AACH,eAAO;AAAA,MACT;AACE,eAAO;AAAA,IACX;AAAA,EACF;AAAA,EAEA,aAAa,QAAgB,SAA8B;AACzD,WAAO,KAAK,QAAQ,QAAQ,SAAS;AAAA,MACnC,gBAAgB,mCAAmC;AAAA,IACrD,CAAC;AAAA,EACH;AAAA,EAEA,iBAAiB,OAAgB;AAC/B,UAAM,QAAQ;AACd,WAAO,MAAM,gBAAgB,OAAO,KAAK,MAAM;AAAA,EACjD;AAAA,EAEA,iBAAuB;AACrB,WAAO;AAAA,EACT;AAAA,EAEA,iBAAiB;AACf,WAAO,KAAK,mBAAmBC,MAAK,CAAC;AAAA,EACvC;AAAA,EAEA,wBAAwB;AACtB,WAAO,KAAK,mBAAmBC,aAAY,CAAC;AAAA,EAC9C;AAAA,EAEA,eAAe,SAAiD;AAC9D,WAAO,KAAK,mBAAmBC,MAAK,OAAO,CAAC;AAAA,EAC9C;AAAA,EAEA,mBACE,gBAMA;AACA,WAAO,IAAI,iCAAiC;AAAA,MAC1C,OAAO,KAAK,aAAa;AAAA,QACvB,eAAe;AAAA,UACb,GAAI,KAAK,SAAS,iBAAiB,CAAC;AAAA,UACpC,GAAG,eAAe;AAAA,QACpB;AAAA,MACF,CAAC;AAAA,MACD;AAAA,IACF,CAAC;AAAA,EACH;AAAA,EAEA,aAAa,oBAAgE;AAC3E,WAAO,IAAI;AAAA,MACT,OAAO,OAAO,CAAC,GAAG,KAAK,UAAU,kBAAkB;AAAA,IACrD;AAAA,EACF;AACF;AAEA,IAAM,qCAAqCC,GAAE,OAAO;AAAA,EAClD,IAAIA,GAAE,OAAO;AAAA,EACb,aAAaA,GAAE;AAAA,IACbA,GAAE,OAAO;AAAA,MACP,IAAIA,GAAE,OAAO;AAAA,MACb,MAAMA,GAAE,OAAO;AAAA,MACf,eAAeA,GAAE,OAAO,EAAE,SAAS;AAAA,IACrC,CAAC;AAAA,EACH;AAAA,EACA,QAAQA,GAAE,OAAO;AAAA,EACjB,MAAMA,GACH,OAAO;AAAA,IACN,aAAaA,GAAE,OAAO;AAAA,MACpB,SAASA,GAAE,OAAO;AAAA,IACpB,CAAC;AAAA,EACH,CAAC,EACA,SAAS;AACd,CAAC;AAMD,IAAM,8BAA8BA,GAAE,mBAAmB,eAAe;AAAA,EACtEA,GAAE,OAAO;AAAA,IACP,MAAMA,GAAE,OAAO;AAAA,IACf,aAAaA,GAAE,QAAQ,KAAK;AAAA,EAC9B,CAAC;AAAA,EACDA,GAAE,OAAO;AAAA,IACP,aAAaA,GAAE,QAAQ,IAAI;AAAA,IAC3B,eAAeA,GAAE,OAAO;AAAA,IACxB,UAAU;AAAA,EACZ,CAAC;AACH,CAAC;AASM,IAAM,qCAAqC;AAAA;AAAA;AAAA;AAAA,EAIhD,MAAM;AAAA,IACJ,QAAQ;AAAA,IACR,SAAS;AAAA,MACP,UAAU,kCAAkC;AAAA,IAC9C;AAAA,EACF;AAAA;AAAA;AAAA;AAAA;AAAA,EAMA,eAAe;AAAA,IACb,QAAQ;AAAA,IACR,SAAS;AAAA,MACP,UAAU,2BAA2B;AAAA,IACvC;AAAA,EACF;AACF;;;AH/TO,SAASC,KACd,UAGA;AACA,SAAO,IAAI,uBAAuB,QAAQ;AAC5C;AAqBO,SAAS,cAAc,UAA6C;AACzE,SAAO,IAAI,0BAA0B,QAAQ;AAC/C;AAkBO,SAAS,aAAa,UAA4C;AACvE,SAAO,IAAI,yBAAyB,QAAQ;AAC9C;AAoBO,SAAS,UAAU,UAAmC;AAC3D,SAAO,IAAI,gBAAgB,QAAQ;AACrC;;;AO7EO,IAAM,6BAAN,cAAyC,oCAAoC;AAAA,EAClF,YACE,WAEI,CAAC,GACL;AACA,UAAM;AAAA,MACJ,GAAG;AAAA,MACH,SAAS;AAAA,QACP,cAAc,WAAW;AAAA,UACvB,QAAQ,SAAS;AAAA,UACjB,yBAAyB;AAAA,UACzB,aAAa;AAAA,QACf,CAAC;AAAA,MACH;AAAA,MACA,iBAAiB;AAAA,QACf,UAAU;AAAA,QACV,MAAM;AAAA,QACN,MAAM;AAAA,QACN,MAAM;AAAA,MACR;AAAA,IACF,CAAC;AAAA,EACH;AAAA,EAEA,IAAI,SAAS;AACX,WAAO,KAAK,kBAAkB,YAAY;AAAA,EAC5C;AACF;;;ACrCA;AAAA;AAAA,aAAAC;AAAA,EAAA;AAAA;;;ACAA,SAAS,KAAAC,UAAS;;;ACclB,eAAsB,sBACpB,KAC0B;AAC1B,UAAgB,cAAc,GAAG;AAAA,IAC/B,KAAK;AAAA,IACL,KAAK;AAAA,IACL,KAAK,WAAW;AACd,aAAO,IAAI,UAAU,GAAG;AAAA,IAC1B;AAAA,IAEA,KAAK,QAAQ;AAKX,UAAIC;AAEJ,UAAI;AACF,QAAAA,cAAa,MAAM,OAAO,IAAI,GAAG;AAAA,MACnC,SAAS,OAAO;AACd,YAAI;AAEF,UAAAA,aAAY,UAAQ,IAAI;AAAA,QAC1B,SAASC,QAAO;AACd,gBAAM,IAAI,MAAM,yCAAyC;AAAA,QAC3D;AAAA,MACF;AAEA,aAAO,IAAID,WAAU,GAAG;AAAA,IAC1B;AAAA,IAEA,SAAS;AACP,YAAM,IAAI,MAAM,iBAAiB;AAAA,IACnC;AAAA,EACF;AACF;;;ADpBA,IAAM,eAAe;AA6Cd,IAAM,wBAAN,MAAM,+BACH,cAEV;AAAA,EACE,YAAY,UAAyC;AACnD,UAAM,EAAE,SAAS,CAAC;AAAA,EACpB;AAAA,EAES,WAAW;AAAA,EAEpB,IAAI,YAAY;AACd,WAAO,KAAK,SAAS;AAAA,EACvB;AAAA,EAEA,MAAc,QACZE,QACA,aACqB;AACrB,UAAM,MAAM,KAAK,SAAS,OAAO,IAAI,2BAA2B;AAChE,UAAM,cAAc,aAAa,KAAK;AAEtC,WAAO,yBAAyB;AAAA,MAC9B,OAAO,IAAI;AAAA,MACX,UAAU,IAAI;AAAA,MACd,MAAM,YACJ,cAAc;AAAA,QACZ,KAAK,IAAI;AAAA,UACP,mBAAmB,KAAK,SAAS,KAAK,GAAG,cAAc;AAAA,YACrD,4BACE,KAAK,SAAS;AAAA,YAChB,eAAe,KAAK,SAAS;AAAA,UAC/B,CAAC,CAAC;AAAA,QACJ;AAAA,QACA,SAAS,IAAI,QAAQ;AAAA,UACnB,cAAc,YAAY;AAAA,UAC1B,YAAY,YAAY;AAAA,UACxB,KAAK,YAAY;AAAA,UACjB,QAAQ,YAAY;AAAA,QACtB,CAAC;AAAA,QACD,MAAM;AAAA,UACJ,MAAAA;AAAA,UACA,UAAU,KAAK,SAAS,SAAS;AAAA,UACjC,gBAAgB,mBAAmB,KAAK,SAAS,aAAa;AAAA,QAChE;AAAA,QACA,uBAAuB,+BAA+B;AAAA,QACtD,2BAA2B,+BAA+B;AAAA,QAC1D;AAAA,MACF,CAAC;AAAA,IACL,CAAC;AAAA,EACH;AAAA,EAEA,IAAI,mBAA2D;AAC7D,WAAO;AAAA,MACL,OAAO,KAAK,SAAS;AAAA,MACrB,OAAO,KAAK,SAAS;AAAA,MACrB,eAAe,KAAK,SAAS;AAAA,IAC/B;AAAA,EACF;AAAA,EAEA,yBAAyBA,QAAc,SAA8B;AACnE,WAAO,KAAK,QAAQA,QAAM,OAAO;AAAA,EACnC;AAAA,EAEA,MAAM,6BACJ,YAE2C;AAC3C,UAAM,QAAQ,IAAI,WAA8B;AAEhD,UAAM,QAAQ,KAAK,SAAS,SAAS;AACrC,UAAM,SAAS,MAAM;AAAA,MACnB,6CACE,KAAK,SAAS,KAChB,gBAAgB,cAAc;AAAA,QAC5B,UAAU;AAAA,QACV,4BAA4B,KAAK,SAAS;AAAA,QAC1C,eAAe,KAAK,SAAS;AAAA,MAC/B,CAAC,CAAC;AAAA,IACJ;AAEA,WAAO,SAAS,YAAY;AAC1B,YAAM,MAAM,KAAK,SAAS,OAAO,IAAI,2BAA2B;AAGhE,aAAO;AAAA,QACL,KAAK,UAAU;AAAA;AAAA;AAAA,UAGb,YAAY,IAAI;AAAA,UAChB,MAAM;AAAA;AAAA,UACN,gBAAgB,mBAAmB,KAAK,SAAS,aAAa;AAAA,UAC9D,mBAAmB,mBAAmB,KAAK,SAAS,gBAAgB;AAAA,QACtE,CAAC;AAAA,MACH;AAGA,UAAI,aAAa;AACjB,uBAAiB,aAAa,YAAY;AACxC,sBAAc;AAId,cAAM,YAAY,WAAW,YAAY,IAAI;AAE7C,YAAI,cAAc,IAAI;AACpB;AAAA,QACF;AAEA,cAAM,gBAAgB,WAAW,MAAM,GAAG,SAAS;AACnD,qBAAa,WAAW,MAAM,YAAY,CAAC;AAE3C,eAAO;AAAA,UACL,KAAK,UAAU;AAAA,YACb,MAAM;AAAA,YACN,wBAAwB;AAAA,UAC1B,CAAC;AAAA,QACH;AAAA,MACF;AAGA,UAAI,WAAW,SAAS,GAAG;AACzB,eAAO;AAAA,UACL,KAAK,UAAU;AAAA,YACb,MAAM,GAAG,UAAU;AAAA;AAAA,YACnB,wBAAwB;AAAA,UAC1B,CAAC;AAAA,QACH;AAAA,MACF;AAGA,aAAO,KAAK,KAAK,UAAU,EAAE,MAAM,GAAG,CAAC,CAAC;AAAA,IAC1C;AAEA,WAAO,YAAY,CAAC,UAAU;AAC5B,YAAM,cAAc,cAAc;AAAA,QAChC,MAAM,MAAM;AAAA,QACZ,QAAQ,UAAU,uBAAuB;AAAA,MAC3C,CAAC;AAED,UAAI,CAAC,YAAY,SAAS;AACxB,cAAM,KAAK,EAAE,MAAM,SAAS,OAAO,YAAY,MAAM,CAAC;AACtD;AAAA,MACF;AAEA,YAAM,WAAW,YAAY;AAE7B,UAAI,WAAW,UAAU;AACvB,cAAM,KAAK,EAAE,MAAM,SAAS,OAAO,SAAS,CAAC;AAC7C;AAAA,MACF;AAEA,UAAI,CAAC,SAAS,SAAS;AACrB,cAAM,KAAK;AAAA,UACT,MAAM;AAAA,UACN,YAAY,mBAAmB,SAAS,KAAK;AAAA,QAC/C,CAAC;AAAA,MACH;AAAA,IACF;AAEA,WAAO,UAAU,CAAC,UAAU;AAC1B,YAAM,KAAK,EAAE,MAAM,SAAS,MAAM,CAAC;AAAA,IACrC;AAEA,WAAO,UAAU,MAAM;AACrB,YAAM,MAAM;AAAA,IACd;AAEA,WAAO;AAAA,EACT;AAAA,EAEA,aAAa,oBAA4D;AACvE,WAAO,IAAI,uBAAsB;AAAA,MAC/B,GAAG,KAAK;AAAA,MACR,GAAG;AAAA,IACL,CAAC;AAAA,EACH;AACF;AAEA,IAAM,0BAA0BC,GAAE,MAAM;AAAA,EACtCA,GAAE,OAAO;AAAA,IACP,OAAOA,GAAE,OAAO;AAAA,IAChB,SAASA,GAAE,QAAQ,KAAK,EAAE,SAAS;AAAA,IACnC,qBAAqBA,GAClB,OAAO;AAAA,MACN,OAAOA,GAAE,MAAMA,GAAE,OAAO,CAAC;AAAA,MACzB,kBAAkBA,GAAE,MAAMA,GAAE,OAAO,CAAC;AAAA,MACpC,iBAAiBA,GAAE,MAAMA,GAAE,OAAO,CAAC;AAAA,IACrC,CAAC,EACA,SAAS;AAAA,EACd,CAAC;AAAA,EACDA,GAAE,OAAO;AAAA,IACP,SAASA,GAAE,QAAQ,IAAI;AAAA,EACzB,CAAC;AAAA,EACDA,GAAE,OAAO;AAAA,IACP,SAASA,GAAE,OAAO;AAAA,IAClB,OAAOA,GAAE,OAAO;AAAA,IAChB,MAAMA,GAAE,OAAO;AAAA,EACjB,CAAC;AACH,CAAC;AAED,SAAS,cAAc,YAAiD;AACtE,MAAI,QAAQ;AACZ,MAAI,kBAAkB;AAEtB,aAAW,CAAC,KAAK,KAAK,KAAK,OAAO,QAAQ,UAAU,GAAG;AACrD,QAAI,SAAS,MAAM;AACjB;AAAA,IACF;AAEA,QAAI,CAAC,iBAAiB;AACpB,eAAS;AACT,wBAAkB;AAAA,IACpB,OAAO;AACL,eAAS;AAAA,IACX;AAEA,aAAS,GAAG,GAAG,IAAI,KAAK;AAAA,EAC1B;AAEA,SAAO;AACT;AAEA,SAAS,mBACP,eACA;AACA,SAAO,iBAAiB,OACpB;AAAA,IACE,WAAW,cAAc;AAAA,IACzB,kBAAkB,cAAc;AAAA,IAChC,OAAO,cAAc;AAAA,IACrB,mBAAmB,cAAc;AAAA,EACnC,IACA;AACN;AAEA,SAAS,mBACP,kBACA;AACA,SAAO,oBAAoB,OACvB,EAAE,uBAAuB,iBAAiB,oBAAoB,IAC9D;AACN;;;ADhTO,SAASC,KACd,UAGA;AACA,SAAO,IAAI,2BAA2B,QAAQ;AAChD;AAYO,SAAS,gBAAgB,UAAyC;AACvE,SAAO,IAAI,sBAAsB,QAAQ;AAC3C;;;AGrBO,IAAM,8BAAN,cAA0C,oCAAoC;AAAA,EACnF,YACE,WAEI,CAAC,GACL;AACA,UAAM;AAAA,MACJ,GAAG;AAAA,MACH,SAAS;AAAA,QACP,eAAe,UAAU,WAAW;AAAA,UAClC,QAAQ,SAAS;AAAA,UACjB,yBAAyB;AAAA,UACzB,aAAa;AAAA,QACf,CAAC,CAAC;AAAA,MACJ;AAAA,MACA,iBAAiB;AAAA,QACf,UAAU;AAAA,QACV,MAAM;AAAA,QACN,MAAM;AAAA,QACN,MAAM;AAAA,MACR;AAAA,IACF,CAAC;AAAA,EACH;AACF;;;ACjCA,SAAS,KAAAC,UAAS;AAIlB,IAAM,6BAA6BC,GAAE,OAAO;AAAA,EAC1C,OAAOA,GAAE,MAAMA,GAAE,OAAO,CAAC,EAAE,GAAGA,GAAE,OAAO,CAAC;AAC1C,CAAC;AAIM,IAAM,uCACX,+BAA+B;AAAA,EAC7B,aAAa,UAAU,0BAA0B;AAAA,EACjD,gBAAgB,CAAC,SACf,OAAO,KAAK,UAAU,WAAW,KAAK,QAAQ,KAAK,MAAM,KAAK,MAAM;AACxE,CAAC;;;ACfH;AAAA;AAAA,aAAAC;AAAA,EAAA,oBAAAC;AAAA,EAAA,qBAAAC;AAAA;;;ACAA,SAAS,KAAAC,UAAS;AAoDX,IAAM,gCAAN,MAAM,uCACH,cAEV;AAAA,EACE,YAAY,UAAiD;AAC3D,UAAM,EAAE,SAAS,CAAC;AAGlB,SAAK,mBAAmB,SAAS,oBAAoB;AACrD,SAAK,aAAa,SAAS;AAAA,EAC7B;AAAA,EAES,WAAW;AAAA,EACpB,IAAI,YAAY;AACd,WAAO,KAAK,SAAS;AAAA,EACvB;AAAA,EAES;AAAA,EACA,mBAAmB;AAAA,EAEnB,oBAAoB;AAAA,EACpB;AAAA,EAEA,YAAY;AAAA,EAErB,MAAM,QACJ,OACA,aAC2C;AAC3C,QAAI,MAAM,SAAS,KAAK,kBAAkB;AACxC,YAAM,IAAI;AAAA,QACR,wEAAwE,KAAK,gBAAgB;AAAA,MAC/F;AAAA,IACF;AAEA,UAAM,MAAM,KAAK,SAAS,OAAO,IAAI,4BAA4B;AACjE,UAAM,cAAc,aAAa,KAAK;AAEtC,WAAO,yBAAyB;AAAA,MAC9B,OAAO,IAAI;AAAA,MACX,UAAU,IAAI;AAAA,MACd,MAAM,YACJ,cAAc;AAAA,QACZ,KAAK,IAAI,YAAY,IAAI,KAAK,SAAS,KAAK,EAAE;AAAA,QAC9C,SAAS,IAAI,QAAQ;AAAA,UACnB,cAAc,YAAY;AAAA,UAC1B,YAAY,YAAY;AAAA,UACxB,KAAK,YAAY;AAAA,UACjB,QAAQ,YAAY;AAAA,QACtB,CAAC;AAAA,QACD,MAAM;AAAA,UACJ,QAAQ;AAAA,UACR,SAAS;AAAA,YACP,WAAW,KAAK,SAAS,SAAS,YAAY;AAAA,YAC9C,gBAAgB,KAAK,SAAS,SAAS,gBAAgB;AAAA,UACzD;AAAA,QACF;AAAA,QACA,uBAAuB;AAAA,QACvB,2BAA2B;AAAA,UACzB,UAAU,sCAAsC;AAAA,QAClD;AAAA,QACA;AAAA,MACF,CAAC;AAAA,IACL,CAAC;AAAA,EACH;AAAA,EAEA,IAAI,mBAAmE;AACrE,WAAO;AAAA,MACL,YAAY,KAAK,SAAS;AAAA,MAC1B,SAAS,KAAK,SAAS;AAAA,IACzB;AAAA,EACF;AAAA,EAES,oBAAoB;AAAA,EAE7B,MAAM,cAAc,OAAiB,SAA8B;AACjE,UAAM,cAAc,MAAM,KAAK,QAAQ,OAAO,OAAO;AAErD,WAAO;AAAA,MACL;AAAA,MACA,YAAY;AAAA,IACd;AAAA,EACF;AAAA,EAEA,aACE,oBACA;AACA,WAAO,IAAI;AAAA,MACT,OAAO,OAAO,CAAC,GAAG,KAAK,UAAU,kBAAkB;AAAA,IACrD;AAAA,EACF;AACF;AAEA,IAAM,yCAAyCC,GAAE,MAAMA,GAAE,MAAMA,GAAE,OAAO,CAAC,CAAC;;;ACjJ1E,SAAS,KAAAC,WAAS;AAqDX,IAAM,iCAAN,MAAM,wCACH,cAGV;AAAA,EACE,YAAY,UAAkD;AAC5D,UAAM,EAAE,SAAS,CAAC;AAAA,EACpB;AAAA,EAES,WAAW;AAAA,EACpB,IAAI,YAAY;AACd,WAAO,KAAK,SAAS;AAAA,EACvB;AAAA,EAES,oBAAoB;AAAA,EACpB,YAAY;AAAA,EACZ,oBAAoB;AAAA,EAE7B,MAAM,QACJ,QACA,aAC4C;AAC5C,UAAM,MAAM,KAAK,SAAS,OAAO,IAAI,4BAA4B;AACjE,UAAM,cAAc,aAAa,KAAK;AAEtC,WAAO,yBAAyB;AAAA,MAC9B,OAAO,IAAI;AAAA,MACX,UAAU,IAAI;AAAA,MACd,MAAM,YACJ,cAAc;AAAA,QACZ,KAAK,IAAI,YAAY,IAAI,KAAK,SAAS,KAAK,EAAE;AAAA,QAC9C,SAAS,IAAI,QAAQ;AAAA,UACnB,cAAc,YAAY;AAAA,UAC1B,YAAY,YAAY;AAAA,UACxB,KAAK,YAAY;AAAA,UACjB,QAAQ,YAAY;AAAA,QACtB,CAAC;AAAA,QACD,MAAM;AAAA,UACJ,QAAQ;AAAA,UACR,OAAO,KAAK,SAAS;AAAA,UACrB,OAAO,KAAK,SAAS;AAAA,UACrB,aAAa,KAAK,SAAS;AAAA,UAC3B,oBAAoB,KAAK,SAAS;AAAA,UAClC,gBAAgB,KAAK,SAAS;AAAA,UAC9B,UAAU,KAAK,SAAS;AAAA,UACxB,sBAAsB,KAAK,SAAS;AAAA,UACpC,WAAW,KAAK,SAAS;AAAA,UACzB,SAAS;AAAA,YACP,WAAW;AAAA,YACX,gBAAgB;AAAA,UAClB;AAAA,QACF;AAAA,QACA,uBAAuB;AAAA,QACvB,2BAA2B;AAAA,UACzB,UAAU,uCAAuC;AAAA,QACnD;AAAA,QACA;AAAA,MACF,CAAC;AAAA,IACL,CAAC;AAAA,EACH;AAAA,EAEA,IAAI,mBAAoE;AACtE,UAAM,yBAAwC;AAAA,MAC5C,GAAG;AAAA,MAEH;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,IACF;AAEA,WAAO,OAAO;AAAA,MACZ,OAAO,QAAQ,KAAK,QAAQ,EAAE;AAAA,QAAO,CAAC,CAAC,GAAG,MACxC,uBAAuB,SAAS,GAAG;AAAA,MACrC;AAAA,IACF;AAAA,EACF;AAAA,EAEA,MAAM,gBAAgB,QAAgB,SAA8B;AAClE,WAAO,KAAK;AAAA,MACV,MAAM,KAAK,QAAQ,QAAQ,OAAO;AAAA,IACpC;AAAA,EACF;AAAA,EAEA,sBAAsB,aAAsB;AAC1C,WAAO,KAAK;AAAA,MACV,cAAc;AAAA,QACZ,OAAO;AAAA,QACP,QAAQ,UAAU,uCAAuC;AAAA,MAC3D,CAAC;AAAA,IACH;AAAA,EACF;AAAA,EAEA,8BACE,aACA;AACA,WAAO;AAAA,MACL;AAAA,MACA,uBAAuB,YAAY,IAAI,CAAC,cAAc;AAAA,QACpD,MAAM,SAAS;AAAA,QACf,cAAc;AAAA,MAChB,EAAE;AAAA,IACJ;AAAA,EACF;AAAA,EAEA,iBAAuB;AACrB,WAAO;AAAA,EACT;AAAA,EAEA,mBACE,gBAMA;AACA,WAAO,IAAI,kCAAkC;AAAA,MAC3C,OAAO;AAAA;AAAA,MACP;AAAA,IACF,CAAC;AAAA,EACH;AAAA,EAEA,aACE,oBACA;AACA,WAAO,IAAI;AAAA,MACT,OAAO,OAAO,CAAC,GAAG,KAAK,UAAU,kBAAkB;AAAA,IACrD;AAAA,EACF;AACF;AAEA,IAAM,0CAA0CC,IAAE;AAAA,EAChDA,IAAE,OAAO;AAAA,IACP,gBAAgBA,IAAE,OAAO;AAAA,EAC3B,CAAC;AACH;;;AFhLO,SAASC,KACd,UAGA;AACA,SAAO,IAAI,4BAA4B,QAAQ;AACjD;AAsBO,SAASC,eACd,UACA;AACA,SAAO,IAAI,+BAA+B,QAAQ;AACpD;AAwBO,SAASC,cAAa,UAAiD;AAC5E,SAAO,IAAI,8BAA8B,QAAQ;AACnD;;;AGhEO,IAAM,2BAAN,cAAuC,oCAAoC;AAAA,EAChF,YAAY,WAAuD,CAAC,GAAG;AACrE,UAAM;AAAA,MACJ,GAAG;AAAA,MACH,iBAAiB;AAAA,QACf,UAAU;AAAA,QACV,MAAM;AAAA,QACN,MAAM;AAAA,QACN,MAAM;AAAA,MACR;AAAA,IACF,CAAC;AAAA,EACH;AACF;;;ACrBA,SAAS,KAAAC,WAAS;;;ACAlB,gBAAuB,qCACrB,QACkB;AAClB,QAAM,SAAS,OAAO,UAAU;AAChC,MAAI;AACF,WAAO,MAAM;AACX,YAAM,EAAE,MAAM,MAAM,IAAI,MAAM,OAAO,KAAK;AAC1C,UAAI,MAAM;AACR;AAAA,MACF;AACA,YAAM;AAAA,IACR;AAAA,EACF,UAAE;AACA,WAAO,YAAY;AAAA,EACrB;AACF;;;ACfA;AAAA,EAGE;AAAA,OACK;AAcA,IAAM,0BAAN,cAAsC,gBAG3C;AAAA,EACA,cAAc;AACZ,QAAI;AAEJ,UAAM;AAAA,MACJ,MAAM,YAAY;AAChB,iBAAS,aAAa,CAAC,UAAU;AAC/B,cAAI,MAAM,SAAS,SAAS;AAC1B,uBAAW,QAAQ,KAAK;AAAA,UAC1B;AAAA,QACF,CAAC;AAAA,MACH;AAAA,MACA,UAAU,OAAO;AACf,eAAO,KAAK,KAAK;AAAA,MACnB;AAAA,IACF,CAAC;AAAA,EACH;AACF;;;AClCA,eAAsB,uBAAuB;AAAA,EAC3C;AACF,GAEwC;AACtC,QAAM,cAAc,OACjB,YAAY,IAAI,kBAAkB,CAAC,EACnC,YAAY,IAAI,wBAAwB,CAAC;AAE5C,SAAO,qCAAqC,WAAW;AACzD;;;ACdA,SAAS,KAAAC,WAAS;AAIlB,IAAM,0BAA0BC,IAAE,OAAO;AAAA,EACvC,OAAOA,IAAE,OAAO;AAClB,CAAC;AAIM,IAAM,oCAAoC;AAAA,EAC/C;AAAA,IACE,aAAa,UAAU,uBAAuB;AAAA,IAC9C,gBAAgB,CAAC,UAAU,MAAM;AAAA,EACnC;AACF;;;ACfA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;;;ACAA;AAAA;AAAA,cAAAC;AAAA,EAAA,mBAAAC;AAAA,EAAA,YAAAC;AAAA;AAUA,IAAMC,0BACJ;AAMK,SAASC,QAGd;AACA,QAAM,WAAWA,MAAW;AAC5B,SAAO;AAAA,IACL,eAAe,CAAC;AAAA,IAChB,OAAO,QAAQ;AACb,aAAO,EAAE,MAAM,SAAS,OAAO,MAAM,EAAE;AAAA,IACzC;AAAA,EACF;AACF;AAOO,SAASC,eAGd;AACA,SAAO;AAAA,IACL,OAAO,QAAQ;AACb,UAAID,SAAO;AAEX,MAAAA,UAAQ,GAAG,OAAO,UAAUD,uBAAsB;AAAA;AAAA;AAElD,MAAAC,UAAQ;AAER,YAAM,SAAiC,CAAC;AAExC,UAAI,OAAO,OAAO,gBAAgB,UAAU;AAC1C,QAAAA,UAAQ,GAAG,OAAO,WAAW;AAAA;AAAA,MAC/B,OAAO;AAEL,YAAI,eAAe;AACnB,mBAAW,WAAW,OAAO,aAAa;AACxC,kBAAQ,QAAQ,MAAM;AAAA,YACpB,KAAK,QAAQ;AACX,cAAAA,UAAQ,QAAQ;AAChB;AAAA,YACF;AAAA,YACA,KAAK,SAAS;AACZ,cAAAA,UAAQ,QAAQ,YAAY;AAC5B,qBAAO,aAAa,SAAS,CAAC,IAC5B,iCAAiC,QAAQ,KAAK;AAChD;AACA;AAAA,YACF;AAAA,UACF;AAEA,UAAAA,UAAQ,GAAG,OAAO;AAAA;AAAA,QACpB;AAAA,MACF;AAEA,MAAAA,UAAQ;AAAA;AAER,aAAO,EAAE,MAAAA,QAAM,OAAO;AAAA,IACxB;AAAA,IACA,eAAe,CAAC;AAAA,MAAS;AAAA,EAC3B;AACF;AAEO,SAASE,QAGd;AACA,SAAO;AAAA,IACL,OAAO,QAAQ;AACb,UAAIF,SAAO;AAEX,MAAAA,UAAQ,GAAG,OAAO,UAAUD,uBAAsB;AAAA;AAAA;AAGlD,UAAI,eAAe;AACnB,YAAM,SAAiC,CAAC;AAExC,iBAAW,EAAE,MAAM,QAAQ,KAAK,OAAO,UAAU;AAC/C,gBAAQ,MAAM;AAAA,UACZ,KAAK,QAAQ;AACX,YAAAC,UAAQ;AAER,gBAAI,OAAO,YAAY,UAAU;AAC/B,cAAAA,UAAQ;AACR;AAAA,YACF;AAEA,uBAAW,QAAQ,SAAS;AAC1B,sBAAQ,KAAK,MAAM;AAAA,gBACjB,KAAK,QAAQ;AACX,kBAAAA,UAAQ,KAAK;AACb;AAAA,gBACF;AAAA,gBACA,KAAK,SAAS;AACZ,kBAAAA,UAAQ,QAAQ,YAAY;AAC5B,yBAAO,aAAa,SAAS,CAAC,IAC5B,iCAAiC,KAAK,KAAK;AAC7C;AACA;AAAA,gBACF;AAAA,cACF;AAAA,YACF;AAEA;AAAA,UACF;AAAA,UACA,KAAK,aAAa;AAChB,YAAAA,UAAQ,cAAc,wBAAwB,SAAS,MAAM,CAAC;AAC9D;AAAA,UACF;AAAA,UACA,KAAK,QAAQ;AACX,kBAAM,IAAI;AAAA,cACR;AAAA,cACA;AAAA,YACF;AAAA,UACF;AAAA,UACA,SAAS;AACP,kBAAM,mBAA0B;AAChC,kBAAM,IAAI,MAAM,qBAAqB,gBAAgB,EAAE;AAAA,UACzD;AAAA,QACF;AAEA,QAAAA,UAAQ;AAAA;AAAA;AAAA,MACV;AAEA,MAAAA,UAAQ;AAER,aAAO,EAAE,MAAAA,QAAM,OAAO;AAAA,IACxB;AAAA,IACA,eAAe,CAAC;AAAA,MAAS;AAAA,EAC3B;AACF;;;ADvIO,SAAS,yBACd,gBACuE;AACvE,SAAO;AAAA,IACL,QAAQ,CAAC,YAAY;AAAA,MACnB,MAAM,eAAe,OAAO,MAAM;AAAA,IACpC;AAAA,IACA,eAAe,eAAe;AAAA,EAChC;AACF;AAEO,SAAS,qCACd,wBACgE;AAChE,SAAO;AAAA,IACL,MAAM,MAAM,yBAAyB,uBAAuB,KAAK,CAAC;AAAA,IAElE,aAAa,MACX,yBAAyB,uBAAuB,YAAY,CAAC;AAAA,IAE/D,MAAM,MAAM,yBAAyB,uBAAuB,KAAK,CAAC;AAAA,EACpE;AACF;AAEO,IAAM,OAAO,qCAAqC,0BAAU;AAkC5D,IAAM,UAAU,qCAAqC,qCAAa;AAElE,IAAM,SAAS,qCAAqC,4BAAY;AAChE,IAAM,SAAS,qCAAqC,4BAAY;AAChE,IAAM,aACX,qCAAqC,gCAAgB;AAChD,IAAM,SAAS,qCAAqC,4BAAY;AAChE,IAAM,UAAU,qCAAqC,6BAAa;AAClE,IAAM,SAAS,qCAAqC,4BAAY;AAChE,IAAM,YAAY;;;AEhFzB,SAAS,KAAAG,WAAS;AA0BX,IAAM,oBAAN,MAAkD;AAAA,EAC9C;AAAA,EAET,YAAY,MAAwB,IAAI,yBAAyB,GAAG;AAClE,SAAK,MAAM;AAAA,EACb;AAAA,EAEA,MAAM,gBACJC,QACA,aACuC;AACvC,UAAM,MAAM,KAAK;AACjB,UAAM,cAAc,aAAa,KAAK;AAEtC,WAAO,yBAAyB;AAAA,MAC9B,OAAO,IAAI;AAAA,MACX,UAAU,IAAI;AAAA,MACd,MAAM,YACJ,cAAc;AAAA,QACZ,KAAK,IAAI,YAAY,WAAW;AAAA,QAChC,SAAS,IAAI,QAAQ;AAAA,UACnB,cAAc;AAAA,UACd,YAAY,aAAa;AAAA,UACzB,KAAK,aAAa;AAAA,UAClB,QAAQ;AAAA,QACV,CAAC;AAAA,QACD,MAAM;AAAA,UACJ,SAASA;AAAA,QACX;AAAA,QACA,uBAAuB;AAAA,QACvB,2BAA2B;AAAA,UACzB,UAAU,kCAAkC;AAAA,QAC9C;AAAA,QACA;AAAA,MACF,CAAC;AAAA,IACL,CAAC;AAAA,EACH;AAAA,EAEA,MAAM,SAASA,QAAc;AAC3B,UAAM,WAAW,MAAM,KAAK,gBAAgBA,MAAI;AAChD,WAAO,SAAS;AAAA,EAClB;AACF;AAEA,IAAM,qCAAqCC,IAAE,OAAO;AAAA,EAClD,QAAQA,IAAE,MAAMA,IAAE,OAAO,CAAC;AAC5B,CAAC;;;AChEM,SAAS,wBAAwB,QAAyB;AAC/D,QAAM,QAAQ,IAAI,QAAQ;AAE1B,QAAM,IAAI,SAAS,UAAU;AAE7B,QAAM,QAAQ,QAAW,KAAK;AAE9B,SAAO,MAAM,OAAO;AACtB;AACA,IAAM,aAAa;AAEnB,IAAM,kBAAkB;AAAA,EACtB,SAAS;AAAA,EACT,QACE;AAAA,EACF,SAAS;AAAA,EACT,QAAQ;AAAA,EACR,MAAM;AACR;AAEA,IAAM,UAAN,MAAc;AAAA,EACH,QAAQ,oBAAI,IAAoB;AAAA,EAEzC,IAAI,MAAc,MAAsB;AACtC,UAAM,cAAc,KAAK,eAAe,MAAM,IAAI;AAClD,SAAK,MAAM,IAAI,aAAa,IAAI;AAChC,WAAO;AAAA,EACT;AAAA;AAAA;AAAA;AAAA;AAAA,EAMQ,eAAe,MAAc,MAAc;AACjD,UAAM,WAAW,KAAK,QAAQ,kBAAkB,GAAG;AAEnD,QAAI,CAAC,KAAK,MAAM,IAAI,QAAQ,KAAK,KAAK,MAAM,IAAI,QAAQ,MAAM,MAAM;AAClE,aAAO;AAAA,IACT;AAEA,QAAI,IAAI;AACR,WAAO,KAAK,MAAM,IAAI,GAAG,QAAQ,GAAG,CAAC,EAAE,GAAG;AACxC,UAAI,KAAK,MAAM,IAAI,GAAG,QAAQ,GAAG,CAAC,EAAE,MAAM,MAAM;AAC9C,eAAO,GAAG,QAAQ,GAAG,CAAC;AAAA,MACxB;AAEA;AAAA,IACF;AAEA,WAAO,GAAG,QAAQ,GAAG,CAAC;AAAA,EACxB;AAAA,EAEA,SAAS;AACP,WAAO,MAAM,KAAK,KAAK,KAAK,EACzB,IAAI,CAAC,CAAC,MAAM,IAAI,MAAM,GAAG,IAAI,QAAQ,IAAI,EAAE,EAC3C,KAAK,IAAI;AAAA,EACd;AACF;AAEA,IAAM,0BAA0B;AAAA,EAC9B,MAAM;AAAA,EACN,MAAM;AAAA,EACN,KAAK;AACP;AAEA,SAAS,cAAc,SAAiB;AACtC,QAAM,UAAU,KAAK,UAAU,OAAO,EAAE;AAAA,IACtC;AAAA,IACA,CAAC,MAAM,wBAAwB,CAAC;AAAA,EAClC;AAEA,SAAO,IAAI,OAAO;AACpB;AAEA,SAAS,MAAM,QAAa,MAA0B,OAAwB;AAC5E,QAAM,aAAa,OAAO;AAC1B,QAAM,WAAW,QAAQ;AAEzB,MAAI,OAAO,SAAS,OAAO,OAAO;AAChC,UAAM,QAAQ,OAAO,SAAS,OAAO,OAClC;AAAA,MAAI,CAAC,WAAgB,MACpB,MAAM,WAAW,GAAG,IAAI,GAAG,OAAO,MAAM,EAAE,GAAG,CAAC,IAAI,KAAK;AAAA,IACzD,EACC,KAAK,KAAK;AAEb,WAAO,MAAM,IAAI,UAAU,IAAI;AAAA,EACjC,WAAW,WAAW,QAAQ;AAC5B,WAAO,MAAM,IAAI,UAAU,cAAc,OAAO,KAAK,CAAC;AAAA,EACxD,WAAW,UAAU,QAAQ;AAC3B,UAAM,OAAO,OAAO,KAAK,IAAI,aAAa,EAAE,KAAK,KAAK;AACtD,WAAO,MAAM,IAAI,UAAU,IAAI;AAAA,EACjC,WAAW,eAAe,YAAY,gBAAgB,QAAQ;AAC5D,UAAM,YAAY,OAAO,QAAQ,OAAO,UAAU;AAElD,QAAI,OAAO;AACX,cAAU,QAAQ,CAAC,CAAC,UAAU,UAAU,GAAG,MAAM;AAC/C,YAAM,eAAe;AAAA,QACnB;AAAA,QACA,GAAG,QAAQ,EAAE,GAAG,OAAO,MAAM,EAAE,GAAG,QAAQ;AAAA,QAC1C;AAAA,MACF;AACA,UAAI,IAAI,GAAG;AACT,gBAAQ;AAAA,MACV;AACA,cAAQ,IAAI,cAAc,QAAQ,CAAC,oBAAoB,YAAY;AAAA,IACrE,CAAC;AACD,YAAQ;AAER,WAAO,MAAM,IAAI,UAAU,IAAI;AAAA,EACjC,WAAW,eAAe,WAAW,WAAW,QAAQ;AACtD,UAAM,eAAe;AAAA,MACnB,OAAO;AAAA,MACP,GAAG,QAAQ,EAAE,GAAG,OAAO,MAAM,EAAE;AAAA,MAC/B;AAAA,IACF;AACA,UAAM,OAAO,cAAc,YAAY,eAAe,YAAY;AAClE,WAAO,MAAM,IAAI,UAAU,IAAI;AAAA,EACjC,OAAO;AACL,QAAI,CAAC,gBAAgB,UAAU,GAAG;AAChC,YAAM,IAAI,MAAM,wBAAwB,KAAK,UAAU,MAAM,CAAC,EAAE;AAAA,IAClE;AACA,WAAO,MAAM;AAAA,MACX,aAAa,SAAS,SAAS;AAAA,MAC/B,gBAAgB,UAAU;AAAA,IAC5B;AAAA,EACF;AACF;;;ARgEO,IAAM,0BAAN,MAAM,iCAGH,cAMV;AAAA,EACE,YACE,WAAiE,CAAC,GAClE;AACA,UAAM,EAAE,SAAS,CAAC;AAClB,SAAK,YAAY,IAAI,kBAAkB,KAAK,SAAS,GAAG;AAAA,EAC1D;AAAA,EAES,WAAW;AAAA,EACpB,IAAI,YAAY;AACd,WAAO;AAAA,EACT;AAAA,EAEA,IAAI,oBAAyC;AAC3C,WAAO,KAAK,SAAS;AAAA,EACvB;AAAA,EAES;AAAA,EAET,MAAM,QACJ,QACA,aACA,SAGmB;AACnB,UAAM,MAAM,KAAK,SAAS,OAAO,IAAI,yBAAyB;AAC9D,UAAM,iBAAiB,QAAQ;AAC/B,UAAM,cAAc,YAAY,KAAK;AAErC,WAAO,yBAAyB;AAAA,MAC9B,OAAO,IAAI;AAAA,MACX,UAAU,IAAI;AAAA,MACd,MAAM,YACJ,cAAc;AAAA,QACZ,KAAK,IAAI,YAAY,aAAa;AAAA,QAClC,SAAS,IAAI,QAAQ;AAAA,UACnB,cAAc,YAAY;AAAA,UAC1B,YAAY,YAAY;AAAA,UACxB,KAAK,YAAY;AAAA,UACjB,QAAQ,YAAY;AAAA,QACtB,CAAC;AAAA,QACD,MAAM;AAAA,UACJ,QAAQ,eAAe;AAAA,UACvB,QAAQ,OAAO;AAAA,UACf,YACE,OAAO,UAAU,OACb,OAAO,QAAQ,OAAO,MAAM,EAAE,IAAI,CAAC,CAAC,IAAI,IAAI,OAAO;AAAA,YACjD,IAAI,CAAC;AAAA,YACL;AAAA,UACF,EAAE,IACF;AAAA,UACN,aAAa,KAAK,SAAS;AAAA,UAC3B,OAAO,KAAK,SAAS;AAAA,UACrB,OAAO,KAAK,SAAS;AAAA,UACrB,OAAO,KAAK,SAAS;AAAA,UACrB,WAAW,KAAK,SAAS;AAAA,UACzB,QAAQ,KAAK,SAAS;AAAA,UACtB,MAAM,KAAK,SAAS;AAAA,UACpB,OAAO,KAAK,SAAS;AAAA,UACrB,WAAW,KAAK,SAAS;AAAA,UACzB,gBAAgB,KAAK,SAAS;AAAA,UAC9B,eAAe,KAAK,SAAS;AAAA,UAC7B,aAAa,KAAK,SAAS;AAAA,UAC3B,kBAAkB,KAAK,SAAS;AAAA,UAChC,mBAAmB,KAAK,SAAS;AAAA,UACjC,gBAAgB,KAAK,SAAS;AAAA,UAC9B,UAAU,KAAK,SAAS;AAAA,UACxB,cAAc,KAAK,SAAS;AAAA,UAC5B,cAAc,KAAK,SAAS;AAAA,UAC5B,SAAS,KAAK,SAAS;AAAA,UACvB,MAAM,KAAK,SAAS;AAAA,UACpB,YAAY,KAAK,SAAS;AAAA,UAC1B,YAAY,KAAK,SAAS;AAAA,UAC1B,SAAS,KAAK,SAAS;AAAA,UACvB,cAAc,KAAK,SAAS;AAAA,UAC5B,SAAS,KAAK,SAAS;AAAA,QACzB;AAAA,QACA,uBAAuB;AAAA,QACvB,2BAA2B,eAAe;AAAA,QAC1C;AAAA,MACF,CAAC;AAAA,IACL,CAAC;AAAA,EACH;AAAA,EAEA,IAAI,mBAEF;AACA,UAAM,yBAAwC;AAAA,MAC5C,GAAG;AAAA,MAEH;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,IACF;AAEA,WAAO,OAAO;AAAA,MACZ,OAAO,QAAQ,KAAK,QAAQ,EAAE;AAAA,QAAO,CAAC,CAAC,GAAG,MACxC,uBAAuB,SAAS,GAAG;AAAA,MACrC;AAAA,IACF;AAAA,EACF;AAAA,EAEA,MAAM,kBAAkB,QAAmD;AACzE,UAAM,SAAS,MAAM,KAAK,UAAU,SAAS,OAAO,IAAI;AACxD,WAAO,OAAO;AAAA,EAChB;AAAA,EAEA,MAAM,gBACJ,QACA,SACA;AACA,WAAO,KAAK;AAAA,MACV,MAAM,KAAK,QAAQ,QAAQ,SAAS;AAAA,QAClC,gBAAgB,iCAAiC;AAAA,MACnD,CAAC;AAAA,IACH;AAAA,EACF;AAAA,EAEA,sBAAsB,aAAsB;AAC1C,WAAO,KAAK;AAAA,MACV,cAAc;AAAA,QACZ,OAAO;AAAA,QACP,QAAQ,UAAU,oCAAoC;AAAA,MACxD,CAAC;AAAA,IACH;AAAA,EACF;AAAA,EAEA,8BAA8B,aAA6C;AACzE,WAAO;AAAA,MACL;AAAA,MACA,uBAAuB;AAAA,QACrB;AAAA,UACE,MAAM,YAAY;AAAA,UAClB,cACE,YAAY,eAAe,YAAY,eAClC,SACD,YAAY,gBACT,WACA;AAAA,QACX;AAAA,MACF;AAAA,MACA,OAAO;AAAA,QACL,cAAc,YAAY;AAAA,QAC1B,kBAAkB,YAAY;AAAA,QAC9B,aACE,YAAY,mBAAmB,YAAY;AAAA,MAC/C;AAAA,IACF;AAAA,EACF;AAAA,EAEA,aAAa,QAAkC,SAA8B;AAC3E,WAAO,KAAK,QAAQ,QAAQ,SAAS;AAAA,MACnC,gBAAgB,iCAAiC;AAAA,IACnD,CAAC;AAAA,EACH;AAAA,EAEA,iBAAiB,OAAgB;AAC/B,WAAQ,MAAkC;AAAA,EAC5C;AAAA,EAEA,wBACE,gBAGA;AACA,WAAO,gBAAgB,iBACnB,IAAI,6BAA6B;AAAA,MAC/B,OAAO,eAAe,WAAW,IAAI;AAAA,MACrC,UAAU;AAAA,IACZ,CAAC,IACD,IAAI,6BAA6B;AAAA,MAC/B,OAAO;AAAA,MACP,UAAU;AAAA,IACZ,CAAC;AAAA,EACP;AAAA,EAEA,eAAe,QAAoD;AAEjE,QAAI,KAAK,SAAS,WAAW,MAAM;AACjC,aAAO;AAAA,IACT;AAEA,UAAM,UAAU,wBAAwB,OAAO,cAAc,CAAC;AAE9D,WAAO,KAAK,aAAa;AAAA,MACvB;AAAA,IACF,CAAC;AAAA,EACH;AAAA,EAEA,IAAY,yBAAyF;AACnG,WAAO,KAAK,SAAS,kBAAkB;AAAA,EACzC;AAAA,EAEA,iBAKE;AACA,WAAO,KAAK,mBAAmB,KAAK,uBAAuB,KAAK,CAAC;AAAA,EACnE;AAAA,EAEA,wBAKE;AACA,WAAO,KAAK,mBAAmB,KAAK,uBAAuB,YAAY,CAAC;AAAA,EAC1E;AAAA,EAEA,iBAKE;AACA,WAAO,KAAK,mBAAmB,KAAK,uBAAuB,KAAK,CAAC;AAAA,EACnE;AAAA;AAAA;AAAA;AAAA,EAKA,mBACE,gBASA;AACA,WAAO,IAAI,iCAAiC;AAAA,MAC1C,OAAO,KAAK,aAAa;AAAA,QACvB,eAAe;AAAA,UACb,GAAI,KAAK,SAAS,iBAAiB,CAAC;AAAA,UACpC,GAAG,eAAe;AAAA,QACpB;AAAA,MACF,CAAC;AAAA,MACD;AAAA,IACF,CAAC;AAAA,EACH;AAAA,EAEA,aACE,oBAGA;AACA,WAAO,IAAI;AAAA,MACT,OAAO,OAAO,CAAC,GAAG,KAAK,UAAU,kBAAkB;AAAA,IACrD;AAAA,EACF;AACF;AAEA,IAAM,uCAAuCC,IAAE,OAAO;AAAA,EACpD,SAASA,IAAE,OAAO;AAAA,EAClB,MAAMA,IAAE,QAAQ,IAAI;AAAA,EACpB,qBAAqBA,IAAE,OAAO;AAAA,IAC5B,mBAAmBA,IAAE,OAAO;AAAA,IAC5B,YAAYA,IAAE,QAAQ;AAAA,IACtB,YAAYA,IAAE,MAAMA,IAAE,OAAO,CAAC;AAAA,IAC9B,UAAUA,IAAE,OAAO;AAAA,IACnB,cAAcA,IAAE,OAAO;AAAA,IACvB,cAAcA,IAAE,OAAO;AAAA,IACvB,OAAOA,IAAE,OAAO;AAAA,IAChB,OAAOA,IAAE,OAAO;AAAA,IAChB,QAAQA,IAAE,OAAO;AAAA,IACjB,WAAWA,IAAE,OAAO;AAAA,IACpB,SAASA,IAAE,OAAO;AAAA,IAClB,aAAaA,IAAE,QAAQ;AAAA,IACvB,kBAAkBA,IAAE,OAAO;AAAA,IAC3B,eAAeA,IAAE,OAAO;AAAA,IACxB,gBAAgBA,IAAE,OAAO;AAAA,IACzB,MAAMA,IAAE,OAAO;AAAA,IACf,MAAMA,IAAE,MAAMA,IAAE,OAAO,CAAC;AAAA,IACxB,QAAQA,IAAE,QAAQ;AAAA,IAClB,aAAaA,IAAE,OAAO,EAAE,SAAS;AAAA;AAAA,IACjC,OAAOA,IAAE,OAAO;AAAA,IAChB,OAAOA,IAAE,OAAO;AAAA,IAChB,OAAOA,IAAE,OAAO;AAAA,IAChB,WAAWA,IAAE,OAAO;AAAA,EACtB,CAAC;AAAA,EACD,OAAOA,IAAE,OAAO;AAAA,EAChB,QAAQA,IAAE,OAAO;AAAA,EACjB,aAAaA,IAAE,QAAQ;AAAA,EACvB,eAAeA,IAAE,QAAQ;AAAA,EACzB,cAAcA,IAAE,QAAQ;AAAA,EACxB,eAAeA,IAAE,OAAO;AAAA,EACxB,SAASA,IAAE,OAAO;AAAA,IAChB,cAAcA,IAAE,OAAO;AAAA,IACvB,aAAaA,IAAE,OAAO;AAAA,IACtB,sBAAsBA,IAAE,OAAO,EAAE,SAAS;AAAA,IAC1C,wBAAwBA,IAAE,OAAO,EAAE,SAAS;AAAA,IAC5C,WAAWA,IAAE,OAAO,EAAE,SAAS,EAAE,SAAS;AAAA,IAC1C,UAAUA,IAAE,OAAO;AAAA,IACnB,mBAAmBA,IAAE,OAAO,EAAE,SAAS;AAAA,IACvC,qBAAqBA,IAAE,OAAO,EAAE,SAAS;AAAA,EAC3C,CAAC;AAAA,EACD,eAAeA,IAAE,OAAO;AAAA,EACxB,kBAAkBA,IAAE,OAAO;AAAA,EAC3B,kBAAkBA,IAAE,OAAO;AAAA,EAC3B,WAAWA,IAAE,QAAQ;AACvB,CAAC;AAMD,IAAM,gCAAgCA,IAAE,mBAAmB,QAAQ;AAAA,EACjEA,IAAE,OAAO;AAAA,IACP,SAASA,IAAE,OAAO;AAAA,IAClB,MAAMA,IAAE,QAAQ,KAAK;AAAA,EACvB,CAAC;AAAA,EACD;AACF,CAAC;AAMD,eAAe,qCACb,QACwD;AACxD,QAAM,QAAQ,IAAI,WAA2C;AAG7D,yBAAuB,EAAE,OAAO,CAAC,EAC9B,KAAK,OAAO,WAAW;AACtB,QAAI;AACF,uBAAiB,SAAS,QAAQ;AAChC,cAAM,OAAO,MAAM;AAEnB,cAAM,YAAY,UAAU;AAAA,UAC1B,MAAM;AAAA,UACN,QAAQ,UAAU,6BAA6B;AAAA,QACjD,CAAC;AAED,cAAM,KAAK,EAAE,MAAM,SAAS,YAAY,UAAU,CAAC;AAEnD,YAAI,UAAU,MAAM;AAClB,gBAAM,MAAM;AAAA,QACd;AAAA,MACF;AAAA,IACF,SAAS,OAAO;AACd,YAAM,KAAK,EAAE,MAAM,SAAS,MAAM,CAAC;AACnC,YAAM,MAAM;AAAA,IACd;AAAA,EACF,CAAC,EACA,MAAM,CAAC,UAAU;AAChB,UAAM,KAAK,EAAE,MAAM,SAAS,MAAM,CAAC;AACnC,UAAM,MAAM;AAAA,EACd,CAAC;AAEH,SAAO;AACT;AAOO,IAAM,mCAAmC;AAAA;AAAA;AAAA;AAAA,EAI9C,MAAM;AAAA,IACJ,QAAQ;AAAA,IACR,SAAS;AAAA,MACP,UAAU,oCAAoC;AAAA,IAChD;AAAA,EACF;AAAA;AAAA;AAAA;AAAA;AAAA,EAMA,eAAe;AAAA,IACb,QAAQ;AAAA,IACR,SAAS,OAAO,EAAE,SAAS,MACzB,qCAAqC,SAAS,IAAK;AAAA,EACvD;AAGF;;;AStmBA;AAAA;AAAA,aAAAC;AAAA,EAAA;AAAA,sBAAAC;AAAA,EAAA,iBAAAC;AAAA,EAAA;AAAA;AAAA;;;ACAA,SAAS,KAAAC,WAAS;AAyBX,IAAM,6BAAN,MAAM,oCACH,cAEV;AAAA,EACE,YAAY,WAA+C,CAAC,GAAG;AAC7D,UAAM,EAAE,SAAS,CAAC;AAElB,SAAK,YAAY,IAAI,kBAAkB,KAAK,SAAS,GAAG;AAAA,EAC1D;AAAA,EAES,WAAW;AAAA,EACpB,IAAI,YAAY;AACd,WAAO;AAAA,EACT;AAAA,EAES,mBAAmB;AAAA,EAC5B,IAAI,mBAAmB;AACrB,WAAO,KAAK,SAAS,oBAAoB;AAAA,EAC3C;AAAA,EAES,oBAAoB;AAAA,EAC7B,IAAI,aAAa;AACf,WAAO,KAAK,SAAS;AAAA,EACvB;AAAA,EAEiB;AAAA,EAEjB,MAAM,SAASC,QAAc;AAC3B,WAAO,KAAK,UAAU,SAASA,MAAI;AAAA,EACrC;AAAA,EAEA,MAAM,QACJ,OACA,aACwC;AACxC,QAAI,MAAM,SAAS,KAAK,kBAAkB;AACxC,YAAM,IAAI;AAAA,QACR,6CAA6C,KAAK,gBAAgB;AAAA,MACpE;AAAA,IACF;AAEA,UAAM,MAAM,KAAK,SAAS,OAAO,IAAI,yBAAyB;AAC9D,UAAM,cAAc,YAAY,KAAK;AAErC,WAAO,yBAAyB;AAAA,MAC9B,OAAO,KAAK,SAAS,KAAK;AAAA,MAC1B,UAAU,KAAK,SAAS,KAAK;AAAA,MAC7B,MAAM,YACJ,cAAc;AAAA,QACZ,KAAK,IAAI,YAAY,YAAY;AAAA,QACjC,SAAS,IAAI,QAAQ;AAAA,UACnB,cAAc,YAAY;AAAA,UAC1B,YAAY,YAAY;AAAA,UACxB,KAAK,YAAY;AAAA,UACjB,QAAQ,YAAY;AAAA,QACtB,CAAC;AAAA,QACD,MAAM,EAAE,SAAS,MAAM,CAAC,EAAE;AAAA,QAC1B,uBAAuB;AAAA,QACvB,2BAA2B;AAAA,UACzB,UAAU,mCAAmC;AAAA,QAC/C;AAAA,QACA;AAAA,MACF,CAAC;AAAA,IACL,CAAC;AAAA,EACH;AAAA,EAEA,IAAI,mBAAgE;AAClE,WAAO;AAAA,MACL,YAAY,KAAK,SAAS;AAAA,IAC5B;AAAA,EACF;AAAA,EAEA,MAAM,cAAc,OAAiB,SAA8B;AACjE,UAAM,cAAc,MAAM,KAAK,QAAQ,OAAO,OAAO;AAErD,WAAO;AAAA,MACL;AAAA,MACA,YAAY,CAAC,YAAY,SAAS;AAAA,IACpC;AAAA,EACF;AAAA,EAEA,aACE,oBACA;AACA,WAAO,IAAI;AAAA,MACT,OAAO,OAAO,CAAC,GAAG,KAAK,UAAU,kBAAkB;AAAA,IACrD;AAAA,EACF;AACF;AAEA,IAAM,sCAAsCC,IAAE,OAAO;AAAA,EACnD,WAAWA,IAAE,MAAMA,IAAE,OAAO,CAAC;AAC/B,CAAC;;;ACrHD;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAKO,IAAM,OAAe;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAiCrB,IAAM,YAAoB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAuC1B,IAAM,OAAe;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;;;AF5DrB,SAASC,KAAI,UAAsD;AACxE,SAAO,IAAI,yBAAyB,QAAQ;AAC9C;AAEO,SAAS,wBACd,WAAiE,CAAC,GAClE;AACA,SAAO,IAAI,wBAA6C,QAAQ;AAClE;AAEO,SAASC,cACd,WAA+C,CAAC,GAChD;AACA,SAAO,IAAI,2BAA2B,QAAQ;AAChD;AAEO,SAASC,WACd,MAAwB,IAAI,yBAAyB,GACrD;AACA,SAAO,IAAI,kBAAkB,GAAG;AAClC;;;AG3BO,IAAM,uBAAN,cAAmC,oCAAoC;AAAA,EAC5E,YACE,WAEI,CAAC,GACL;AACA,UAAM;AAAA,MACJ,GAAG;AAAA,MACH,SAAS;AAAA,QACP,aAAa,WAAW;AAAA,UACtB,QAAQ,SAAS;AAAA,UACjB,yBAAyB;AAAA,UACzB,aAAa;AAAA,QACf,CAAC;AAAA,MACH;AAAA,MACA,iBAAiB;AAAA,QACf,UAAU;AAAA,QACV,MAAM;AAAA,QACN,MAAM;AAAA,QACN,MAAM;AAAA,MACR;AAAA,IACF,CAAC;AAAA,EACH;AACF;;;ACjCA;AAAA;AAAA,aAAAC;AAAA,EAAA,uBAAAC;AAAA;;;ACAA,SAAS,KAAAC,WAAS;AA+CX,IAAM,kBAAN,MAAM,yBACH,cAEV;AAAA,EACE,YAAY,UAAmC;AAC7C,UAAM,EAAE,SAAS,CAAC;AAAA,EACpB;AAAA,EAES,WAAW;AAAA,EAEpB,IAAI,YAAY;AACd,WAAO,KAAK,SAAS;AAAA,EACvB;AAAA,EAEA,MAAc,QACZC,QACA,aAC6B;AAC7B,UAAM,MAAM,KAAK,SAAS,OAAO,IAAI,qBAAqB;AAC1D,UAAM,cAAc,YAAY,KAAK;AAErC,WAAO,yBAAyB;AAAA,MAC9B,OAAO,IAAI;AAAA,MACX,UAAU,IAAI;AAAA,MACd,MAAM,YAAY;AAChB,cAAM,WAAW,IAAI,SAAS;AAC9B,iBAAS,OAAO,QAAQA,MAAI;AAC5B,iBAAS,OAAO,SAAS,KAAK,SAAS,KAAK;AAC5C,iBAAS,OAAO,UAAU,KAAK;AAC/B,iBAAS,OAAO,oBAAoB,MAAM;AAE1C,YAAI,KAAK,SAAS,SAAS,MAAM;AAC/B,mBAAS,OAAO,SAAS,KAAK,SAAS,MAAM,SAAS,CAAC;AAAA,QACzD;AACA,YAAI,KAAK,SAAS,QAAQ,MAAM;AAC9B,mBAAS,OAAO,QAAQ,KAAK,SAAS,KAAK,SAAS,CAAC;AAAA,QACvD;AACA,YAAI,KAAK,SAAS,UAAU,MAAM;AAChC,mBAAS,OAAO,UAAU,KAAK,SAAS,OAAO,SAAS,CAAC;AAAA,QAC3D;AAEA,eAAO,UAAU;AAAA,UACf,KAAK,IAAI,YAAY,YAAY;AAAA,UACjC,SAAS,IAAI,QAAQ;AAAA,YACnB,cAAc,YAAY;AAAA,YAC1B,YAAY,YAAY;AAAA,YACxB,KAAK,YAAY;AAAA,YACjB,QAAQ,YAAY;AAAA,UACtB,CAAC;AAAA,UACD,MAAM;AAAA,YACJ,SAAS;AAAA,YACT,QAAQ;AAAA,cACN,MAAAA;AAAA,cACA,OAAO,KAAK,SAAS;AAAA,cACrB,OAAO,KAAK,SAAS;AAAA,cACrB,MAAM,KAAK,SAAS;AAAA,cACpB,QAAQ,KAAK,SAAS;AAAA,YACxB;AAAA,UACF;AAAA,UACA,uBAAuB,+BAA+B;AAAA,UACtD,2BAA2B;AAAA,YACzB,UAAU,wBAAwB;AAAA,UACpC;AAAA,UACA;AAAA,QACF,CAAC;AAAA,MACH;AAAA,IACF,CAAC;AAAA,EACH;AAAA,EAEA,IAAI,mBAAqD;AACvD,WAAO;AAAA,MACL,OAAO,KAAK,SAAS;AAAA,MACrB,OAAO,KAAK,SAAS;AAAA,MACrB,MAAM,KAAK,SAAS;AAAA,MACpB,QAAQ,KAAK,SAAS;AAAA,IACxB;AAAA,EACF;AAAA,EAEA,MAAM,yBAAyBA,QAAc,SAA8B;AACzE,UAAM,WAAW,MAAM,KAAK,QAAQA,QAAM,OAAO;AACjD,WAAO,mBAAmB,SAAS,KAAK;AAAA,EAC1C;AAAA,EAEA,aAAa,oBAAsD;AACjE,WAAO,IAAI,iBAAgB;AAAA,MACzB,GAAG,KAAK;AAAA,MACR,GAAG;AAAA,IACL,CAAC;AAAA,EACH;AACF;AAEA,IAAM,2BAA2BC,IAAE,OAAO;AAAA,EACxC,OAAOA,IAAE,OAAO;AAAA,EAChB,WAAWA,IAAE;AAAA,IACXA,IAAE,OAAO;AAAA,MACP,UAAUA,IAAE,OAAO;AAAA,MACnB,OAAOA,IAAE,OAAO;AAAA,MAChB,MAAMA,IAAE,OAAO;AAAA,IACjB,CAAC;AAAA,EACH;AAAA,EACA,MAAMA,IAAE,OAAO;AACjB,CAAC;;;AD5IM,SAASC,KACd,UAGA;AACA,SAAO,IAAI,qBAAqB,QAAQ;AAC1C;AASO,SAASC,iBAAgB,UAAmC;AACjE,SAAO,IAAI,gBAAgB,QAAQ;AACrC;;;AEfO,IAAM,0BAAN,cAAsC,oCAAoC;AAAA,EAC/E,YACE,WAEI,CAAC,GACL;AACA,UAAM;AAAA,MACJ,GAAG;AAAA,MACH,SAAS;AAAA,QACP,eAAe,UAAU,WAAW;AAAA,UAClC,QAAQ,SAAS;AAAA,UACjB,yBAAyB;AAAA,UACzB,aAAa;AAAA,QACf,CAAC,CAAC;AAAA,MACJ;AAAA,MACA,iBAAiB;AAAA,QACf,UAAU;AAAA,QACV,MAAM;AAAA,QACN,MAAM;AAAA,QACN,MAAM;AAAA,MACR;AAAA,IACF,CAAC;AAAA,EACH;AACF;;;ACjCA,SAAS,KAAAC,WAAS;;;ACMlB,eAAsB,sCAAyC;AAAA,EAC7D;AAAA,EACA;AACF,GAGqC;AACnC,QAAM,QAAQ,IAAI,WAAqB;AAGvC,yBAAuB,EAAE,OAAO,CAAC,EAC9B,KAAK,OAAO,WAAW;AACtB,QAAI;AACF,uBAAiB,SAAS,QAAQ;AAChC,cAAM,OAAO,MAAM;AAEnB,YAAI,SAAS,UAAU;AACrB,gBAAM,MAAM;AACZ;AAAA,QACF;AAEA,cAAM,cAAc,cAAc;AAAA,UAChC,MAAM;AAAA,UACN;AAAA,QACF,CAAC;AAED,YAAI,CAAC,YAAY,SAAS;AACxB,gBAAM,KAAK;AAAA,YACT,MAAM;AAAA,YACN,OAAO,YAAY;AAAA,UACrB,CAAC;AAID;AAAA,QACF;AAEA,cAAM,kBAAkB,YAAY;AAEpC,cAAM,KAAK;AAAA,UACT,MAAM;AAAA,UACN,YAAY;AAAA,QACd,CAAC;AAAA,MACH;AAAA,IACF,SAAS,OAAO;AACd,YAAM,KAAK,EAAE,MAAM,SAAS,MAAM,CAAC;AACnC,YAAM,MAAM;AACZ;AAAA,IACF;AAAA,EACF,CAAC,EACA,MAAM,CAAC,UAAU;AAChB,UAAM,KAAK,EAAE,MAAM,SAAS,MAAM,CAAC;AACnC,UAAM,MAAM;AACZ;AAAA,EACF,CAAC;AAEH,SAAO;AACT;;;AC5DO,IAAM,mCACX,CAAI,WACJ,CAAC,EAAE,SAAS,MACV,sCAAsC;AAAA,EACpC,QAAQ,SAAS;AAAA,EACjB;AACF,CAAC;;;ACCE,SAASC,SAGd;AACA,SAAO;AAAA,IACL,QAAQ,CAAC,WAAW,CAAC,EAAE,MAAM,QAAQ,SAAS,OAAO,CAAC;AAAA,IACtD,eAAe,CAAC;AAAA,EAClB;AACF;AAKO,SAASC,gBAGd;AACA,SAAO;AAAA,IACL,OAAO,QAAQ;AACb,YAAM,WAA8B,CAAC;AAErC,UAAI,OAAO,UAAU,MAAM;AACzB,iBAAS,KAAK,EAAE,MAAM,UAAU,SAAS,OAAO,OAAO,CAAC;AAAA,MAC1D;AAEA,YAAMA,gBAAc,wBAAwB,OAAO,aAAa,MAAM;AACtE,eAAS,KAAK,EAAE,MAAM,QAAQ,SAASA,cAAY,CAAC;AAEpD,aAAO;AAAA,IACT;AAAA,IACA,eAAe,CAAC;AAAA,EAClB;AACF;AAKO,SAASC,SAGd;AACA,SAAO;AAAA,IACL,OAAO,QAAQ;AACb,YAAM,WAA8B,CAAC;AAErC,UAAI,OAAO,UAAU,MAAM;AACzB,iBAAS,KAAK,EAAE,MAAM,UAAU,SAAS,OAAO,OAAO,CAAC;AAAA,MAC1D;AAEA,iBAAW,EAAE,MAAM,QAAQ,KAAK,OAAO,UAAU;AAC/C,gBAAQ,MAAM;AAAA,UACZ,KAAK,QAAQ;AACX,kBAAM,cAAc,wBAAwB,SAAS,MAAM;AAC3D,qBAAS,KAAK,EAAE,MAAM,QAAQ,SAAS,YAAY,CAAC;AACpD;AAAA,UACF;AAAA,UACA,KAAK,aAAa;AAChB,qBAAS,KAAK;AAAA,cACZ,MAAM;AAAA,cACN,SAAS,wBAAwB,SAAS,MAAM;AAAA,YAClD,CAAC;AACD;AAAA,UACF;AAAA,UACA,KAAK,QAAQ;AACX,kBAAM,IAAI;AAAA,cACR;AAAA,cACA;AAAA,YACF;AAAA,UACF;AAAA,UACA,SAAS;AACP,kBAAM,mBAA0B;AAChC,kBAAM,IAAI,MAAM,qBAAqB,gBAAgB,EAAE;AAAA,UACzD;AAAA,QACF;AAAA,MACF;AAEA,aAAO;AAAA,IACT;AAAA,IACA,eAAe,CAAC;AAAA,EAClB;AACF;;;AC1FA,SAAS,KAAAC,WAAS;AAQlB,IAAM,yBAAyBC,IAAE,OAAO;AAAA,EACtC,QAAQA,IAAE,QAAQ,OAAO;AAAA,EACzB,SAASA,IAAE,OAAO;AAAA,EAClB,MAAMA,IAAE,OAAO;AAAA,EACf,OAAOA,IAAE,OAAO,EAAE,SAAS;AAAA,EAC3B,MAAMA,IAAE,OAAO;AACjB,CAAC;AAIM,IAAM,mCACX,+BAA+B;AAAA,EAC7B,aAAa,UAAU,sBAAsB;AAAA,EAC7C,gBAAgB,CAAC,UAAU,MAAM;AACnC,CAAC;;;AJmDI,IAAM,mBAAN,MAAM,0BACH,cAGV;AAAA,EACE,YAAY,UAAoC;AAC9C,UAAM,EAAE,SAAS,CAAC;AAAA,EACpB;AAAA,EAES,WAAW;AAAA,EACpB,IAAI,YAAY;AACd,WAAO,KAAK,SAAS;AAAA,EACvB;AAAA,EAES,oBAAoB;AAAA,EACpB,YAAY;AAAA,EACZ,oBAAoB;AAAA,EAE7B,MAAM,QACJ,QACA,aACA,SAGA;AACA,UAAM,MAAM,KAAK,SAAS,OAAO,IAAI,wBAAwB;AAC7D,UAAM,cAAc,YAAY,KAAK;AACrC,UAAM,SAAS,QAAQ,eAAe;AACtC,UAAMC,6BAA4B,QAAQ,eAAe;AAEzD,WAAO,yBAAyB;AAAA,MAC9B,OAAO,IAAI;AAAA,MACX,UAAU,IAAI;AAAA,MACd,MAAM,YACJ,cAAc;AAAA,QACZ,KAAK,IAAI,YAAY,mBAAmB;AAAA,QACxC,SAAS,IAAI,QAAQ;AAAA,UACnB,cAAc,YAAY;AAAA,UAC1B,YAAY,YAAY;AAAA,UACxB,KAAK,YAAY;AAAA,UACjB,QAAQ,YAAY;AAAA,QACtB,CAAC;AAAA,QACD,MAAM;AAAA,UACJ;AAAA,UACA,UAAU;AAAA,UACV,OAAO,KAAK,SAAS;AAAA,UACrB,aAAa,KAAK,SAAS;AAAA,UAC3B,OAAO,KAAK,SAAS;AAAA,UACrB,YAAY,KAAK,SAAS;AAAA,UAC1B,WAAW,KAAK,SAAS;AAAA,UACzB,aAAa,KAAK,SAAS;AAAA,QAC7B;AAAA,QACA,uBAAuB;AAAA,QACvB,2BAAAA;AAAA,QACA;AAAA,MACF,CAAC;AAAA,IACL,CAAC;AAAA,EACH;AAAA,EAEA,IAAI,mBAAsD;AACxD,UAAM,yBAAwC;AAAA,MAC5C,GAAG;AAAA,MAEH;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,IACF;AAEA,WAAO,OAAO;AAAA,MACZ,OAAO,QAAQ,KAAK,QAAQ,EAAE;AAAA,QAAO,CAAC,CAAC,GAAG,MACxC,uBAAuB,SAAS,GAAG;AAAA,MACrC;AAAA,IACF;AAAA,EACF;AAAA,EAEA,MAAM,gBACJ,QACA,SACA;AACA,WAAO,KAAK;AAAA,MACV,MAAM,KAAK,QAAQ,QAAQ,SAAS;AAAA,QAClC,gBAAgB,0BAA0B;AAAA,MAC5C,CAAC;AAAA,IACH;AAAA,EACF;AAAA,EAEA,sBAAsB,aAAsB;AAC1C,WAAO,KAAK;AAAA,MACV,cAAc;AAAA,QACZ,OAAO;AAAA,QACP,QAAQ,UAAU,yBAAyB;AAAA,MAC7C,CAAC;AAAA,IACH;AAAA,EACF;AAAA,EAEA,8BAA8B,aAAkC;AAC9D,WAAO;AAAA,MACL;AAAA,MACA,uBAAuB,YAAY,QAAQ,IAAI,CAAC,YAAY;AAAA,QAC1D,MAAM,OAAO,QAAQ;AAAA,QACrB,cAAc,KAAK,sBAAsB,OAAO,aAAa;AAAA,MAC/D,EAAE;AAAA,IACJ;AAAA,EACF;AAAA,EAEQ,sBACN,cAC4B;AAC5B,YAAQ,cAAc;AAAA,MACpB,KAAK;AACH,eAAO;AAAA,MACT,KAAK;AAAA,MACL,KAAK;AACH,eAAO;AAAA,MACT;AACE,eAAO;AAAA,IACX;AAAA,EACF;AAAA,EAEA,aAAa,QAA2B,SAA8B;AACpE,WAAO,KAAK,QAAQ,QAAQ,SAAS;AAAA,MACnC,gBAAgB,0BAA0B;AAAA,IAC5C,CAAC;AAAA,EACH;AAAA,EAEA,iBAAiB,OAAgB;AAC/B,UAAM,QAAQ;AACd,WAAO,MAAM,QAAQ,CAAC,EAAE,MAAM,WAAW;AAAA,EAC3C;AAAA,EAEA,iBAAiB;AACf,WAAO,KAAK,mBAAmBC,OAAK,CAAC;AAAA,EACvC;AAAA,EAEA,wBAAwB;AACtB,WAAO,KAAK,mBAAmBC,cAAY,CAAC;AAAA,EAC9C;AAAA,EAEA,iBAAiB;AACf,WAAO,KAAK,mBAAmBC,OAAK,CAAC;AAAA,EACvC;AAAA,EAEA,iBAAuB;AACrB,WAAO;AAAA,EACT;AAAA,EAEA,mBACE,gBASA;AACA,WAAO,IAAI,iCAAiC;AAAA,MAC1C,OAAO;AAAA;AAAA,MACP;AAAA,IACF,CAAC;AAAA,EACH;AAAA,EAEA,aAAa,oBAAuD;AAClE,WAAO,IAAI;AAAA,MACT,OAAO,OAAO,CAAC,GAAG,KAAK,UAAU,kBAAkB;AAAA,IACrD;AAAA,EACF;AACF;AAEA,IAAM,4BAA4BC,IAAE,OAAO;AAAA,EACzC,IAAIA,IAAE,OAAO;AAAA,EACb,QAAQA,IAAE,OAAO;AAAA,EACjB,SAASA,IAAE,OAAO;AAAA,EAClB,OAAOA,IAAE,OAAO;AAAA,EAChB,SAASA,IAAE;AAAA,IACTA,IAAE,OAAO;AAAA,MACP,OAAOA,IAAE,OAAO;AAAA,MAChB,SAASA,IAAE,OAAO;AAAA,QAChB,MAAMA,IAAE,KAAK,CAAC,QAAQ,WAAW,CAAC;AAAA,QAClC,SAASA,IAAE,OAAO;AAAA,MACpB,CAAC;AAAA,MACD,eAAeA,IAAE,KAAK,CAAC,QAAQ,UAAU,cAAc,CAAC;AAAA,IAC1D,CAAC;AAAA,EACH;AAAA,EACA,OAAOA,IAAE,OAAO;AAAA,IACd,eAAeA,IAAE,OAAO;AAAA,IACxB,mBAAmBA,IAAE,OAAO;AAAA,IAC5B,cAAcA,IAAE,OAAO;AAAA,EACzB,CAAC;AACH,CAAC;AAID,IAAM,+BAA+BA,IAAE,OAAO;AAAA,EAC5C,IAAIA,IAAE,OAAO;AAAA,EACb,QAAQA,IAAE,OAAO,EAAE,SAAS;AAAA,EAC5B,SAASA,IAAE,OAAO,EAAE,SAAS;AAAA,EAC7B,OAAOA,IAAE,OAAO;AAAA,EAChB,SAASA,IAAE;AAAA,IACTA,IAAE,OAAO;AAAA,MACP,OAAOA,IAAE,OAAO;AAAA,MAChB,OAAOA,IAAE,OAAO;AAAA,QACd,MAAMA,IAAE,KAAK,CAAC,aAAa,MAAM,CAAC,EAAE,SAAS,EAAE,SAAS;AAAA,QACxD,SAASA,IAAE,OAAO,EAAE,SAAS,EAAE,SAAS;AAAA,MAC1C,CAAC;AAAA,MACD,eAAeA,IACZ,KAAK,CAAC,QAAQ,UAAU,cAAc,CAAC,EACvC,SAAS,EACT,SAAS;AAAA,IACd,CAAC;AAAA,EACH;AACF,CAAC;AAWM,IAAM,4BAA4B;AAAA;AAAA;AAAA;AAAA,EAIvC,MAAM;AAAA,IACJ,QAAQ;AAAA,IACR,SAAS,0BAA0B,UAAU,yBAAyB,CAAC;AAAA,EACzE;AAAA;AAAA;AAAA;AAAA,EAKA,mBAAmB;AAAA,IACjB,QAAQ;AAAA,IACR,SAAS;AAAA,MACP,UAAU,4BAA4B;AAAA,IACxC;AAAA,EACF;AACF;;;AK3TA;AAAA;AAAA,aAAAC;AAAA,EAAA;AAAA,sBAAAC;AAAA;;;ACAA,SAAS,KAAAC,WAAS;AAkCX,IAAM,4BAAN,MAAM,mCACH,cAEV;AAAA,EACE,YAAY,UAA6C;AACvD,UAAM,EAAE,SAAS,CAAC;AAAA,EACpB;AAAA,EAES,WAAW;AAAA,EACpB,IAAI,YAAY;AACd,WAAO,KAAK,SAAS;AAAA,EACvB;AAAA,EAES,mBAAmB;AAAA;AAAA;AAAA;AAAA;AAAA,EAMnB,mBAAmB;AAAA,EAEnB,aAAa;AAAA,EAEtB,MAAM,QACJ,OACA,aACuC;AACvC,QAAI,MAAM,SAAS,KAAK,kBAAkB;AACxC,YAAM,IAAI;AAAA,QACR,2CAA2C,KAAK,gBAAgB;AAAA,MAClE;AAAA,IACF;AAEA,UAAM,MAAM,KAAK,SAAS,OAAO,IAAI,wBAAwB;AAC7D,UAAM,cAAc,YAAY,KAAK;AACrC,UAAM,QAAQ,KAAK,SAAS;AAC5B,UAAM,iBAAiB,KAAK,SAAS,kBAAkB;AAEvD,WAAO,yBAAyB;AAAA,MAC9B,OAAO,KAAK,SAAS,KAAK;AAAA,MAC1B,UAAU,KAAK,SAAS,KAAK;AAAA,MAC7B,MAAM,YACJ,cAAc;AAAA,QACZ,KAAK,IAAI,YAAY,aAAa;AAAA,QAClC,SAAS,IAAI,QAAQ;AAAA,UACnB,cAAc,YAAY;AAAA,UAC1B,YAAY,YAAY;AAAA,UACxB,KAAK,YAAY;AAAA,UACjB,QAAQ,YAAY;AAAA,QACtB,CAAC;AAAA,QACD,MAAM;AAAA,UACJ;AAAA,UACA,OAAO;AAAA,UACP,iBAAiB;AAAA,QACnB;AAAA,QACA,uBAAuB;AAAA,QACvB,2BAA2B;AAAA,UACzB,UAAU,kCAAkC;AAAA,QAC9C;AAAA,QACA;AAAA,MACF,CAAC;AAAA,IACL,CAAC;AAAA,EACH;AAAA,EAEA,IAAI,mBAA+D;AACjE,WAAO;AAAA,MACL,gBAAgB,KAAK,SAAS;AAAA,IAChC;AAAA,EACF;AAAA,EAEA,MAAM,cAAc,OAAiB,SAA8B;AACjE,UAAM,cAAc,MAAM,KAAK,QAAQ,OAAO,OAAO;AAErD,WAAO;AAAA,MACL;AAAA,MACA,YAAY,YAAY,KAAK,IAAI,CAAC,UAAU,MAAM,SAAS;AAAA,IAC7D;AAAA,EACF;AAAA,EAEA,aAAa,oBAAgE;AAC3E,WAAO,IAAI;AAAA,MACT,OAAO,OAAO,CAAC,GAAG,KAAK,UAAU,kBAAkB;AAAA,IACrD;AAAA,EACF;AACF;AAEA,IAAM,qCAAqCC,IAAE,OAAO;AAAA,EAClD,IAAIA,IAAE,OAAO;AAAA,EACb,QAAQA,IAAE,OAAO;AAAA,EACjB,MAAMA,IAAE;AAAA,IACNA,IAAE,OAAO;AAAA,MACP,QAAQA,IAAE,OAAO;AAAA,MACjB,WAAWA,IAAE,MAAMA,IAAE,OAAO,CAAC;AAAA,MAC7B,OAAOA,IAAE,OAAO;AAAA,IAClB,CAAC;AAAA,EACH;AAAA,EACA,OAAOA,IAAE,OAAO;AAAA,EAChB,OAAOA,IAAE,OAAO;AAAA,IACd,eAAeA,IAAE,OAAO;AAAA,IACxB,cAAcA,IAAE,OAAO;AAAA,EACzB,CAAC;AACH,CAAC;;;AD3HM,SAASC,KACd,UAGA;AACA,SAAO,IAAI,wBAAwB,QAAQ;AAC7C;AAEO,SAAS,kBAAkB,UAAoC;AACpE,SAAO,IAAI,iBAAiB,QAAQ;AACtC;AAEO,SAASC,cAAa,UAA6C;AACxE,SAAO,IAAI,0BAA0B,QAAQ;AAC/C;;;AEjBO,IAAM,yBAAN,cAAqC,oCAAoC;AAAA,EAC9E,YAAY,WAAuD,CAAC,GAAG;AACrE,UAAM;AAAA,MACJ,GAAG;AAAA,MACH,iBAAiB;AAAA,QACf,UAAU;AAAA,QACV,MAAM;AAAA,QACN,MAAM;AAAA,QACN,MAAM;AAAA,MACR;AAAA,IACF,CAAC;AAAA,EACH;AACF;;;ACrBA,SAAS,KAAAC,WAAS;;;ACwBX,SAASC,SAA+D;AAC7E,SAAO;AAAA,IACL,QAAQ,CAAC,WAAW,CAAC,EAAE,MAAM,QAAQ,SAAS,OAAO,CAAC;AAAA,IACtD,eAAe,CAAC;AAAA,EAClB;AACF;AAKO,SAASC,gBAGd;AACA,SAAO;AAAA,IACL,OAAO,QAAQ;AACb,YAAM,WAA6B,CAAC;AAEpC,UAAI,OAAO,UAAU,MAAM;AACzB,iBAAS,KAAK;AAAA,UACZ,MAAM;AAAA,UACN,SAAS,OAAO;AAAA,QAClB,CAAC;AAAA,MACH;AAEA,eAAS,KAAK;AAAA,QACZ,MAAM;AAAA,QACN,GAAG,mBAAmB,OAAO,WAAW;AAAA,MAC1C,CAAC;AAED,aAAO;AAAA,IACT;AAAA,IACA,eAAe,CAAC;AAAA,EAClB;AACF;AAKO,SAASC,SAGd;AACA,SAAO;AAAA,IACL,OAAO,QAAQ;AACb,YAAM,WAA6B,CAAC;AAEpC,UAAI,OAAO,UAAU,MAAM;AACzB,iBAAS,KAAK,EAAE,MAAM,UAAU,SAAS,OAAO,OAAO,CAAC;AAAA,MAC1D;AAEA,iBAAW,EAAE,MAAM,QAAQ,KAAK,OAAO,UAAU;AAC/C,gBAAQ,MAAM;AAAA,UACZ,KAAK,QAAQ;AACX,qBAAS,KAAK;AAAA,cACZ,MAAM;AAAA,cACN,GAAG,mBAAmB,OAAO;AAAA,YAC/B,CAAC;AACD;AAAA,UACF;AAAA,UAEA,KAAK,aAAa;AAChB,qBAAS,KAAK;AAAA,cACZ,MAAM;AAAA,cACN,SAAS,wBAAwB,SAAS,MAAM;AAAA,YAClD,CAAC;AACD;AAAA,UACF;AAAA,UAEA,KAAK,QAAQ;AACX,kBAAM,IAAI;AAAA,cACR;AAAA,cACA;AAAA,YACF;AAAA,UACF;AAAA,UAEA,SAAS;AACP,kBAAM,mBAA0B;AAChC,kBAAM,IAAI,MAAM,qBAAqB,gBAAgB,EAAE;AAAA,UACzD;AAAA,QACF;AAAA,MACF;AAEA,aAAO;AAAA,IACT;AAAA,IACA,eAAe,CAAC;AAAA,EAClB;AACF;AAEA,SAAS,mBAAmB,OAAoB;AAC9C,MAAI,OAAO,UAAU,UAAU;AAC7B,WAAO,EAAE,SAAS,OAAO,QAAQ,OAAU;AAAA,EAC7C;AAEA,QAAM,SAAmB,CAAC;AAC1B,MAAI,UAAU;AAEd,aAAW,QAAQ,OAAO;AACxB,QAAI,KAAK,SAAS,QAAQ;AACxB,iBAAW,KAAK;AAAA,IAClB,OAAO;AACL,aAAO,KAAK,iCAAiC,KAAK,KAAK,CAAC;AAAA,IAC1D;AAAA,EACF;AAEA,SAAO,EAAE,SAAS,OAAO;AAC3B;;;AClIA,SAAS,KAAAC,WAAS;AAQlB,IAAM,wBAAwBC,IAAE,OAAO;AAAA,EACrC,OAAOA,IAAE,OAAO;AAClB,CAAC;AAIM,IAAM,kCACX,+BAA+B;AAAA,EAC7B,aAAa,UAAU,qBAAqB;AAAA,EAC5C,gBAAgB,CAAC,UAAU,MAAM;AACnC,CAAC;;;AFiCI,IAAM,kBAAN,MAAM,yBACH,cAEV;AAAA,EACE,YAAY,UAAmC;AAC7C,UAAM,EAAE,SAAS,CAAC;AAAA,EACpB;AAAA,EAES,WAAW;AAAA,EACpB,IAAI,YAAY;AACd,WAAO,KAAK,SAAS;AAAA,EACvB;AAAA,EAES,YAAY;AAAA,EACZ,oBAAoB;AAAA,EACpB,oBAAoB;AAAA,EAE7B,MAAM,QACJ,QACA,aACA,SAGmB;AACnB,UAAM,EAAE,eAAe,IAAI;AAC3B,UAAM,MAAM,KAAK,SAAS,OAAO,IAAI,uBAAuB;AAC5D,UAAM,cAAc,YAAY,KAAK;AAErC,WAAO,yBAAyB;AAAA,MAC9B,OAAO,IAAI;AAAA,MACX,UAAU,IAAI;AAAA,MACd,MAAM,YACJ,cAAc;AAAA,QACZ,KAAK,IAAI,YAAY,WAAW;AAAA,QAChC,SAAS,IAAI,QAAQ;AAAA,UACnB,cAAc,YAAY;AAAA,UAC1B,YAAY,YAAY;AAAA,UACxB,KAAK,YAAY;AAAA,UACjB,QAAQ,YAAY;AAAA,QACtB,CAAC;AAAA,QACD,MAAM;AAAA,UACJ,QAAQ,eAAe;AAAA,UACvB,OAAO,KAAK,SAAS;AAAA,UACrB,UAAU;AAAA,UACV,QAAQ,KAAK,SAAS;AAAA,UACtB,SAAS;AAAA,YACP,UAAU,KAAK,SAAS;AAAA,YACxB,cAAc,KAAK,SAAS;AAAA,YAC5B,cAAc,KAAK,SAAS;AAAA,YAC5B,SAAS,KAAK,SAAS;AAAA,YACvB,SAAS,KAAK,SAAS;AAAA,YACvB,aAAa,KAAK,SAAS;AAAA,YAC3B,aAAa,KAAK,SAAS;AAAA,YAC3B,eAAe,KAAK,SAAS;AAAA,YAC7B,gBAAgB,KAAK,SAAS;AAAA,YAC9B,MAAM,KAAK,SAAS;AAAA,YACpB,MAAM,KAAK,SAAS;AAAA,YACpB,aAAa,KAAK,SAAS;AAAA,YAC3B,OAAO,KAAK,SAAS;AAAA,YACrB,OAAO,KAAK,SAAS;AAAA,YACrB,OAAO,KAAK,SAAS;AAAA,UACvB;AAAA,UACA,UAAU,KAAK,SAAS;AAAA,QAC1B;AAAA,QACA,uBAAuB;AAAA,QACvB,2BAA2B,eAAe;AAAA,QAC1C;AAAA,MACF,CAAC;AAAA,IACL,CAAC;AAAA,EACH;AAAA,EAEA,IAAI,mBAAqD;AACvD,UAAM,yBAAwC;AAAA,MAC5C,GAAG;AAAA,MAEH;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,IACF;AAEA,WAAO,OAAO;AAAA,MACZ,OAAO,QAAQ,KAAK,QAAQ,EAAE;AAAA,QAAO,CAAC,CAAC,GAAG,MACxC,uBAAuB,SAAS,GAAG;AAAA,MACrC;AAAA,IACF;AAAA,EACF;AAAA,EAEA,MAAM,gBACJ,QACA,SACA;AACA,WAAO,KAAK;AAAA,MACV,MAAM,KAAK,QAAQ,QAAQ,SAAS;AAAA,QAClC,gBAAgB,yBAAyB;AAAA,MAC3C,CAAC;AAAA,IACH;AAAA,EACF;AAAA,EAEA,sBAAsB,aAAsB;AAC1C,WAAO,KAAK;AAAA,MACV,cAAc;AAAA,QACZ,OAAO;AAAA,QACP,QAAQ,UAAU,wBAAwB;AAAA,MAC5C,CAAC;AAAA,IACH;AAAA,EACF;AAAA,EAEQ,8BAA8B,aAAiC;AACrE,WAAO;AAAA,MACL;AAAA,MACA,uBAAuB;AAAA,QACrB;AAAA,UACE,MAAM,YAAY,QAAQ;AAAA,UAC1B,cAAc;AAAA,QAChB;AAAA,MACF;AAAA,IACF;AAAA,EACF;AAAA,EAEA,aAAa,QAA0B,SAA8B;AACnE,WAAO,KAAK,QAAQ,QAAQ,SAAS;AAAA,MACnC,gBAAgB,yBAAyB;AAAA,IAC3C,CAAC;AAAA,EACH;AAAA,EAEA,iBAAiB,OAAgB;AAC/B,UAAM,QAAQ;AACd,WAAO,MAAM,SAAS,OAAO,SAAY,MAAM,QAAQ;AAAA,EACzD;AAAA,EAEA,0BACE,gBACA;AACA,WAAO,IAAI,4BAA4B;AAAA,MACrC,OAAO;AAAA,MACP,UAAU;AAAA,IACZ,CAAC;AAAA,EACH;AAAA,EAEA,iCACE,gBACA;AACA,WAAO,IAAI,6BAA6B;AAAA,MACtC,OAAO;AAAA,MACP,UAAU;AAAA,IACZ,CAAC;AAAA,EACH;AAAA,EAEA,wBACE,gBAGA;AACA,WAAO,gBAAgB,iBACnB,IAAI,6BAA6B;AAAA,MAC/B,OAAO,eAAe,WAAW,IAAI;AAAA,MACrC,UAAU;AAAA,IACZ,CAAC,IACD,IAAI,6BAA6B;AAAA,MAC/B,OAAO;AAAA,MACP,UAAU;AAAA,IACZ,CAAC;AAAA,EACP;AAAA,EAEA,iBAAiB;AACf,WAAO,KAAK,mBAAmBC,OAAK,CAAC;AAAA,EACvC;AAAA,EAEA,wBAAwB;AACtB,WAAO,KAAK,mBAAmBC,cAAY,CAAC;AAAA,EAC9C;AAAA,EAEA,iBAAiB;AACf,WAAO,KAAK,mBAAmBC,OAAK,CAAC;AAAA,EACvC;AAAA,EAEA,mBACE,gBAMA;AACA,WAAO,IAAI,iCAAiC;AAAA,MAC1C,OAAO,KAAK,aAAa;AAAA,QACvB,eAAe;AAAA,UACb,GAAI,KAAK,SAAS,iBAAiB,CAAC;AAAA,UACpC,GAAG,eAAe;AAAA,QACpB;AAAA,MACF,CAAC;AAAA,MACD;AAAA,IACF,CAAC;AAAA,EACH;AAAA,EAEA,iBAAiB;AACf,WAAO,KAAK,aAAa,EAAE,QAAQ,OAAO,CAAC;AAAA,EAC7C;AAAA,EAEA,aAAa,oBAAsD;AACjE,WAAO,IAAI;AAAA,MACT,OAAO,OAAO,CAAC,GAAG,KAAK,UAAU,kBAAkB;AAAA,IACrD;AAAA,EACF;AACF;AAEA,IAAM,2BAA2BC,IAAE,OAAO;AAAA,EACxC,OAAOA,IAAE,OAAO;AAAA,EAChB,YAAYA,IAAE,OAAO;AAAA,EACrB,MAAMA,IAAE,QAAQ,IAAI;AAAA,EACpB,SAASA,IAAE,OAAO;AAAA,IAChB,MAAMA,IAAE,OAAO;AAAA,IACf,SAASA,IAAE,OAAO;AAAA,EACpB,CAAC;AAAA,EACD,gBAAgBA,IAAE,OAAO;AAAA,EACzB,eAAeA,IAAE,OAAO,EAAE,SAAS;AAAA,EACnC,mBAAmBA,IAAE,OAAO,EAAE,SAAS;AAAA,EACvC,sBAAsBA,IAAE,OAAO,EAAE,SAAS;AAAA,EAC1C,YAAYA,IAAE,OAAO;AAAA,EACrB,eAAeA,IAAE,OAAO;AAC1B,CAAC;AAID,IAAM,8BAA8BA,IAAE,mBAAmB,QAAQ;AAAA,EAC/DA,IAAE,OAAO;AAAA,IACP,MAAMA,IAAE,QAAQ,KAAK;AAAA,IACrB,OAAOA,IAAE,OAAO;AAAA,IAChB,YAAYA,IAAE,OAAO;AAAA,IACrB,SAASA,IAAE,OAAO;AAAA,MAChB,MAAMA,IAAE,OAAO;AAAA,MACf,SAASA,IAAE,OAAO;AAAA,IACpB,CAAC;AAAA,EACH,CAAC;AAAA,EACDA,IAAE,OAAO;AAAA,IACP,MAAMA,IAAE,QAAQ,IAAI;AAAA,IACpB,OAAOA,IAAE,OAAO;AAAA,IAChB,YAAYA,IAAE,OAAO;AAAA,IACrB,gBAAgBA,IAAE,OAAO;AAAA,IACzB,eAAeA,IAAE,OAAO,EAAE,SAAS;AAAA,IACnC,mBAAmBA,IAAE,OAAO,EAAE,SAAS;AAAA,IACvC,sBAAsBA,IAAE,OAAO,EAAE,SAAS;AAAA,IAC1C,YAAYA,IAAE,OAAO;AAAA,IACrB,eAAeA,IAAE,OAAO;AAAA,EAC1B,CAAC;AACH,CAAC;AASM,IAAM,2BAA2B;AAAA;AAAA;AAAA;AAAA,EAItC,MAAM;AAAA,IACJ,QAAQ;AAAA,IACR,SAAU,OAAO,EAAE,UAAU,KAAK,kBAAkB,MAAM;AACxD,YAAM,eAAe,MAAM,SAAS,KAAK;AAEzC,YAAM,eAAe,cAAc;AAAA,QACjC,MAAM;AAAA,QACN,QAAQ;AAAA,UACNA,IAAE,MAAM;AAAA,YACN;AAAA,YACAA,IAAE,OAAO;AAAA,cACP,MAAMA,IAAE,QAAQ,KAAK;AAAA,cACrB,OAAOA,IAAE,OAAO;AAAA,cAChB,YAAYA,IAAE,OAAO;AAAA,YACvB,CAAC;AAAA,UACH,CAAC;AAAA,QACH;AAAA,MACF,CAAC;AAED,UAAI,CAAC,aAAa,SAAS;AACzB,cAAM,IAAI,aAAa;AAAA,UACrB,SAAS;AAAA,UACT,OAAO,aAAa;AAAA,UACpB,YAAY,SAAS;AAAA,UACrB;AAAA,UACA;AAAA,UACA;AAAA,QACF,CAAC;AAAA,MACH;AAEA,UAAI,aAAa,MAAM,SAAS,OAAO;AACrC,cAAM,IAAI,aAAa;AAAA,UACrB,SAAS;AAAA,UACT,YAAY,SAAS;AAAA,UACrB;AAAA,UACA;AAAA,UACA;AAAA,UACA,aAAa;AAAA,QACf,CAAC;AAAA,MACH;AAEA,aAAO,aAAa;AAAA,IACtB;AAAA,EACF;AAAA;AAAA;AAAA;AAAA;AAAA,EAMA,eAAe;AAAA,IACb,QAAQ;AAAA,IACR,SAAS;AAAA,MACP,UAAU,2BAA2B;AAAA,IACvC;AAAA,EACF;AACF;;;AGxXA,SAAS,KAAAC,WAAS;;;ACAlB;AAAA;AAAA,gBAAAC;AAAA,EAAA,cAAAC;AAAA,EAAA,cAAAC;AAAA,EAAA,eAAAC;AAAA,EAAA,kBAAAC;AAAA,EAAA,eAAAC;AAAA,EAAA,YAAAC;AAAA,EAAA,cAAAC;AAAA,EAAA;AAAA;AAAA;AAYO,SAAS,iCACd,gBACqE;AACrE,SAAO;AAAA,IACL,QAAQ,CAAC,YAAY;AAAA,MACnB,QAAQ,eAAe,OAAO,MAAM;AAAA,IACtC;AAAA,IACA,eAAe,eAAe;AAAA,EAChC;AACF;AAEO,SAAS,6CACd,wBAC8D;AAC9D,SAAO;AAAA,IACL,MAAM,MAAM,iCAAiC,uBAAuB,KAAK,CAAC;AAAA,IAE1E,aAAa,MACX,iCAAiC,uBAAuB,YAAY,CAAC;AAAA,IAEvE,MAAM,MAAM,iCAAiC,uBAAuB,KAAK,CAAC;AAAA,EAC5E;AACF;AAEO,IAAMC,QAAO,6CAA6C,0BAAU;AAkCpE,IAAMC,WACX,6CAA6C,qCAAa;AAErD,IAAMC,UACX,6CAA6C,4BAAY;AACpD,IAAMC,UACX,6CAA6C,4BAAY;AACpD,IAAMC,cACX,6CAA6C,gCAAgB;AACxD,IAAMC,UACX,6CAA6C,4BAAY;AACpD,IAAMC,WACX,6CAA6C,6BAAa;AACrD,IAAMC,UACX,6CAA6C,4BAAY;;;ADNpD,IAAM,wBAAN,MAAM,+BAGH,cAMV;AAAA,EACE,YAAY,UAA8D;AACxE,UAAM,EAAE,SAAS,CAAC;AAAA,EACpB;AAAA,EAES,WAAW;AAAA,EACpB,IAAI,YAAY;AACd,WAAO,KAAK,SAAS;AAAA,EACvB;AAAA,EAES,YAAY;AAAA,EACZ,oBAAoB;AAAA,EAE7B,IAAI,oBAAyC;AAC3C,WAAO,KAAK,SAAS;AAAA,EACvB;AAAA,EAEA,MAAM,QACJ,QACA,aACA,SAGmB;AACnB,UAAM,EAAE,eAAe,IAAI;AAC3B,UAAM,MAAM,KAAK,SAAS,OAAO,IAAI,uBAAuB;AAC5D,UAAM,cAAc,YAAY,KAAK;AAErC,WAAO,yBAAyB;AAAA,MAC9B,OAAO,IAAI;AAAA,MACX,UAAU,IAAI;AAAA,MACd,MAAM,YACJ,cAAc;AAAA,QACZ,KAAK,IAAI,YAAY,eAAe;AAAA,QACpC,SAAS,IAAI,QAAQ;AAAA,UACnB,cAAc,YAAY;AAAA,UAC1B,YAAY,YAAY;AAAA,UACxB,KAAK,YAAY;AAAA,UACjB,QAAQ,YAAY;AAAA,QACtB,CAAC;AAAA,QACD,MAAM;AAAA,UACJ,QAAQ,eAAe;AAAA,UACvB,OAAO,KAAK,SAAS;AAAA,UACrB,QAAQ,OAAO;AAAA,UACf,QAAQ,OAAO;AAAA,UACf,QAAQ,KAAK,SAAS;AAAA,UACtB,SAAS;AAAA,YACP,UAAU,KAAK,SAAS;AAAA,YACxB,cAAc,KAAK,SAAS;AAAA,YAC5B,cAAc,KAAK,SAAS;AAAA,YAC5B,SAAS,KAAK,SAAS;AAAA,YACvB,SAAS,KAAK,SAAS;AAAA,YACvB,SAAS,KAAK,SAAS;AAAA,YACvB,aAAa,KAAK,SAAS;AAAA,YAC3B,aAAa,KAAK,SAAS;AAAA,YAC3B,eAAe,KAAK,SAAS;AAAA,YAC7B,gBAAgB,KAAK,SAAS;AAAA,YAC9B,MAAM,KAAK,SAAS;AAAA,YACpB,MAAM,KAAK,SAAS;AAAA,YACpB,aAAa,KAAK,SAAS;AAAA,YAC3B,OAAO,KAAK,SAAS;AAAA,YACrB,OAAO,KAAK,SAAS;AAAA,YACrB,OAAO,KAAK,SAAS;AAAA,UACvB;AAAA,UACA,QAAQ,KAAK,SAAS;AAAA,UACtB,UAAU,KAAK,SAAS;AAAA,UACxB,SAAS,KAAK,SAAS;AAAA,UACvB,KAAK,KAAK,SAAS;AAAA,QACrB;AAAA,QACA,uBAAuB;AAAA,QACvB,2BAA2B,eAAe;AAAA,QAC1C;AAAA,MACF,CAAC;AAAA,IACL,CAAC;AAAA,EACH;AAAA,EAEA,IAAI,mBAEF;AACA,UAAM,yBAAwC;AAAA,MAC5C,GAAG;AAAA,MAEH;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,IACF;AAEA,WAAO,OAAO;AAAA,MACZ,OAAO,QAAQ,KAAK,QAAQ,EAAE;AAAA,QAAO,CAAC,CAAC,GAAG,MACxC,uBAAuB,SAAS,GAAG;AAAA,MACrC;AAAA,IACF;AAAA,EACF;AAAA,EAEA,MAAM,gBACJ,QACA,SACA;AACA,WAAO,KAAK;AAAA,MACV,MAAM,KAAK,QAAQ,QAAQ,SAAS;AAAA,QAClC,gBAAgB,+BAA+B;AAAA,MACjD,CAAC;AAAA,IACH;AAAA,EACF;AAAA,EAEA,sBAAsB,aAAsB;AAC1C,WAAO,KAAK;AAAA,MACV,cAAc;AAAA,QACZ,OAAO;AAAA,QACP,QAAQ,UAAU,8BAA8B;AAAA,MAClD,CAAC;AAAA,IACH;AAAA,EACF;AAAA,EAEA,8BAA8B,aAAuC;AACnE,WAAO;AAAA,MACL;AAAA,MACA,uBAAuB;AAAA,QACrB;AAAA,UACE,MAAM,YAAY;AAAA,UAClB,cAAc;AAAA,QAChB;AAAA,MACF;AAAA,IACF;AAAA,EACF;AAAA,EAEA,aAAa,QAAgC,SAA8B;AACzE,WAAO,KAAK,QAAQ,QAAQ,SAAS;AAAA,MACnC,GAAG;AAAA,MACH,gBAAgB,+BAA+B;AAAA,IACjD,CAAC;AAAA,EACH;AAAA,EAEA,iBAAiB,OAAgB;AAC/B,UAAM,QAAQ;AACd,WAAO,MAAM,SAAS,OAAO,SAAY,MAAM;AAAA,EACjD;AAAA,EAEA,wBACE,gBAGA;AACA,WAAO,gBAAgB,iBACnB,IAAI,6BAA6B;AAAA,MAC/B,OAAO,eAAe,WAAW,IAAI;AAAA,MACrC,UAAU;AAAA,IACZ,CAAC,IACD,IAAI,6BAA6B;AAAA,MAC/B,OAAO;AAAA,MACP,UAAU;AAAA,IACZ,CAAC;AAAA,EACP;AAAA,EAEA,0BACE,gBACA;AACA,WAAO,IAAI,4BAA4B;AAAA,MACrC,OAAO;AAAA,MACP,UAAU;AAAA,IACZ,CAAC;AAAA,EACH;AAAA,EAEA,iCACE,gBAIA;AACA,WAAO,IAAI,6BAA6B;AAAA,MACtC,OAAO;AAAA,MACP,UAAU;AAAA,IACZ,CAAC;AAAA,EACH;AAAA,EAEA,IAAY,yBAAuF;AACjG,WAAO,KAAK,SAAS,kBAAkBC;AAAA,EACzC;AAAA,EAEA,iBAAiB;AACf,WAAO,KAAK,aAAa,EAAE,QAAQ,OAAO,CAAC;AAAA,EAC7C;AAAA,EAEA,iBAKE;AACA,WAAO,KAAK,mBAAmB,KAAK,uBAAuB,KAAK,CAAC;AAAA,EACnE;AAAA,EAEA,wBAKE;AACA,WAAO,KAAK,mBAAmB,KAAK,uBAAuB,YAAY,CAAC;AAAA,EAC1E;AAAA,EAEA,iBAKE;AACA,WAAO,KAAK,mBAAmB,KAAK,uBAAuB,KAAK,CAAC;AAAA,EACnE;AAAA,EAEA,mBACE,gBASA;AACA,WAAO,IAAI,iCAAiC;AAAA,MAC1C,OAAO,KAAK,aAAa;AAAA,QACvB,eAAe;AAAA,UACb,GAAI,KAAK,SAAS,iBAAiB,CAAC;AAAA,UACpC,GAAG,eAAe;AAAA,QACpB;AAAA,MACF,CAAC;AAAA,MACD;AAAA,IACF,CAAC;AAAA,EACH;AAAA,EAEA,aACE,oBAGA;AACA,WAAO,IAAI;AAAA,MACT,OAAO,OAAO,CAAC,GAAG,KAAK,UAAU,kBAAkB;AAAA,IACrD;AAAA,EACF;AACF;AAEA,IAAM,iCAAiCC,IAAE,OAAO;AAAA,EAC9C,MAAMA,IAAE,QAAQ,IAAI;AAAA,EACpB,OAAOA,IAAE,OAAO;AAAA,EAChB,YAAYA,IAAE,OAAO;AAAA,EACrB,UAAUA,IAAE,OAAO;AAAA,EACnB,gBAAgBA,IAAE,OAAO;AAAA,EACzB,eAAeA,IAAE,OAAO,EAAE,SAAS;AAAA,EACnC,mBAAmBA,IAAE,OAAO,EAAE,SAAS;AAAA,EACvC,sBAAsBA,IAAE,OAAO,EAAE,SAAS;AAAA,EAC1C,YAAYA,IAAE,OAAO;AAAA,EACrB,eAAeA,IAAE,OAAO;AAAA,EACxB,SAASA,IAAE,MAAMA,IAAE,OAAO,CAAC,EAAE,SAAS;AACxC,CAAC;AAMD,IAAM,oCAAoCA,IAAE,mBAAmB,QAAQ;AAAA,EACrEA,IAAE,OAAO;AAAA,IACP,MAAMA,IAAE,QAAQ,KAAK;AAAA,IACrB,OAAOA,IAAE,OAAO;AAAA,IAChB,YAAYA,IAAE,OAAO;AAAA,IACrB,UAAUA,IAAE,OAAO;AAAA,EACrB,CAAC;AAAA,EACDA,IAAE,OAAO;AAAA,IACP,MAAMA,IAAE,QAAQ,IAAI;AAAA,IACpB,OAAOA,IAAE,OAAO;AAAA,IAChB,YAAYA,IAAE,OAAO;AAAA,IACrB,gBAAgBA,IAAE,OAAO;AAAA,IACzB,eAAeA,IAAE,OAAO,EAAE,SAAS;AAAA,IACnC,cAAcA,IAAE,OAAO,EAAE,SAAS;AAAA,IAClC,iBAAiBA,IAAE,OAAO,EAAE,SAAS;AAAA,IACrC,mBAAmBA,IAAE,OAAO,EAAE,SAAS;AAAA,IACvC,sBAAsBA,IAAE,OAAO,EAAE,SAAS;AAAA,IAC1C,YAAYA,IAAE,OAAO;AAAA,IACrB,eAAeA,IAAE,OAAO;AAAA,IACxB,SAASA,IAAE,MAAMA,IAAE,OAAO,CAAC,EAAE,SAAS;AAAA,EACxC,CAAC;AACH,CAAC;AAWM,IAAM,iCAAiC;AAAA;AAAA;AAAA;AAAA,EAI5C,MAAM;AAAA,IACJ,QAAQ;AAAA,IACR,SAAU,OAAO,EAAE,UAAU,KAAK,kBAAkB,MAAM;AACxD,YAAM,eAAe,MAAM,SAAS,KAAK;AAEzC,YAAM,eAAe,cAAc;AAAA,QACjC,MAAM;AAAA,QACN,QAAQ;AAAA,UACNA,IAAE,MAAM;AAAA,YACN;AAAA,YACAA,IAAE,OAAO;AAAA,cACP,MAAMA,IAAE,QAAQ,KAAK;AAAA,cACrB,OAAOA,IAAE,OAAO;AAAA,cAChB,YAAYA,IAAE,OAAO;AAAA,cACrB,UAAUA,IAAE,OAAO;AAAA,YACrB,CAAC;AAAA,UACH,CAAC;AAAA,QACH;AAAA,MACF,CAAC;AAED,UAAI,CAAC,aAAa,SAAS;AACzB,cAAM,IAAI,aAAa;AAAA,UACrB,SAAS;AAAA,UACT,OAAO,aAAa;AAAA,UACpB,YAAY,SAAS;AAAA,UACrB;AAAA,UACA;AAAA,UACA;AAAA,QACF,CAAC;AAAA,MACH;AAEA,UAAI,aAAa,MAAM,SAAS,OAAO;AACrC,cAAM,IAAI,aAAa;AAAA,UACrB,SAAS;AAAA,UACT,YAAY,SAAS;AAAA,UACrB;AAAA,UACA;AAAA,UACA;AAAA,UACA,aAAa;AAAA,QACf,CAAC;AAAA,MACH;AAEA,aAAO,aAAa;AAAA,IACtB;AAAA,EACF;AAAA;AAAA;AAAA;AAAA;AAAA,EAMA,eAAe;AAAA,IACb,QAAQ;AAAA,IACR,SAAS;AAAA,MACP,UAAU,iCAAiC;AAAA,IAC7C;AAAA,EACF;AACF;;;AExcA;AAAA;AAAA,aAAAC;AAAA,EAAA,yBAAAC;AAAA,EAAA,+BAAAC;AAAA,EAAA,oBAAAC;AAAA,EAAA;AAAA;;;ACAA,SAAS,KAAAC,WAAS;AAyBX,IAAM,2BAAN,MAAM,kCACH,cAEV;AAAA,EACE,YAAY,UAA4C;AACtD,UAAM,EAAE,SAAS,CAAC;AAAA,EACpB;AAAA,EAES,WAAW;AAAA,EACpB,IAAI,YAAY;AACd,WAAO;AAAA,EACT;AAAA,EAES,mBAAmB;AAAA,EAC5B,IAAI,mBAAmB;AACrB,WAAO,KAAK,SAAS,oBAAoB;AAAA,EAC3C;AAAA,EAEA,IAAI,aAAa;AACf,WAAO,KAAK,SAAS;AAAA,EACvB;AAAA,EAEA,MAAM,QACJ,OACA,aACsC;AACtC,QAAI,MAAM,SAAS,KAAK,kBAAkB;AACxC,YAAM,IAAI;AAAA,QACR,0CAA0C,KAAK,gBAAgB;AAAA,MACjE;AAAA,IACF;AAEA,UAAM,MAAM,KAAK,SAAS,OAAO,IAAI,uBAAuB;AAC5D,UAAM,cAAc,YAAY,KAAK;AAErC,WAAO,yBAAyB;AAAA,MAC9B,OAAO,IAAI;AAAA,MACX,UAAU,IAAI;AAAA,MACd,MAAM,YACJ,cAAc;AAAA,QACZ,KAAK,IAAI,YAAY,iBAAiB;AAAA,QACtC,SAAS,IAAI,QAAQ;AAAA,UACnB,cAAc,YAAY;AAAA,UAC1B,YAAY,YAAY;AAAA,UACxB,KAAK,YAAY;AAAA,UACjB,QAAQ,YAAY;AAAA,QACtB,CAAC;AAAA,QACD,MAAM;AAAA,UACJ,OAAO,KAAK,SAAS;AAAA,UACrB,QAAQ,MAAM,CAAC;AAAA,QACjB;AAAA,QACA,uBAAuB;AAAA,QACvB,2BAA2B;AAAA,UACzB,UAAU,iCAAiC;AAAA,QAC7C;AAAA,QACA;AAAA,MACF,CAAC;AAAA,IACL,CAAC;AAAA,EACH;AAAA,EAEA,IAAI,mBAA8D;AAChE,WAAO;AAAA,MACL,YAAY,KAAK,SAAS;AAAA,IAC5B;AAAA,EACF;AAAA,EAEA,MAAM,cAAc,OAAiB,SAA8B;AACjE,UAAM,cAAc,MAAM,KAAK,QAAQ,OAAO,OAAO;AAErD,WAAO;AAAA,MACL;AAAA,MACA,YAAY,CAAC,YAAY,SAAS;AAAA,IACpC;AAAA,EACF;AAAA,EAEA,aAAa,oBAA+D;AAC1E,WAAO,IAAI;AAAA,MACT,OAAO,OAAO,CAAC,GAAG,KAAK,UAAU,kBAAkB;AAAA,IACrD;AAAA,EACF;AACF;AAEA,IAAM,oCAAoCC,IAAE,OAAO;AAAA,EACjD,WAAWA,IAAE,MAAMA,IAAE,OAAO,CAAC;AAC/B,CAAC;;;AD7FM,SAASC,KAAI,UAAsD;AACxE,SAAO,IAAI,uBAAuB,QAAQ;AAC5C;AAEO,SAASC,yBACd,UACA;AACA,SAAO,IAAI,sBAAsB,QAAQ;AAC3C;AAEO,SAASC,mBAAkB,UAAmC;AACnE,SAAO,IAAI,gBAAgB,QAAQ;AACrC;AAEO,SAASC,cAAa,UAA4C;AACvE,SAAO,IAAI,yBAAyB,QAAQ;AAC9C;;;AEhCA,SAAS,KAAAC,WAAS;;;ACUX,IAAM,yBAAN,cAAqC,oCAAoC;AAAA,EAC9E,YACE,WAEI,CAAC,GACL;AACA,UAAM;AAAA,MACJ,GAAG;AAAA,MACH,SAAS;AAAA,QACP,eAAe,UAAU,WAAW;AAAA,UAClC,QAAQ,SAAS;AAAA,UACjB,yBAAyB;AAAA,UACzB,aAAa;AAAA,QACf,CAAC,CAAC;AAAA,MACJ;AAAA,MACA,iBAAiB;AAAA,QACf,UAAU;AAAA,QACV,MAAM;AAAA,QACN,MAAM;AAAA,QACN,MAAM;AAAA,MACR;AAAA,IACF,CAAC;AAAA,EACH;AACF;;;ACjCA,SAAS,KAAAC,WAAS;AAIlB,IAAM,wBAAwBC,IAAE,OAAO;AAAA,EACrC,OAAOA,IAAE,OAAO;AAAA,IACd,SAASA,IAAE,OAAO;AAAA,IAClB,MAAMA,IAAE,OAAO;AAAA,IACf,OAAOA,IAAE,IAAI,EAAE,SAAS;AAAA,IACxB,MAAMA,IAAE,OAAO,EAAE,SAAS;AAAA,EAC5B,CAAC;AACH,CAAC;AAIM,IAAM,kCAAkC,+BAA+B;AAAA,EAC5E,aAAa,UAAU,qBAAqB;AAAA,EAC5C,gBAAgB,CAAC,SAAS,KAAK,MAAM;AAAA,EACrC,aAAa,CAAC,UAAU,UACtB,SAAS,UAAU,OAClB,SAAS,WAAW;AAAA,EAEnB,OAAO,MAAM,SAAS;AAC5B,CAAC;;;AFiFM,IAAe,0BAAf,cAEG,cAAwB;AAAA,EAChC,YAAY,UAAoB;AAC9B,UAAM,EAAE,SAAS,CAAC;AAAA,EACpB;AAAA,EAEA,MAAM,QACJ,UACA,aACA,SAOiB;AACjB,UAAM,MAAM,KAAK,SAAS,OAAO,IAAI,uBAAuB;AAC5D,UAAM,iBAAiB,QAAQ;AAC/B,UAAM,cAAc,YAAY,KAAK;AACrC,UAAM,OAAO,KAAK,SAAS,4BACvB,YAAY,KAAK,SACjB;AACJ,UAAM,uBAAuB,KAAK,SAAS;AAG3C,UAAM,YAAY,QAAQ,aAAa,KAAK,SAAS;AACrD,UAAM,eAAe,QAAQ,gBAAgB,KAAK,SAAS;AAC3D,UAAM,QAAQ,QAAQ,SAAS,KAAK,SAAS;AAC7C,UAAM,aAAa,QAAQ,cAAc,KAAK,SAAS;AAEvD,QAAI,EAAE,cAAc,IAAI,KAAK;AAE7B,WAAO,yBAAyB;AAAA,MAC9B,OAAO,KAAK,SAAS,KAAK;AAAA,MAC1B,UAAU,KAAK,SAAS,KAAK;AAAA,MAC7B,MAAM,YAAY;AAEhB,YACE,iBAAiB,QACjB,MAAM,QAAQ,aAAa,KAC3B,cAAc,WAAW,GACzB;AACA,0BAAgB;AAAA,QAClB;AAEA,eAAO,cAAc;AAAA,UACnB,KAAK,IAAI,YAAY,mBAAmB;AAAA,UACxC,SAAS,IAAI,QAAQ;AAAA,YACnB,cAAc,YAAY;AAAA,YAC1B,YAAY,YAAY;AAAA,YACxB,KAAK,YAAY;AAAA,YACjB,QAAQ,YAAY;AAAA,UACtB,CAAC;AAAA,UACD,MAAM;AAAA,YACJ,QAAQ,eAAe;AAAA,YACvB,OAAO,KAAK,SAAS;AAAA,YACrB;AAAA,YACA;AAAA,YACA,eAAe;AAAA,YACf;AAAA,YACA,aAAa;AAAA,YACb,aAAa,KAAK,SAAS;AAAA,YAC3B,OAAO,KAAK,SAAS;AAAA,YACrB,GAAG,KAAK,SAAS;AAAA,YACjB,MAAM;AAAA,YACN,YAAY,KAAK,SAAS;AAAA,YAC1B,kBAAkB,KAAK,SAAS;AAAA,YAChC,mBAAmB,KAAK,SAAS;AAAA,YACjC,YAAY,KAAK,SAAS;AAAA,YAC1B,MAAM,KAAK,SAAS;AAAA,YACpB,iBAAiB;AAAA,YACjB;AAAA,UACF;AAAA,UACA,uBAAuB;AAAA,UACvB,2BAA2B,eAAe;AAAA,UAC1C;AAAA,QACF,CAAC;AAAA,MACH;AAAA,IACF,CAAC;AAAA,EACH;AAAA,EAEA,MAAM,gBACJ,QACA,SACA;AACA,WAAO,KAAK;AAAA,MACV,MAAM,KAAK,QAAQ,QAAQ,SAAS;AAAA,QAClC,gBAAgB,yBAAyB;AAAA,MAC3C,CAAC;AAAA,IACH;AAAA,EACF;AAAA,EAEA,sBAAsB,aAAsB;AAC1C,WAAO,KAAK;AAAA,MACV,cAAc;AAAA,QACZ,OAAO;AAAA,QACP,QAAQ,UAAU,wBAAwB;AAAA,MAC5C,CAAC;AAAA,IACH;AAAA,EACF;AAAA,EAEA,8BAA8B,aAAiC;AAC7D,WAAO;AAAA,MACL;AAAA,MACA,uBAAuB,YAAY,QAAQ,IAAI,CAAC,YAAY;AAAA,QAC1D,MAAM,OAAO,QAAQ,WAAW;AAAA,QAChC,cAAc,KAAK,sBAAsB,OAAO,aAAa;AAAA,MAC/D,EAAE;AAAA,MACF,OAAO,KAAK,aAAa,WAAW;AAAA,IACtC;AAAA,EACF;AAAA,EAEQ,sBACN,cAC4B;AAC5B,YAAQ,cAAc;AAAA,MACpB,KAAK;AACH,eAAO;AAAA,MACT,KAAK;AACH,eAAO;AAAA,MACT,KAAK;AACH,eAAO;AAAA,MACT,KAAK;AAAA,MACL,KAAK;AACH,eAAO;AAAA,MACT;AACE,eAAO;AAAA,IACX;AAAA,EACF;AAAA,EAEA,aAAa,QAA0B,SAA8B;AACnE,WAAO,KAAK,QAAQ,QAAQ,SAAS;AAAA,MACnC,gBAAgB,yBAAyB;AAAA,IAC3C,CAAC;AAAA,EACH;AAAA,EAEA,iBAAiB,OAAgB;AAC/B,UAAM,QAAQ;AAEd,QACE,MAAM,WAAW,2BACjB,MAAM,WAAW,mBACjB;AACA,aAAO;AAAA,IACT;AAEA,UAAM,YAAY;AAElB,UAAM,cAAc,UAAU,QAAQ,CAAC;AAEvC,QAAI,YAAY,QAAQ,GAAG;AACzB,aAAO;AAAA,IACT;AAEA,WAAO,YAAY,MAAM,WAAW;AAAA,EACtC;AAAA,EAEA,MAAM,mBACJ,MACA,QACA,SACA;AACA,UAAM,cAAc,MAAM,KAAK,QAAQ,QAAQ,SAAS;AAAA,MACtD,gBAAgB,yBAAyB;AAAA,MACzC,YAAY;AAAA,QACV,MAAM;AAAA,QACN,UAAU,EAAE,MAAM,KAAK,KAAK;AAAA,MAC9B;AAAA,MACA,OAAO;AAAA,QACL;AAAA,UACE,MAAM;AAAA,UACN,UAAU;AAAA,YACR,MAAM,KAAK;AAAA,YACX,aAAa,KAAK;AAAA,YAClB,YAAY,KAAK,WAAW,cAAc;AAAA,UAC5C;AAAA,QACF;AAAA,MACF;AAAA,IACF,CAAC;AAED,UAAM,YAAY,YAAY,QAAQ,CAAC,GAAG,QAAQ;AAElD,WAAO;AAAA,MACL;AAAA,MACA,UACE,aAAa,QAAQ,UAAU,WAAW,IACtC,OACA;AAAA,QACE,IAAI,UAAU,CAAC,EAAE;AAAA,QACjB,MAAM,UAAU,EAAE,MAAM,UAAU,CAAC,EAAE,SAAS,UAAU,CAAC;AAAA,MAC3D;AAAA,MACN,OAAO,KAAK,aAAa,WAAW;AAAA,IACtC;AAAA,EACF;AAAA,EAEA,MAAM,oBACJ,OACA,QACA,SACA;AACA,UAAM,cAAc,MAAM,KAAK,QAAQ,QAAQ,SAAS;AAAA,MACtD,gBAAgB,yBAAyB;AAAA,MACzC,YAAY;AAAA,MACZ,OAAO,MAAM,IAAI,CAAC,UAAU;AAAA,QAC1B,MAAM;AAAA,QACN,UAAU;AAAA,UACR,MAAM,KAAK;AAAA,UACX,aAAa,KAAK;AAAA,UAClB,YAAY,KAAK,WAAW,cAAc;AAAA,QAC5C;AAAA,MACF,EAAE;AAAA,IACJ,CAAC;AAED,UAAM,UAAU,YAAY,QAAQ,CAAC,GAAG;AAExC,WAAO;AAAA,MACL;AAAA,MACA,MAAM,QAAQ,WAAW;AAAA,MACzB,WACE,QAAQ,YAAY,IAAI,CAAC,cAAc;AAAA,QACrC,IAAI,SAAS;AAAA,QACb,MAAM,SAAS,SAAS;AAAA,QACxB,MAAM,UAAU,EAAE,MAAM,SAAS,SAAS,UAAU,CAAC;AAAA,MACvD,EAAE,KAAK;AAAA,MACT,OAAO,KAAK,aAAa,WAAW;AAAA,IACtC;AAAA,EACF;AAAA,EAEA,aAAa,UAA8B;AACzC,WAAO;AAAA,MACL,cAAc,SAAS,MAAM;AAAA,MAC7B,kBAAkB,SAAS,MAAM;AAAA,MACjC,aAAa,SAAS,MAAM;AAAA,IAC9B;AAAA,EACF;AACF;AAEA,IAAM,2BAA2BC,IAAE,OAAO;AAAA,EACxC,IAAIA,IAAE,OAAO;AAAA,EACb,SAASA,IAAE;AAAA,IACTA,IAAE,OAAO;AAAA,MACP,SAASA,IAAE,OAAO;AAAA,QAChB,MAAMA,IAAE,QAAQ,WAAW;AAAA,QAC3B,SAASA,IAAE,OAAO,EAAE,SAAS;AAAA,QAC7B,eAAeA,IACZ,OAAO;AAAA,UACN,MAAMA,IAAE,OAAO;AAAA,UACf,WAAWA,IAAE,OAAO;AAAA,QACtB,CAAC,EACA,SAAS;AAAA,QACZ,YAAYA,IACT;AAAA,UACCA,IAAE,OAAO;AAAA,YACP,IAAIA,IAAE,OAAO;AAAA,YACb,MAAMA,IAAE,QAAQ,UAAU;AAAA,YAC1B,UAAUA,IAAE,OAAO;AAAA,cACjB,MAAMA,IAAE,OAAO;AAAA,cACf,WAAWA,IAAE,OAAO;AAAA,YACtB,CAAC;AAAA,UACH,CAAC;AAAA,QACH,EACC,SAAS;AAAA,MACd,CAAC;AAAA,MACD,OAAOA,IAAE,OAAO,EAAE,SAAS;AAAA;AAAA,MAC3B,UAAUA,IAAE,SAASA,IAAE,IAAI,CAAC;AAAA,MAC5B,eAAeA,IACZ,KAAK;AAAA,QACJ;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,MACF,CAAC,EACA,SAAS,EACT,SAAS;AAAA,IACd,CAAC;AAAA,EACH;AAAA,EACA,SAASA,IAAE,OAAO;AAAA,EAClB,OAAOA,IAAE,OAAO;AAAA,EAChB,oBAAoBA,IAAE,OAAO,EAAE,SAAS,EAAE,SAAS;AAAA,EACnD,QAAQA,IAAE,QAAQ,iBAAiB;AAAA,EACnC,OAAOA,IAAE,OAAO;AAAA,IACd,eAAeA,IAAE,OAAO;AAAA,IACxB,mBAAmBA,IAAE,OAAO;AAAA,IAC5B,cAAcA,IAAE,OAAO;AAAA,EACzB,CAAC;AACH,CAAC;AAID,IAAM,wBAAwBA,IAAE,OAAO;AAAA,EACrC,QAAQA,IAAE,OAAO;AAAA;AAAA,EACjB,IAAIA,IAAE,OAAO;AAAA,EACb,SAASA,IAAE;AAAA,IACTA,IAAE,OAAO;AAAA,MACP,OAAOA,IAAE,OAAO;AAAA,QACd,MAAMA,IAAE,KAAK,CAAC,aAAa,MAAM,CAAC,EAAE,SAAS;AAAA,QAC7C,SAASA,IAAE,OAAO,EAAE,SAAS,EAAE,SAAS;AAAA,QACxC,eAAeA,IACZ,OAAO;AAAA,UACN,MAAMA,IAAE,OAAO,EAAE,SAAS;AAAA,UAC1B,WAAWA,IAAE,OAAO,EAAE,SAAS;AAAA,QACjC,CAAC,EACA,SAAS;AAAA,QACZ,YAAYA,IACT;AAAA,UACCA,IAAE,OAAO;AAAA,YACP,IAAIA,IAAE,OAAO;AAAA,YACb,MAAMA,IAAE,QAAQ,UAAU;AAAA,YAC1B,UAAUA,IAAE,OAAO;AAAA,cACjB,MAAMA,IAAE,OAAO;AAAA,cACf,WAAWA,IAAE,OAAO;AAAA,YACtB,CAAC;AAAA,UACH,CAAC;AAAA,QACH,EACC,SAAS;AAAA,MACd,CAAC;AAAA,MACD,eAAeA,IACZ,KAAK;AAAA,QACJ;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,MACF,CAAC,EACA,SAAS,EACT,SAAS;AAAA,MACZ,OAAOA,IAAE,OAAO;AAAA,IAClB,CAAC;AAAA,EACH;AAAA,EACA,SAASA,IAAE,OAAO;AAAA,EAClB,OAAOA,IAAE,OAAO,EAAE,SAAS;AAAA;AAAA,EAC3B,oBAAoBA,IAAE,OAAO,EAAE,SAAS,EAAE,SAAS;AACrD,CAAC;AASM,IAAM,2BAA2B;AAAA;AAAA;AAAA;AAAA,EAItC,MAAM;AAAA,IACJ,QAAQ;AAAA,IACR,SAAS,0BAA0B,UAAU,wBAAwB,CAAC;AAAA,EACxE;AAAA;AAAA;AAAA;AAAA,EAKA,eAAe;AAAA,IACb,QAAQ;AAAA,IACR,SAAS,iCAAiC,UAAU,qBAAqB,CAAC;AAAA,EAC5E;AACF;;;AGhdA,SAAS,KAAAC,WAAS;AA2CX,IAAe,gCAAf,cAEG,cAAwB;AAAA,EAChC,YAAY,UAAoB;AAC9B,UAAM,EAAE,SAAS,CAAC;AAAA,EACpB;AAAA,EAEA,MAAM,QACJ,QACA,aACA,SAGiB;AACjB,UAAM,MAAM,KAAK,SAAS,OAAO,IAAI,uBAAuB;AAC5D,UAAM,OAAO,KAAK,SAAS,4BACvB,YAAY,KAAK,SACjB;AACJ,UAAM,cAAc,YAAY,KAAK;AACrC,UAAM,uBAAuB,QAAQ;AAGrC,UAAM,gBACJ,KAAK,SAAS,iBAAiB,QAC/B,MAAM,QAAQ,KAAK,SAAS,aAAa,KACzC,KAAK,SAAS,cAAc,WAAW,IACnC,SACA,KAAK,SAAS;AAEpB,WAAO,yBAAyB;AAAA,MAC9B,OAAO,IAAI;AAAA,MACX,UAAU,IAAI;AAAA,MACd,MAAM,YACJ,cAAc;AAAA,QACZ,KAAK,IAAI,YAAY,cAAc;AAAA,QACnC,SAAS,IAAI,QAAQ;AAAA,UACnB,cAAc,YAAY;AAAA,UAC1B,YAAY,YAAY;AAAA,UACxB,KAAK,YAAY;AAAA,UACjB,QAAQ,YAAY;AAAA,QACtB,CAAC;AAAA,QACD,MAAM;AAAA,UACJ,QAAQ,qBAAqB;AAAA,UAC7B,OAAO,KAAK,SAAS;AAAA,UACrB;AAAA,UACA,QAAQ,KAAK,SAAS;AAAA,UACtB,YAAY,KAAK,SAAS;AAAA,UAC1B,aAAa,KAAK,SAAS;AAAA,UAC3B,OAAO,KAAK,SAAS;AAAA,UACrB,GAAG,KAAK,SAAS;AAAA,UACjB,UAAU,KAAK,SAAS;AAAA,UACxB,MAAM,KAAK,SAAS;AAAA,UACpB,MAAM;AAAA,UACN,MAAM,KAAK,SAAS;AAAA,UACpB,kBAAkB,KAAK,SAAS;AAAA,UAChC,mBAAmB,KAAK,SAAS;AAAA,UACjC,SAAS,KAAK,SAAS;AAAA,UACvB,YAAY,KAAK,SAAS;AAAA,UAC1B;AAAA,QACF;AAAA,QACA,uBAAuB;AAAA,QACvB,2BAA2B,qBAAqB;AAAA,QAChD;AAAA,MACF,CAAC;AAAA,IACL,CAAC;AAAA,EACH;AAAA,EAEA,MAAM,gBAAgB,QAAgB,SAA8B;AAClE,WAAO,KAAK;AAAA,MACV,MAAM,KAAK,QAAQ,QAAQ,SAAS;AAAA,QAClC,gBAAgB,yBAAyB;AAAA,MAC3C,CAAC;AAAA,IACH;AAAA,EACF;AAAA,EAEA,sBAAsB,aAAsB;AAC1C,WAAO,KAAK;AAAA,MACV,cAAc;AAAA,QACZ,OAAO;AAAA,QACP,QAAQ,UAAU,8BAA8B;AAAA,MAClD,CAAC;AAAA,IACH;AAAA,EACF;AAAA,EAEQ,8BAA8B,aAAuC;AAC3E,WAAO;AAAA,MACL;AAAA,MACA,uBAAuB,YAAY,QAAQ,IAAI,CAAC,WAAW;AACzD,eAAO;AAAA,UACL,cAAc,KAAK,sBAAsB,OAAO,aAAa;AAAA,UAC7D,MAAM,OAAO;AAAA,QACf;AAAA,MACF,CAAC;AAAA,MACD,OAAO;AAAA,QACL,cAAc,YAAY,MAAM;AAAA,QAChC,kBAAkB,YAAY,MAAM;AAAA,QACpC,aAAa,YAAY,MAAM;AAAA,MACjC;AAAA,IACF;AAAA,EACF;AAAA,EAEQ,sBACN,cAC4B;AAC5B,YAAQ,cAAc;AAAA,MACpB,KAAK;AACH,eAAO;AAAA,MACT,KAAK;AACH,eAAO;AAAA,MACT,KAAK;AACH,eAAO;AAAA,MACT;AACE,eAAO;AAAA,IACX;AAAA,EACF;AAAA,EAEA,aAAa,QAAgB,SAA8B;AACzD,WAAO,KAAK,QAAQ,QAAQ,SAAS;AAAA,MACnC,gBAAgB,yBAAyB;AAAA,IAC3C,CAAC;AAAA,EACH;AAAA,EAEA,iBAAiB,OAAgB;AAC/B,UAAM,QAAQ;AAEd,UAAM,cAAc,MAAM,QAAQ,CAAC;AAEnC,QAAI,YAAY,QAAQ,GAAG;AACzB,aAAO;AAAA,IACT;AAEA,WAAO,MAAM,QAAQ,CAAC,EAAE;AAAA,EAC1B;AAAA,EAEA,iBAAuB;AACrB,WAAO;AAAA,EACT;AACF;AAEA,IAAM,iCAAiCC,IAAE,OAAO;AAAA,EAC9C,IAAIA,IAAE,OAAO;AAAA,EACb,SAASA,IAAE;AAAA,IACTA,IAAE,OAAO;AAAA,MACP,eAAeA,IACZ,KAAK,CAAC,QAAQ,UAAU,gBAAgB,CAAC,EACzC,SAAS,EACT,SAAS;AAAA,MACZ,OAAOA,IAAE,OAAO;AAAA,MAChB,UAAUA,IAAE,SAASA,IAAE,IAAI,CAAC;AAAA,MAC5B,MAAMA,IAAE,OAAO;AAAA,IACjB,CAAC;AAAA,EACH;AAAA,EACA,SAASA,IAAE,OAAO;AAAA,EAClB,OAAOA,IAAE,OAAO;AAAA,EAChB,oBAAoBA,IAAE,OAAO,EAAE,SAAS;AAAA,EACxC,QAAQA,IAAE,QAAQ,iBAAiB;AAAA,EACnC,OAAOA,IAAE,OAAO;AAAA,IACd,eAAeA,IAAE,OAAO;AAAA,IACxB,mBAAmBA,IAAE,OAAO;AAAA,IAC5B,cAAcA,IAAE,OAAO;AAAA,EACzB,CAAC;AACH,CAAC;AAMD,IAAM,oCAAoCA,IAAE,OAAO;AAAA,EACjD,SAASA,IAAE;AAAA,IACTA,IAAE,OAAO;AAAA,MACP,MAAMA,IAAE,OAAO;AAAA,MACf,eAAeA,IACZ,KAAK,CAAC,QAAQ,UAAU,gBAAgB,CAAC,EACzC,SAAS,EACT,SAAS;AAAA,MACZ,OAAOA,IAAE,OAAO;AAAA,IAClB,CAAC;AAAA,EACH;AAAA,EACA,SAASA,IAAE,OAAO;AAAA,EAClB,IAAIA,IAAE,OAAO;AAAA,EACb,OAAOA,IAAE,OAAO;AAAA,EAChB,oBAAoBA,IAAE,OAAO,EAAE,SAAS;AAAA,EACxC,QAAQA,IAAE,QAAQ,iBAAiB;AACrC,CAAC;AAWM,IAAM,2BAA2B;AAAA;AAAA;AAAA;AAAA,EAItC,MAAM;AAAA,IACJ,QAAQ;AAAA,IACR,SAAS;AAAA,MACP,UAAU,8BAA8B;AAAA,IAC1C;AAAA,EACF;AAAA;AAAA;AAAA;AAAA;AAAA,EAMA,eAAe;AAAA,IACb,QAAQ;AAAA,IACR,SAAS;AAAA,MACP,UAAU,iCAAiC;AAAA,IAC7C;AAAA,EACF;AACF;;;AClQA,SAAS,KAAAC,WAAS;AA8BX,IAAe,mCAAf,cAEG,cAAwB;AAAA,EAChC,YAAY,UAAoB;AAC9B,UAAM,EAAE,SAAS,CAAC;AAAA,EACpB;AAAA,EAEA,IAAI,mBAAmB;AACrB,WAAO,KAAK,SAAS,oBAAoB;AAAA,EAC3C;AAAA,EAES,mBAAmB;AAAA,EAE5B,MAAM,QACJ,OACA,aACsC;AACtC,UAAM,MAAM,KAAK,SAAS,OAAO,IAAI,uBAAuB;AAC5D,UAAM,cAAc,YAAY,KAAK;AAErC,WAAO,yBAAyB;AAAA,MAC9B,OAAO,IAAI;AAAA,MACX,UAAU,IAAI;AAAA,MACd,MAAM,YACJ,cAAc;AAAA,QACZ,KAAK,IAAI,YAAY,aAAa;AAAA,QAClC,SAAS,IAAI,QAAQ;AAAA,UACnB,cAAc,YAAY;AAAA,UAC1B,YAAY,YAAY;AAAA,UACxB,KAAK,YAAY;AAAA,UACjB,QAAQ,YAAY;AAAA,QACtB,CAAC;AAAA,QACD,MAAM;AAAA,UACJ,OAAO,KAAK;AAAA,UACZ,OAAO;AAAA,UACP,YAAY,KAAK,SAAS;AAAA,UAC1B,MAAM,KAAK,SAAS,4BAChB,YAAY,KAAK,SACjB;AAAA,QACN;AAAA,QACA,uBAAuB;AAAA,QACvB,2BAA2B;AAAA,UACzB,UAAU,iCAAiC;AAAA,QAC7C;AAAA,QACA;AAAA,MACF,CAAC;AAAA,IACL,CAAC;AAAA,EACH;AAAA,EAEA,MAAM,cAAc,OAAiB,aAAkC;AACrE,QAAI,MAAM,SAAS,KAAK,kBAAkB;AACxC,YAAM,IAAI;AAAA,QACR,0CAA0C,KAAK,gBAAgB;AAAA,MACjE;AAAA,IACF;AAEA,UAAM,cAAc,MAAM,KAAK,QAAQ,OAAO,WAAW;AAEzD,WAAO;AAAA,MACL;AAAA,MACA,YAAY,YAAY,KAAK,IAAI,CAAC,SAAS,KAAK,SAAS;AAAA,IAC3D;AAAA,EACF;AACF;AAEA,IAAM,oCAAoCC,IAAE,OAAO;AAAA,EACjD,QAAQA,IAAE,QAAQ,MAAM;AAAA,EACxB,MAAMA,IAAE;AAAA,IACNA,IAAE,OAAO;AAAA,MACP,QAAQA,IAAE,QAAQ,WAAW;AAAA,MAC7B,WAAWA,IAAE,MAAMA,IAAE,OAAO,CAAC;AAAA,MAC7B,OAAOA,IAAE,OAAO;AAAA,IAClB,CAAC;AAAA,EACH;AAAA,EACA,OAAOA,IAAE,OAAO;AAAA,EAChB,OAAOA,IACJ,OAAO;AAAA,IACN,eAAeA,IAAE,OAAO;AAAA,IACxB,cAAcA,IAAE,OAAO;AAAA,EACzB,CAAC,EACA,SAAS;AAAA;AACd,CAAC;;;AC1FM,IAAM,8BAAN,cAA0C,yBAAyB;AAAA,EAC/D;AAAA,EACA;AAAA,EACA;AAAA,EAEA;AAAA,EAET,YAAY;AAAA,IACV;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,EACF,GAAuC;AACrC,UAAM,EAAE,OAAO,SAAS,CAAC;AAEzB,SAAK,eAAe;AACpB,SAAK,eAAe;AACpB,SAAK,aAAa;AAElB,SAAK,mBAAmB;AAAA,MACtB,WAAW,WAAW;AAAA,QACpB;AAAA,QACA,yBAAyB;AAAA,QACzB,aAAa;AAAA,MACf,CAAC;AAAA,IACH;AAAA,EACF;AAAA,EAEA,YAAY,MAAsB;AAChC,WAAO,WAAW,KAAK,YAAY,wCAAwC,KAAK,YAAY,GAAG,IAAI,gBAAgB,KAAK,UAAU;AAAA,EACpI;AAAA,EAEA,eAAe;AACb,WAAO,KAAK;AAAA,EACd;AACF;;;ACCO,IAAM,oBAAoB;AAAA;AAAA;AAAA;AAAA,EAI/B,OAAO,SAAoC;AACzC,WAAO,EAAE,MAAM,UAAU,QAAQ;AAAA,EACnC;AAAA;AAAA;AAAA;AAAA,EAKA,KACE,SACA,SACmB;AACnB,WAAO;AAAA,MACL,MAAM;AAAA,MACN,SACE,OAAO,YAAY,WACf,UACA,QAAQ,IAAI,CAAC,SAAS;AACpB,gBAAQ,KAAK,MAAM;AAAA,UACjB,KAAK,QAAQ;AACX,mBAAO,EAAE,MAAM,QAAQ,MAAM,KAAK,KAAK;AAAA,UACzC;AAAA,UACA,KAAK,SAAS;AACZ,mBAAO;AAAA,cACL,MAAM;AAAA,cACN,WAAW,QACT,KAAK,YAAY,YACnB,WAAW,iCAAiC,KAAK,KAAK,CAAC;AAAA,YACzD;AAAA,UACF;AAAA,QACF;AAAA,MACF,CAAC;AAAA,MACP,MAAM,SAAS;AAAA,IACjB;AAAA,EACF;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAOA,UACE,SACA,SAImB;AACnB,WAAO;AAAA,MACL,MAAM;AAAA,MACN;AAAA,MACA,eACE,SAAS,gBAAgB,OACrB,SACA;AAAA,QACE,MAAM,QAAQ,aAAa;AAAA,QAC3B,WAAW,QAAQ,aAAa;AAAA,MAClC;AAAA,MACN,YACE,SAAS,WAAW,IAAI,CAAC,cAAc;AAAA,QACrC,IAAI,SAAS;AAAA,QACb,MAAM;AAAA,QACN,UAAU;AAAA,UACR,MAAM,SAAS;AAAA,UACf,WAAW,KAAK,UAAU,SAAS,IAAI;AAAA,QACzC;AAAA,MACF,EAAE,KAAK;AAAA,IACX;AAAA,EACF;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAOA,GAAG;AAAA,IACD;AAAA,IACA;AAAA,EACF,GAGsB;AACpB,WAAO,EAAE,MAAM,YAAY,MAAM,QAAQ,SAAS,KAAK,UAAU,OAAO,EAAE;AAAA,EAC5E;AAAA;AAAA;AAAA;AAAA,EAKA,KAAK;AAAA,IACH;AAAA,IACA;AAAA,EACF,GAGsB;AACpB,WAAO;AAAA,MACL,MAAM;AAAA,MACN,cAAc;AAAA,MACd,SAAS,KAAK,UAAU,OAAO;AAAA,IACjC;AAAA,EACF;AACF;;;ACnKA,OAAOC,iBAAgB;;;ACShB,SAAS,WAGd;AACA,SAAO,EAAE,QAAQ,CAAC,WAAW,QAAQ,eAAe,CAAC,EAAE;AACzD;AAKO,SAASC,SAA+D;AAC7E,SAAO;AAAA,IACL,QAAQ,CAAC,WAAW,CAAC,kBAAkB,KAAK,MAAM,CAAC;AAAA,IACnD,eAAe,CAAC;AAAA,EAClB;AACF;AAKO,SAASC,gBAGd;AACA,SAAO;AAAA,IACL,OAAO,QAAQ;AACb,YAAM,WAA6B,CAAC;AAEpC,UAAI,OAAO,UAAU,MAAM;AACzB,iBAAS,KAAK,kBAAkB,OAAO,OAAO,MAAM,CAAC;AAAA,MACvD;AAEA,eAAS,KAAK,kBAAkB,KAAK,OAAO,WAAW,CAAC;AAExD,aAAO;AAAA,IACT;AAAA,IACA,eAAe,CAAC;AAAA,EAClB;AACF;AAKO,SAASC,SAGd;AACA,SAAO;AAAA,IACL,OAAO,QAAQ;AACb,YAAM,WAAqC,CAAC;AAE5C,UAAI,OAAO,UAAU,MAAM;AACzB,iBAAS,KAAK,kBAAkB,OAAO,OAAO,MAAM,CAAC;AAAA,MACvD;AAEA,iBAAW,EAAE,MAAM,QAAQ,KAAK,OAAO,UAAU;AAC/C,gBAAQ,MAAM;AAAA,UACZ,KAAK,QAAQ;AACX,qBAAS,KAAK,kBAAkB,KAAK,OAAO,CAAC;AAC7C;AAAA,UACF;AAAA,UACA,KAAK,aAAa;AAChB,gBAAI,OAAO,YAAY,UAAU;AAC/B,uBAAS,KAAK,kBAAkB,UAAU,OAAO,CAAC;AAAA,YACpD,OAAO;AACL,kBAAIF,SAAO;AACX,oBAAM,YAID,CAAC;AAEN,yBAAW,QAAQ,SAAS;AAC1B,wBAAQ,KAAK,MAAM;AAAA,kBACjB,KAAK,QAAQ;AACX,oBAAAA,UAAQ,KAAK;AACb;AAAA,kBACF;AAAA,kBACA,KAAK,aAAa;AAChB,8BAAU,KAAK;AAAA,sBACb,IAAI,KAAK;AAAA,sBACT,MAAM;AAAA,sBACN,UAAU;AAAA,wBACR,MAAM,KAAK;AAAA,wBACX,WAAW,KAAK,UAAU,KAAK,IAAI;AAAA,sBACrC;AAAA,oBACF,CAAC;AACD;AAAA,kBACF;AAAA,kBACA,SAAS;AACP,0BAAM,mBAA0B;AAChC,0BAAM,IAAI,MAAM,qBAAqB,gBAAgB,EAAE;AAAA,kBACzD;AAAA,gBACF;AAAA,cACF;AAEA,uBAAS,KAAK;AAAA,gBACZ,MAAM;AAAA,gBACN,SAASA;AAAA,gBACT,YAAY;AAAA,cACd,CAAC;AAAA,YACH;AAEA;AAAA,UACF;AAAA,UACA,KAAK,QAAQ;AACX,uBAAW,gBAAgB,SAAS;AAClC,uBAAS,KAAK;AAAA,gBACZ,MAAM;AAAA,gBACN,cAAc,aAAa;AAAA,gBAC3B,SAAS,KAAK,UAAU,aAAa,QAAQ;AAAA,cAC/C,CAAC;AAAA,YACH;AACA;AAAA,UACF;AAAA,UACA,SAAS;AACP,kBAAM,mBAA0B;AAChC,kBAAM,IAAI,MAAM,qBAAqB,gBAAgB,EAAE;AAAA,UACzD;AAAA,QACF;AAAA,MACF;AAEA,aAAO;AAAA,IACT;AAAA,IACA,eAAe,CAAC;AAAA,EAClB;AACF;;;ADtHO,IAAM,8CAAN,MAAM,6CAUb;AAAA,EACW;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EAET,YAAY;AAAA,IACV;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,EACF,GAKG;AACD,SAAK,QAAQ;AACb,SAAK,SAAS;AACd,SAAK,gBAAgB;AACrB,SAAK,iBAAiB;AAAA,EACxB;AAAA,EAEA,IAAI,mBAAmB;AACrB,WAAO,KAAK,MAAM;AAAA,EACpB;AAAA,EAEA,IAAI,WAAW;AACb,WAAO;AAAA,MACL,GAAG,KAAK,MAAM;AAAA,MACd,QAAQ,KAAK;AAAA,MACb,eAAe,KAAK;AAAA,IACtB;AAAA,EACF;AAAA,EAEA,IAAI,mBAAmB;AACrB,WAAO;AAAA,MACL,GAAG,KAAK,MAAM;AAAA,MACd,QAAQ,KAAK;AAAA,MACb,eAAe,KAAK;AAAA,IACtB;AAAA,EACF;AAAA;AAAA;AAAA;AAAA,EAKA,iBAAiB;AACf,WAAO,KAAK,mBAAmBG,OAAK,CAAC;AAAA,EACvC;AAAA;AAAA;AAAA;AAAA,EAKA,wBAAwB;AACtB,WAAO,KAAK,mBAAmBC,cAAY,CAAC;AAAA,EAC9C;AAAA;AAAA;AAAA;AAAA,EAKA,iBAAiB;AACf,WAAO,KAAK,mBAAmBC,OAAK,CAAC;AAAA,EACvC;AAAA,EAEA,mBAME,gBACmE;AACnE,WAAO,IAAI,6CAA4C;AAAA,MACrD,OAAO,KAAK;AAAA,MACZ,QAAQ,KAAK;AAAA,MACb,eAAe,KAAK;AAAA,MACpB;AAAA,IACF,CAAC;AAAA,EACH;AAAA,EAEA,aAAa,oBAAiD;AAC5D,WAAO,IAAI,6CAA4C;AAAA,MACrD,OAAO,KAAK,MAAM,aAAa,kBAAkB;AAAA,MACjD,QAAQ,KAAK;AAAA,MACb,eAAe,KAAK;AAAA,MACpB,gBAAgB,KAAK;AAAA,IACvB,CAAC;AAAA,EACH;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EASA,MAAM,iBACJ,QACA,QACA,SACA;AACA,UAAM,iBAAiB,KAAK,eAAe,OAAO,MAAM;AAExD,UAAM,cAAc,MAAM,KAAK,MAC5B,aAAa;AAAA,MACZ,eAAe;AAAA,QACb,GAAI,KAAK,SAAS,iBAAiB,CAAC;AAAA,QACpC,GAAG,KAAK,eAAe;AAAA,MACzB;AAAA,IACF,CAAC,EACA,QAAQ,gBAAgB,SAAS;AAAA,MAChC,gBAAgB,yBAAyB;AAAA,MACzC,cAAc,EAAE,MAAM,KAAK,OAAO;AAAA,MAClC,WAAW;AAAA,QACT;AAAA,UACE,MAAM,KAAK;AAAA,UACX,aAAa,KAAK;AAAA,UAClB,YAAY,OAAO,cAAc;AAAA,QACnC;AAAA,MACF;AAAA,IACF,CAAC;AAEH,UAAM,YAAY,YAAY,QAAQ,CAAC,EAAG,QAAQ,cAAe;AAEjE,QAAI;AACF,aAAO;AAAA,QACL;AAAA,QACA;AAAA,QACA,OAAOC,YAAW,MAAM,SAAS;AAAA,QACjC,OAAO,KAAK,MAAM,aAAa,WAAW;AAAA,MAC5C;AAAA,IACF,SAAS,OAAO;AACd,YAAM,IAAI,iBAAiB;AAAA,QACzB;AAAA,QACA,OAAO;AAAA,MACT,CAAC;AAAA,IACH;AAAA,EACF;AAAA,EAEA,MAAM,eACJ,QACA,QACA,SACA;AACA,UAAM,iBAAiB,KAAK,eAAe,OAAO,MAAM;AAExD,WAAO,KAAK,MAAM,QAAQ,gBAAgB,SAAS;AAAA,MACjD,gBAAgB,yBAAyB;AAAA,MACzC,cAAc,EAAE,MAAM,KAAK,OAAO;AAAA,MAClC,WAAW;AAAA,QACT;AAAA,UACE,MAAM,KAAK;AAAA,UACX,aAAa,KAAK;AAAA,UAClB,YAAY,OAAO,cAAc;AAAA,QACnC;AAAA,MACF;AAAA,IACF,CAAC;AAAA,EACH;AAAA,EAEA,uBAAuB,OAAgB;AACrC,UAAM,QAAQ;AAEd,QAAI,MAAM,WAAW,yBAAyB;AAC5C,aAAO;AAAA,IACT;AAEA,UAAM,YAAY;AAElB,UAAM,cAAc,UAAU,QAAQ,CAAC;AAEvC,QAAI,YAAY,QAAQ,GAAG;AACzB,aAAO;AAAA,IACT;AAEA,WAAO,YAAY,MAAM,eAAe;AAAA,EAC1C;AAAA,EAEA,2BAA2B,iBAAyB;AAClD,WAAO,iBAAiB,eAAe;AAAA,EACzC;AACF;;;AE/MA,SAAS,gBAAgB;AACzB,OAAO,iBAAiB;;;ACAjB,SAAS,MAAM,GAAU;AAAC;;;AD6B1B,IAAM,oBAAN,MAAiD;AAAA;AAAA;AAAA;AAAA,EAItD,YAAY,UAAqC;AAC/C,SAAK,WAAW,IAAI,SAAS,eAAe,SAAS,KAAK,CAAC;AAAA,EAC7D;AAAA,EAEiB;AAAA,EAEjB,MAAM,SAASC,QAAc;AAC3B,WAAO,KAAK,SAAS,OAAOA,MAAI;AAAA,EAClC;AAAA,EAEA,MAAM,kBAAkBA,QAAc;AACpC,UAAM,SAAS,KAAK,SAAS,OAAOA,MAAI;AAExC,WAAO;AAAA,MACL;AAAA,MACA,YAAY,OAAO,IAAI,CAAC,UAAU,KAAK,SAAS,OAAO,CAAC,KAAK,CAAC,CAAC;AAAA,IACjE;AAAA,EACF;AAAA,EAEA,MAAM,WAAW,QAAkB;AACjC,WAAO,KAAK,SAAS,OAAO,MAAM;AAAA,EACpC;AACF;AAIA,SAAS,eACP,OAIA;AACA,UAAQ,OAAO;AAAA,IACb,KAAK;AAAA,IACL,KAAK;AAAA,IACL,KAAK;AAAA,IACL,KAAK;AAAA,IACL,KAAK;AAAA,IACL,KAAK;AAAA,IACL,KAAK;AAAA,IACL,KAAK;AAAA,IACL,KAAK;AAAA,IACL,KAAK;AAAA,IACL,KAAK;AAAA,IACL,KAAK;AAAA,IACL,KAAK;AAAA,IACL,KAAK;AAAA,IACL,KAAK;AAAA,IACL,KAAK;AAAA,IACL,KAAK;AAAA,IACL,KAAK;AAAA,IACL,KAAK;AAAA,IACL,KAAK;AAAA,IACL,KAAK;AAAA,IACL,KAAK,0BAA0B;AAC7B,aAAO;AAAA,IACT;AAAA,IACA,SAAS;AACP,YAAM,KAAK;AACX,YAAM,IAAI,MAAM,kBAAkB,KAAK,EAAE;AAAA,IAC3C;AAAA,EACF;AACF;;;AEpFO,IAAM,sCAAsC;AAM5C,IAAM,uCAAuC;AAEpD,eAAsB,6BAA6B;AAAA,EACjD;AAAA,EACA;AACF,GAGG;AACD,QAAM,YAAY,IAAI,kBAAkB;AAAA,IACtC,OAAO,8BAA8B,KAAK,EAAE;AAAA,EAC9C,CAAC;AAGD,MAAI,QAAQ,WAAW,MAAM;AAC3B,WAAO;AAAA,EACT;AAGA,MAAI,OAAO,QAAQ,YAAY,UAAU;AACvC,WACE,uCACC,MAAM,YAAY,WAAW,QAAQ,OAAO;AAAA,EAEjD;AAGA,MAAI,oBAAoB;AACxB,aAAW,WAAW,QAAQ,SAAS;AACrC,QAAI,QAAQ,SAAS,QAAQ;AAC3B,2BAAqB,MAAM,YAAY,WAAW,QAAQ,IAAI;AAAA,IAChE;AAAA,EACF;AAEA,SAAO;AACT;AAEA,eAAsB,4BAA4B;AAAA,EAChD;AAAA,EACA;AACF,GAGG;AACD,MAAI,SAAS;AACb,aAAW,WAAW,UAAU;AAC9B,cAAU,MAAM,6BAA6B,EAAE,SAAS,MAAM,CAAC;AAAA,EACjE;AACA,SAAO;AACT;;;ACzCO,IAAM,kCAAkC;AAAA,EAC7C,SAAS;AAAA,EACT,UAAU;AAAA,EACV,cAAc;AAAA,EACd,cAAc;AAAA,EACd,uBAAuB;AAAA,EACvB,sBAAsB;AAAA,EACtB,sBAAsB;AAAA,EACtB,wBAAwB;AAAA,EACxB,aAAa;AAAA,EACb,kBAAkB;AAAA,EAClB,kBAAkB;AAAA,EAClB,iBAAiB;AAAA,EACjB,sBAAsB;AAAA,EACtB,sBAAsB;AAAA,EACtB,sBAAsB;AAAA,EACtB,sBAAsB;AAAA,EACtB,qBAAqB;AAAA,EACrB,0BAA0B;AAC5B;AAEO,SAAS,8BAA8B,OAI5C;AAEA,MAAI,SAAS,iCAAiC;AAC5C,UAAM,oBACJ,gCAAgC,KAAgC;AAElE,WAAO;AAAA,MACL,WAAW;AAAA,MACX,aAAa;AAAA,MACb;AAAA,IACF;AAAA,EACF;AAIA,QAAM,CAAC,GAAG,WAAW,KAAK,MAAM,KAAK,IAAI,MAAM,MAAM,GAAG;AAExD,MACE,CAAC,iBAAiB,sBAAsB,cAAc,QAAQ,EAAE;AAAA,IAC9D;AAAA,EACF,GACA;AACA,UAAM,oBACJ,gCACE,SACF;AAEF,WAAO;AAAA,MACL;AAAA,MACA,aAAa;AAAA,MACb;AAAA,IACF;AAAA,EACF;AAEA,QAAM,IAAI,MAAM,kCAAkC,SAAS,GAAG;AAChE;AAwCO,IAAM,kBAAN,MAAM,yBACH,wBAKV;AAAA,EACE,YAAY,UAA8B;AACxC,UAAM,QAAQ;AAEd,UAAM,mBAAmB,8BAA8B,KAAK,SAAS,KAAK;AAE1E,SAAK,YAAY,IAAI,kBAAkB;AAAA,MACrC,OAAO,iBAAiB;AAAA,IAC1B,CAAC;AACD,SAAK,oBAAoB,iBAAiB;AAAA,EAC5C;AAAA,EAES,WAAW;AAAA,EACpB,IAAI,YAAY;AACd,WAAO,KAAK,SAAS;AAAA,EACvB;AAAA,EAES;AAAA,EACA;AAAA;AAAA;AAAA;AAAA;AAAA,EAMT,kBAAkB,UAA4B;AAC5C,WAAO,4BAA4B;AAAA,MACjC;AAAA,MACA,OAAO,KAAK;AAAA,IACd,CAAC;AAAA,EACH;AAAA,EAEA,IAAI,mBAAgD;AAClD,UAAM,yBAAwC;AAAA,MAC5C,GAAG;AAAA,MAEH;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,IACF;AAEA,WAAO,OAAO;AAAA,MACZ,OAAO,QAAQ,KAAK,QAAQ,EAAE;AAAA,QAAO,CAAC,CAAC,GAAG,MACxC,uBAAuB,SAAS,GAAG;AAAA,MACrC;AAAA,IACF;AAAA,EACF;AAAA,EAEA,oCAAoC;AAAA,IAClC;AAAA,IACA;AAAA,EACF,GAGG;AACD,WAAO,IAAI,4CAA4C;AAAA,MACrD,OAAO;AAAA,MACP;AAAA,MACA;AAAA,MACA,gBAAgB,SAAS;AAAA,IAC3B,CAAC;AAAA,EACH;AAAA,EAEA,wBACE,gBAGA;AACA,WAAO,gBAAgB,iBACnB,IAAI,6BAA6B;AAAA,MAC/B,OAAO,eAAe,WAAW,IAAI;AAAA,MACrC,UAAU;AAAA,IACZ,CAAC,IACD,IAAI,6BAA6B;AAAA,MAC/B,OAAO;AAAA,MACP,UAAU;AAAA,IACZ,CAAC;AAAA,EACP;AAAA,EAEA,iBAAiB;AACf,WAAO,KAAK,mBAAmBC,OAAK,CAAC;AAAA,EACvC;AAAA,EAEA,wBAAwB;AACtB,WAAO,KAAK,mBAAmBC,cAAY,CAAC;AAAA,EAC9C;AAAA,EAEA,iBAAiB;AACf,WAAO,KAAK,mBAAmBC,OAAK,CAAC;AAAA,EACvC;AAAA,EAEA,mBACE,gBAMA;AACA,WAAO,IAAI,4BAA4B;AAAA,MACrC,OAAO,KAAK,aAAa;AAAA,QACvB,eAAe;AAAA,UACb,GAAI,KAAK,SAAS,iBAAiB,CAAC;AAAA,UACpC,GAAG,eAAe;AAAA,QACpB;AAAA,MACF,CAAC;AAAA,MACD;AAAA,IACF,CAAC;AAAA,EACH;AAAA,EAEA,iBAAiB;AACf,WAAO,KAAK,aAAa,EAAE,gBAAgB,EAAE,MAAM,cAAc,EAAE,CAAC;AAAA,EACtE;AAAA,EAEA,aAAa,oBAAiD;AAC5D,WAAO,IAAI;AAAA,MACT,OAAO,OAAO,CAAC,GAAG,KAAK,UAAU,kBAAkB;AAAA,IACrD;AAAA,EACF;AACF;;;AC1OO,IAAM,gCAAgC;AAAA,EAC3C,0BAA0B;AAAA,IACxB,mBAAmB;AAAA,EACrB;AACF;AAEO,SAAS,oCACd,OAGA;AACA,SAAO,8BAA8B,KAAK;AAC5C;AA4BO,IAAM,wBAAN,MAAM,+BACH,8BAEV;AAAA,EACE,YAAY,UAAyC;AACnD,UAAM,QAAQ;AAEd,UAAM,mBAAmB;AAAA,MACvB,KAAK,SAAS;AAAA,IAChB;AAEA,SAAK,YAAY,IAAI,kBAAkB;AAAA,MACrC,OAAO,KAAK,SAAS;AAAA,IACvB,CAAC;AACD,SAAK,oBAAoB,iBAAiB;AAAA,EAC5C;AAAA,EAES,WAAW;AAAA,EACpB,IAAI,YAAY;AACd,WAAO,KAAK,SAAS;AAAA,EACvB;AAAA,EAES;AAAA,EACA;AAAA,EAET,MAAM,kBAAkB,OAAe;AACrC,WAAO,YAAY,KAAK,WAAW,KAAK;AAAA,EAC1C;AAAA,EAEA,IAAI,mBAA2D;AAC7D,UAAM,yBAAwC;AAAA,MAC5C,GAAG;AAAA,MAEH;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,IACF;AAEA,WAAO,OAAO;AAAA,MACZ,OAAO,QAAQ,KAAK,QAAQ,EAAE;AAAA,QAAO,CAAC,CAAC,GAAG,MACxC,uBAAuB,SAAS,GAAG;AAAA,MACrC;AAAA,IACF;AAAA,EACF;AAAA,EAEA,iBAAiB;AACf,WAAO,KAAK,mBAAmBC,MAAK,CAAC;AAAA,EACvC;AAAA,EAEA,wBAAwB;AACtB,WAAO,KAAK,mBAAmBC,aAAY,CAAC;AAAA,EAC9C;AAAA,EAEA,eAAe,SAAiD;AAC9D,WAAO,KAAK,mBAAmBC,MAAK,OAAO,CAAC;AAAA,EAC9C;AAAA,EAEA,mBACE,gBAMA;AACA,WAAO,IAAI,iCAAiC;AAAA,MAC1C,OAAO,KAAK,aAAa;AAAA,QACvB,eAAe;AAAA,UACb,GAAI,KAAK,SAAS,iBAAiB,CAAC;AAAA,UACpC,GAAG,eAAe;AAAA,QACpB;AAAA,MACF,CAAC;AAAA,MACD;AAAA,IACF,CAAC;AAAA,EACH;AAAA,EAEA,aAAa,oBAA4D;AACvE,WAAO,IAAI;AAAA,MACT,OAAO,OAAO,CAAC,GAAG,KAAK,UAAU,kBAAkB;AAAA,IACrD;AAAA,EACF;AACF;;;ACtJA;AAAA;AAAA,aAAAC;AAAA,EAAA;AAAA;AAAA,2BAAAC;AAAA,EAAA,+BAAAC;AAAA,EAAA,sBAAAC;AAAA,EAAA,uBAAAC;AAAA,EAAA,oBAAAC;AAAA,EAAA,iBAAAC;AAAA,EAAA;AAAA;;;ACAA,SAAS,KAAAC,WAAS;AA6CX,IAAM,6BAAN,MAAM,oCACH,cAEV;AAAA,EACE,YAAY,UAAyC;AACnD,UAAM,EAAE,SAAS,CAAC;AAAA,EACpB;AAAA,EAES,WAAW;AAAA,EACpB,IAAI,YAAY;AACd,WAAO,KAAK,SAAS;AAAA,EACvB;AAAA,EAEA,MAAM,QACJ,QACA,aACA,SAGiB;AACjB,UAAM,MAAM,KAAK,SAAS,OAAO,IAAI,uBAAuB;AAC5D,UAAM,cAAc,YAAY,KAAK;AACrC,UAAM,SAAS,YAAY,KAAK;AAChC,UAAM,iBAAiB,QAAQ;AAE/B,WAAO,yBAAyB;AAAA,MAC9B,OAAO,IAAI;AAAA,MACX,UAAU,IAAI;AAAA,MACd,MAAM,YACJ,cAAc;AAAA,QACZ,KAAK,IAAI,YAAY,qBAAqB;AAAA,QAC1C,SAAS,IAAI,QAAQ;AAAA,UACnB,cAAc,YAAY;AAAA,UAC1B,YAAY,YAAY;AAAA,UACxB,KAAK,YAAY;AAAA,UACjB,QAAQ,YAAY;AAAA,QACtB,CAAC;AAAA,QACD,MAAM;AAAA,UACJ;AAAA,UACA,GAAG,KAAK,SAAS;AAAA,UACjB,MAAM,KAAK,SAAS;AAAA,UACpB,iBAAiB,eAAe;AAAA,UAChC,MAAM,KAAK,SAAS,4BAA4B,SAAS;AAAA,QAC3D;AAAA,QACA,uBAAuB;AAAA,QACvB,2BAA2B,eAAe;AAAA,QAC1C;AAAA,MACF,CAAC;AAAA,IACL,CAAC;AAAA,EACH;AAAA,EAEA,IAAI,mBAA2D;AAC7D,UAAM,yBAAwC;AAAA,MAC5C;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,IACF;AAEA,WAAO,OAAO;AAAA,MACZ,OAAO,QAAQ,KAAK,QAAQ,EAAE;AAAA,QAAO,CAAC,CAAC,GAAG,MACxC,uBAAuB,SAAS,GAAG;AAAA,MACrC;AAAA,IACF;AAAA,EACF;AAAA,EAEA,MAAM,iBAAiB,QAAgB,SAA8B;AACnE,UAAM,cAAc,MAAM,KAAK,QAAQ,QAAQ,SAAS;AAAA,MACtD,gBAAgB,oCAAoC;AAAA,IACtD,CAAC;AAED,WAAO;AAAA,MACL;AAAA,MACA,cAAc,YAAY,KAAK,IAAI,CAAC,SAAS,KAAK,QAAQ;AAAA,IAC5D;AAAA,EACF;AAAA,EACA,mBACE,gBAMA;AACA,WAAO,IAAI,mCAAmC;AAAA,MAC5C,OAAO;AAAA,MACP;AAAA,IACF,CAAC;AAAA,EACH;AAAA,EAEA,aAAa,oBAA4D;AACvE,WAAO,IAAI;AAAA,MACT,OAAO,OAAO,CAAC,GAAG,KAAK,UAAU,kBAAkB;AAAA,IACrD;AAAA,EACF;AACF;AAOA,IAAM,iCAAiCC,IAAE,OAAO;AAAA,EAC9C,SAASA,IAAE,OAAO;AAAA,EAClB,MAAMA,IAAE;AAAA,IACNA,IAAE,OAAO;AAAA,MACP,KAAKA,IAAE,OAAO;AAAA,IAChB,CAAC;AAAA,EACH;AACF,CAAC;AAMD,IAAM,wCAAwCA,IAAE,OAAO;AAAA,EACrD,SAASA,IAAE,OAAO;AAAA,EAClB,MAAMA,IAAE;AAAA,IACNA,IAAE,OAAO;AAAA,MACP,UAAUA,IAAE,OAAO;AAAA,IACrB,CAAC;AAAA,EACH;AACF,CAAC;AAMM,IAAM,sCAAsC;AAAA,EACjD,KAAK;AAAA,IACH,MAAM;AAAA,IACN,SAAS;AAAA,MACP,UAAU,8BAA8B;AAAA,IAC1C;AAAA,EACF;AAAA,EACA,YAAY;AAAA,IACV,MAAM;AAAA,IACN,SAAS;AAAA,MACP,UAAU,qCAAqC;AAAA,IACjD;AAAA,EACF;AACF;;;ACzIO,IAAM,oBAAN,MAAM,2BACH,cAEV;AAAA,EACE,YAAY,UAAqC;AAC/C,UAAM,EAAE,SAAS,CAAC;AAAA,EACpB;AAAA,EAES,WAAW;AAAA,EAEpB,IAAI,QAAQ;AACV,WAAO,KAAK,SAAS;AAAA,EACvB;AAAA,EAEA,IAAI,YAAY;AACd,WAAO,KAAK,SAAS;AAAA,EACvB;AAAA,EAEA,MAAc,QACZC,QACA,aACqB;AACrB,UAAM,MAAM,KAAK,SAAS,OAAO,IAAI,uBAAuB;AAC5D,UAAM,cAAc,YAAY,KAAK;AAErC,WAAO,yBAAyB;AAAA,MAC9B,OAAO,IAAI;AAAA,MACX,UAAU,IAAI;AAAA,MACd,MAAM,YACJ,cAAc;AAAA,QACZ,KAAK,IAAI,YAAY,eAAe;AAAA,QACpC,SAAS,IAAI,QAAQ;AAAA,UACnB,cAAc,YAAY;AAAA,UAC1B,YAAY,YAAY;AAAA,UACxB,KAAK,YAAY;AAAA,UACjB,QAAQ,YAAY;AAAA,QACtB,CAAC;AAAA,QACD,MAAM;AAAA,UACJ,OAAOA;AAAA,UACP,OAAO,KAAK,SAAS;AAAA,UACrB,OAAO,KAAK,SAAS;AAAA,UACrB,OAAO,KAAK,SAAS;AAAA,UACrB,iBAAiB,KAAK,SAAS;AAAA,QACjC;AAAA,QACA,uBAAuB;AAAA,QACvB,2BAA2B,+BAA+B;AAAA,QAC1D;AAAA,MACF,CAAC;AAAA,IACL,CAAC;AAAA,EACH;AAAA,EAEA,IAAI,mBAAuD;AACzD,WAAO;AAAA,MACL,OAAO,KAAK,SAAS;AAAA,MACrB,OAAO,KAAK,SAAS;AAAA,MACrB,OAAO,KAAK,SAAS;AAAA,MACrB,gBAAgB,KAAK,SAAS;AAAA,IAChC;AAAA,EACF;AAAA,EAEA,yBAAyBA,QAAc,SAA8B;AACnE,WAAO,KAAK,QAAQA,QAAM,OAAO;AAAA,EACnC;AAAA,EAEA,aAAa,oBAAwD;AACnE,WAAO,IAAI,mBAAkB;AAAA,MAC3B,GAAG,KAAK;AAAA,MACR,GAAG;AAAA,IACL,CAAC;AAAA,EACH;AACF;;;ACvHA,OAAOC,SAAO;AASP,IAAM,+BAA+B;AAAA,EAC1C,0BAA0B;AAAA,IACxB,mBAAmB;AAAA,IACnB,YAAY;AAAA,EACd;AAAA,EACA,0BAA0B;AAAA,IACxB,mBAAmB;AAAA,IACnB,YAAY;AAAA,EACd;AAAA,EAEA,0BAA0B;AAAA,IACxB,mBAAmB;AAAA,IACnB,YAAY;AAAA,EACd;AACF;AAUO,IAAMC,qCAAoCC,IAAE,OAAO;AAAA,EACxD,QAAQA,IAAE,QAAQ,MAAM;AAAA,EACxB,MAAMA,IAAE;AAAA,IACNA,IAAE,OAAO;AAAA,MACP,QAAQA,IAAE,QAAQ,WAAW;AAAA,MAC7B,WAAWA,IAAE,MAAMA,IAAE,OAAO,CAAC;AAAA,MAC7B,OAAOA,IAAE,OAAO;AAAA,IAClB,CAAC;AAAA,EACH;AAAA,EACA,OAAOA,IAAE,OAAO;AAAA,EAChB,OAAOA,IACJ,OAAO;AAAA,IACN,eAAeA,IAAE,OAAO;AAAA,IACxB,cAAcA,IAAE,OAAO;AAAA,EACzB,CAAC,EACA,SAAS;AAAA;AACd,CAAC;AAgBM,IAAM,2BAAN,MAAM,kCACH,iCAEV;AAAA,EACE,YAAY,UAA4C;AACtD,UAAM,QAAQ;AAEd,SAAK,YAAY,IAAI,kBAAkB,EAAE,OAAO,KAAK,UAAU,CAAC;AAChE,SAAK,oBACH,6BAA6B,KAAK,SAAS,EAAE;AAE/C,SAAK,aACH,KAAK,SAAS,cACd,6BAA6B,KAAK,SAAS,EAAE;AAAA,EACjD;AAAA,EAES,WAAW;AAAA,EACpB,IAAI,YAAY;AACd,WAAO,KAAK,SAAS;AAAA,EACvB;AAAA,EAES;AAAA,EAEA;AAAA,EACA;AAAA,EAET,MAAM,YAAY,OAAe;AAC/B,WAAO,YAAY,KAAK,WAAW,KAAK;AAAA,EAC1C;AAAA,EAEA,IAAI,mBAA8D;AAChE,WAAO,CAAC;AAAA,EACV;AAAA,EAEA,aAAa,oBAAsD;AACjE,WAAO,IAAI;AAAA,MACT,OAAO,OAAO,CAAC,GAAG,KAAK,UAAU,kBAAkB;AAAA,IACrD;AAAA,EACF;AACF;;;ACxGA,SAAS,KAAAC,WAAS;;;ACAX,SAAS,sBAAsB,UAAkB;AACtD,QAAM,qBAAqB,SAAS,MAAM,GAAG,EAAE,CAAC,EAAE,YAAY;AAE9D,UAAQ,oBAAoB;AAAA,IAC1B,KAAK;AACH,aAAO;AAAA,IACT,KAAK;AACH,aAAO;AAAA,IACT,KAAK;AACH,aAAO;AAAA,IACT,KAAK;AACH,aAAO;AAAA,IACT,KAAK;AAAA,IACL,KAAK;AACH,aAAO;AAAA,IACT,KAAK;AAAA,IACL,KAAK;AACH,aAAO;AAAA,IACT,KAAK;AACH,aAAO;AAAA,IACT,KAAK;AACH,aAAO;AAAA,IACT;AACE,YAAM,IAAI,MAAM,6BAA6B,QAAQ,EAAE;AAAA,EAC3D;AACF;;;AD8CO,IAAM,2BAAN,MAAM,kCACH,cAEV;AAAA,EACE,YAAY,UAA4C;AACtD,UAAM,EAAE,SAAS,CAAC;AAAA,EACpB;AAAA,EAES,WAAW;AAAA,EACpB,IAAI,YAAY;AACd,WAAO,KAAK,SAAS;AAAA,EACvB;AAAA,EAEA,MAAM,aACJ;AAAA,IACE;AAAA,IACA;AAAA,EACF,GAIA,SACA;AACA,UAAM,cAAc,MAAM,KAAK;AAAA,MAC7B;AAAA,QACE,eAAe,sBAAsB,QAAQ;AAAA,QAC7C,WAAW,+BAA+B,SAAS;AAAA,MACrD;AAAA,MACA;AAAA,MACA,EAAE,gBAAgB,kCAAkC,YAAY;AAAA,IAClE;AAEA,WAAO;AAAA,MACL;AAAA,MACA,eAAe,YAAY;AAAA,IAC7B;AAAA,EACF;AAAA,EAEA,MAAM,QACJ,OAIA,aACA,SAGiB;AACjB,UAAM,MAAM,KAAK,SAAS,OAAO,IAAI,uBAAuB;AAC5D,UAAM,cAAc,aAAa,KAAK;AAEtC,WAAO,yBAAyB;AAAA,MAC9B,OAAO,IAAI;AAAA,MACX,UAAU,IAAI;AAAA,MACd,MAAM,YAAY;AAChB,cAAM,WAAW,SAAS,MAAM,aAAa;AAE7C,cAAM,WAAW,IAAI,SAAS;AAC9B,iBAAS,OAAO,QAAQ,IAAI,KAAK,CAAC,MAAM,SAAS,CAAC,GAAG,QAAQ;AAC7D,iBAAS,OAAO,SAAS,KAAK,SAAS,KAAK;AAE5C,YAAI,KAAK,SAAS,UAAU,MAAM;AAChC,mBAAS,OAAO,UAAU,KAAK,SAAS,MAAM;AAAA,QAChD;AAEA,YAAI,QAAQ,kBAAkB,MAAM;AAClC,mBAAS,OAAO,mBAAmB,QAAQ,eAAe,IAAI;AAAA,QAChE;AAEA,YAAI,KAAK,SAAS,eAAe,MAAM;AACrC,mBAAS,OAAO,eAAe,KAAK,SAAS,YAAY,SAAS,CAAC;AAAA,QACrE;AAEA,YAAI,KAAK,SAAS,YAAY,MAAM;AAClC,mBAAS,OAAO,YAAY,KAAK,SAAS,QAAQ;AAAA,QACpD;AAEA,eAAO,UAAU;AAAA,UACf,KAAK,IAAI,YAAY,uBAAuB;AAAA,UAC5C,SAAS,IAAI,QAAQ;AAAA,YACnB,cAAc,YAAY;AAAA,YAC1B,YAAY,YAAY;AAAA,YACxB,KAAK,YAAY;AAAA,YACjB,QAAQ,YAAY;AAAA,UACtB,CAAC;AAAA,UACD,MAAM;AAAA,YACJ,SAAS;AAAA,YACT,QAAQ;AAAA,cACN,OAAO,KAAK,SAAS;AAAA,cACrB,QAAQ,KAAK,SAAS;AAAA,cACtB,iBAAiB,QAAQ;AAAA,cACzB,aAAa,KAAK,SAAS;AAAA,cAC3B,UAAU,KAAK,SAAS;AAAA,YAC1B;AAAA,UACF;AAAA,UACA,uBAAuB;AAAA,UACvB,2BAA2B,QAAQ,eAAe;AAAA,UAClD;AAAA,QACF,CAAC;AAAA,MACH;AAAA,IACF,CAAC;AAAA,EACH;AAAA,EAEA,IAAI,mBAA8D;AAChE,WAAO;AAAA,MACL,UAAU,KAAK,SAAS;AAAA,MACxB,aAAa,KAAK,SAAS;AAAA,IAC7B;AAAA,EACF;AAAA,EAEA,aAAa,oBAAsD;AACjE,WAAO,IAAI;AAAA,MACT,OAAO,OAAO,CAAC,GAAG,KAAK,UAAU,kBAAkB;AAAA,IACrD;AAAA,EACF;AACF;AAEA,IAAM,gCAAgCC,IAAE,OAAO;AAAA,EAC7C,MAAMA,IAAE,OAAO;AACjB,CAAC;AAMD,IAAM,uCAAuCA,IAAE,OAAO;AAAA,EACpD,MAAMA,IAAE,QAAQ,YAAY;AAAA,EAC5B,UAAUA,IAAE,OAAO;AAAA,EACnB,UAAUA,IAAE,OAAO;AAAA,EACnB,UAAUA,IAAE;AAAA,IACVA,IAAE,OAAO;AAAA,MACP,IAAIA,IAAE,OAAO;AAAA,MACb,MAAMA,IAAE,OAAO;AAAA,MACf,OAAOA,IAAE,OAAO;AAAA,MAChB,KAAKA,IAAE,OAAO;AAAA,MACd,MAAMA,IAAE,OAAO;AAAA,MACf,QAAQA,IAAE,MAAMA,IAAE,OAAO,CAAC;AAAA,MAC1B,aAAaA,IAAE,OAAO;AAAA,MACtB,aAAaA,IAAE,OAAO;AAAA,MACtB,mBAAmBA,IAAE,OAAO;AAAA,MAC5B,gBAAgBA,IAAE,OAAO;AAAA,MACzB,WAAWA,IAAE,QAAQ,EAAE,SAAS;AAAA,IAClC,CAAC;AAAA,EACH;AAAA,EACA,MAAMA,IAAE,OAAO;AACjB,CAAC;AAWM,IAAM,oCAAoC;AAAA,EAC/C,MAAM;AAAA,IACJ,MAAM;AAAA,IACN,SAAS;AAAA,MACP,UAAU,6BAA6B;AAAA,IACzC;AAAA,EACF;AAAA,EACA,aAAa;AAAA,IACX,MAAM;AAAA,IACN,SAAS;AAAA,MACP,UAAU,oCAAoC;AAAA,IAChD;AAAA,EACF;AAAA,EACA,MAAM;AAAA,IACJ,MAAM;AAAA,IACN,SAAS,0BAA0B;AAAA,EACrC;AAAA,EACA,KAAK;AAAA,IACH,MAAM;AAAA,IACN,SAAS,0BAA0B;AAAA,EACrC;AAAA,EACA,KAAK;AAAA,IACH,MAAM;AAAA,IACN,SAAS,0BAA0B;AAAA,EACrC;AACF;;;AJxNO,SAASC,KACd,UAGA;AACA,SAAO,IAAI,uBAAuB,QAAQ;AAC5C;AASO,SAAS,SAAS,UAA8C;AACrE,SAAO,IAAI,4BAA4B,QAAQ;AACjD;AAsBO,SAASC,yBACd,UACA;AACA,SAAO,IAAI,sBAAsB,QAAQ;AAC3C;AAuBO,SAASC,mBAAkB,UAA8B;AAC9D,SAAO,IAAI,gBAAgB,QAAQ;AACrC;AAkBO,SAASC,cAAa,UAA4C;AACvE,SAAO,IAAI,yBAAyB,QAAQ;AAC9C;AASO,SAASC,iBAAgB,UAAqC;AACnE,SAAO,IAAI,kBAAkB,QAAQ;AACvC;AAoBO,SAAS,YAAY,UAA4C;AACtE,SAAO,IAAI,yBAAyB,QAAQ;AAC9C;AAeO,SAASC,gBAAe,UAAyC;AACtE,SAAO,IAAI,2BAA2B,QAAQ;AAChD;AAmBO,SAASC,WAAU,UAAqC;AAC7D,SAAO,IAAI,kBAAkB,QAAQ;AACvC;;;AMrLO,IAAM,8BAAN,cACG,oCAEV;AAAA,EACE,YACE,WAEI,CAAC,GACL;AACA,UAAM;AAAA,MACJ,GAAG;AAAA,MACH,SAAS;AAAA,QACP,eAAe,UAAU,WAAW;AAAA,UAClC,QAAQ,SAAS;AAAA,UACjB,yBAAyB;AAAA,UACzB,aAAa;AAAA,QACf,CAAC,CAAC;AAAA,MACJ;AAAA,MACA,iBAAiB;AAAA,QACf,UAAU;AAAA,QACV,MAAM;AAAA,QACN,MAAM;AAAA,QACN,MAAM;AAAA,MACR;AAAA,IACF,CAAC;AAAA,EACH;AAAA,EAES,WAAW;AACtB;;;ACHO,IAAM,4BAAN,MAAM,mCACH,wBAKV;AAAA,EACE,YAAY,UAAwC;AAClD,UAAM,QAAQ;AAAA,EAChB;AAAA,EAEA,IAAI,WAAyC;AAC3C,WACE,KAAK,SAAS,YAAY,KAAK,SAAS,IAAI,YAAY;AAAA,EAE5D;AAAA,EAEA,IAAI,YAAY;AACd,WAAO,KAAK,SAAS;AAAA,EACvB;AAAA,EAES,oBAAoB;AAAA,EACpB,YAAY;AAAA,EACZ,oBAAoB;AAAA,EAE7B,IAAI,mBAA0D;AAC5D,UAAM,yBAAwC;AAAA,MAC5C,GAAG;AAAA,MAEH;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,IACF;AAEA,WAAO,OAAO;AAAA,MACZ,OAAO,QAAQ,KAAK,QAAQ,EAAE;AAAA,QAAO,CAAC,CAAC,GAAG,MACxC,uBAAuB,SAAS,GAAG;AAAA,MACrC;AAAA,IACF;AAAA,EACF;AAAA,EAEA,wBACE,gBAGA;AACA,WAAO,gBAAgB,iBACnB,IAAI,6BAA6B;AAAA,MAC/B,OAAO,eAAe,WAAW,IAAI;AAAA,MACrC,UAAU;AAAA,IACZ,CAAC,IACD,IAAI,6BAA6B;AAAA,MAC/B,OAAO;AAAA,MACP,UAAU;AAAA,IACZ,CAAC;AAAA,EACP;AAAA,EAEA,iBAAiB;AACf,WAAO,KAAK,mBAAmBC,OAAK,CAAC;AAAA,EACvC;AAAA,EAEA,wBAAwB;AACtB,WAAO,KAAK,mBAAmBC,cAAY,CAAC;AAAA,EAC9C;AAAA,EAEA,iBAAiB;AACf,WAAO,KAAK,mBAAmBC,OAAK,CAAC;AAAA,EACvC;AAAA,EAEA,mBACE,gBAMA;AACA,WAAO,IAAI,4BAA4B;AAAA,MACrC,OAAO,KAAK,aAAa;AAAA,QACvB,eAAe;AAAA,UACb,GAAI,KAAK,SAAS,iBAAiB,CAAC;AAAA,UACpC,GAAG,eAAe;AAAA,QACpB;AAAA,MACF,CAAC;AAAA,MACD;AAAA,IACF,CAAC;AAAA,EACH;AAAA,EAEA,iBAAiB;AACf,WAAO,KAAK,aAAa,EAAE,gBAAgB,EAAE,MAAM,cAAc,EAAE,CAAC;AAAA,EACtE;AAAA,EAEA,aAAa,oBAA2D;AACtE,WAAO,IAAI;AAAA,MACT,OAAO,OAAO,CAAC,GAAG,KAAK,UAAU,kBAAkB;AAAA,IACrD;AAAA,EACF;AACF;;;AC5GO,IAAM,kCAAN,MAAM,yCACH,8BAGV;AAAA,EACE,YAAY,UAAmD;AAC7D,UAAM,QAAQ;AAAA,EAChB;AAAA,EAEA,IAAI,WAAyC;AAC3C,WACE,KAAK,SAAS,YAAY,KAAK,SAAS,IAAI,YAAY;AAAA,EAE5D;AAAA,EAEA,IAAI,YAAY;AACd,WAAO,KAAK,SAAS;AAAA,EACvB;AAAA,EAES,oBAAoB;AAAA,EACpB,YAAY;AAAA,EACZ,oBAAoB;AAAA,EAE7B,IAAI,mBAAqE;AACvE,UAAM,yBAAwC;AAAA,MAC5C,GAAG;AAAA,MAEH;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,IACF;AAEA,WAAO,OAAO;AAAA,MACZ,OAAO,QAAQ,KAAK,QAAQ,EAAE;AAAA,QAAO,CAAC,CAAC,GAAG,MACxC,uBAAuB,SAAS,GAAG;AAAA,MACrC;AAAA,IACF;AAAA,EACF;AAAA,EAEA,iBAAiB;AACf,WAAO,KAAK,mBAAmBC,MAAK,CAAC;AAAA,EACvC;AAAA,EAEA,wBAAwB;AACtB,WAAO,KAAK,mBAAmBC,aAAY,CAAC;AAAA,EAC9C;AAAA,EAEA,eAAe,SAAiD;AAC9D,WAAO,KAAK,mBAAmBC,MAAK,OAAO,CAAC;AAAA,EAC9C;AAAA,EAEA,mBACE,gBAMA;AACA,WAAO,IAAI,iCAAiC;AAAA,MAC1C,OAAO,KAAK,aAAa;AAAA,QACvB,eAAe;AAAA,UACb,GAAI,KAAK,SAAS,iBAAiB,CAAC;AAAA,UACpC,GAAG,eAAe;AAAA,QACpB;AAAA,MACF,CAAC;AAAA,MACD;AAAA,IACF,CAAC;AAAA,EACH;AAAA,EAEA,aACE,oBACA;AACA,WAAO,IAAI;AAAA,MACT,OAAO,OAAO,CAAC,GAAG,KAAK,UAAU,kBAAkB;AAAA,IACrD;AAAA,EACF;AACF;;;ACtHA;AAAA;AAAA,2BAAAC;AAAA,EAAA,+BAAAC;AAAA,EAAA;AAAA;AAAA,sBAAAC;AAAA,EAAA;AAAA;;;ACkBO,IAAM,qCAAN,MAAM,4CACH,iCAEV;AAAA,EACE,YAAY,UAAsD;AAChE,UAAM,QAAQ;AAAA,EAChB;AAAA,EAEA,IAAI,WAAyC;AAC3C,WACE,KAAK,SAAS,YAAY,KAAK,SAAS,IAAI,YAAY;AAAA,EAE5D;AAAA,EAEA,IAAI,YAAY;AACd,WAAO,KAAK,SAAS;AAAA,EACvB;AAAA,EAEA,IAAI,aAAa;AACf,WAAO,KAAK,SAAS;AAAA,EACvB;AAAA,EAEA,IAAI,mBAAwE;AAC1E,WAAO;AAAA,MACL,YAAY,KAAK,SAAS;AAAA,IAC5B;AAAA,EACF;AAAA,EAEA,aAAa,oBAAgE;AAC3E,WAAO,IAAI;AAAA,MACT,OAAO,OAAO,CAAC,GAAG,KAAK,UAAU,kBAAkB;AAAA,IACrD;AAAA,EACF;AACF;;;ACrCO,IAAM,6BAAN,cACG,oCAEV;AAAA,EACE,YACE,WAEI,CAAC,GACL;AACA,UAAM;AAAA,MACJ,GAAG;AAAA,MACH,SAAS;AAAA,QACP,eAAe,UAAU,WAAW;AAAA,UAClC,QAAQ,SAAS;AAAA,UACjB,yBAAyB;AAAA,UACzB,aAAa;AAAA,QACf,CAAC,CAAC;AAAA,MACJ;AAAA,MACA,iBAAiB;AAAA,QACf,UAAU;AAAA,QACV,MAAM;AAAA,QACN,MAAM;AAAA,QACN,MAAM;AAAA,MACR;AAAA,IACF,CAAC;AAAA,EACH;AAAA,EAES,WAAW;AACtB;;;AC5BO,IAAM,6BAAN,cACG,oCAEV;AAAA,EACE,YACE,WAEI,CAAC,GACL;AACA,UAAM;AAAA,MACJ,GAAG;AAAA,MACH,SAAS;AAAA,QACP,eAAe,UAAU,WAAW;AAAA,UAClC,QAAQ,SAAS;AAAA,UACjB,yBAAyB;AAAA,UACzB,aAAa;AAAA,QACf,CAAC,CAAC;AAAA,MACJ;AAAA,MACA,iBAAiB;AAAA,QACf,UAAU;AAAA,QACV,MAAM;AAAA,QACN,MAAM;AAAA,QACN,MAAM;AAAA,MACR;AAAA,IACF,CAAC;AAAA,EACH;AAAA,EAES,WAAW;AACtB;;;AHrBO,SAAS,eACd,WAEI,CAAC,GACL;AACA,SAAO,IAAI,4BAA4B,QAAQ;AACjD;AASO,SAAS,cACd,WAEI,CAAC,GACL;AACA,SAAO,IAAI,2BAA2B,QAAQ;AAChD;AASO,SAAS,cACd,WAEI,CAAC,GACL;AACA,SAAO,IAAI,2BAA2B,QAAQ;AAChD;AAwBO,SAASC,yBACd,UACA;AACA,SAAO,IAAI,gCAAgC,QAAQ;AACrD;AA4BO,SAASC,mBAAkB,UAAwC;AACxE,SAAO,IAAI,0BAA0B,QAAQ;AAC/C;AAkBO,SAASC,cACd,UACA;AACA,SAAO,IAAI,mCAAmC,QAAQ;AACxD;;;AI/HO,IAAM,4BAAN,cAAwC,oCAAoC;AAAA,EACjF,YACE,WAEI,CAAC,GACL;AACA,UAAM;AAAA,MACJ,GAAG;AAAA,MACH,SAAS,SAAS,WAAW;AAAA,QAC3B,eAAe,UAAU,WAAW;AAAA,UAClC,QAAQ,SAAS;AAAA,UACjB,yBAAyB;AAAA,UACzB,aAAa;AAAA,QACf,CAAC,CAAC;AAAA,MACJ;AAAA,MACA,iBAAiB;AAAA,QACf,UAAU;AAAA,QACV,MAAM;AAAA,QACN,MAAM;AAAA,QACN,MAAM;AAAA,MACR;AAAA,IACF,CAAC;AAAA,EACH;AACF;;;ACjCA,SAAS,KAAAC,WAAS;AAQlB,IAAM,2BAA2BC,IAAE,OAAO;AAAA,EACxC,SAASA,IAAE,OAAO;AACpB,CAAC;AAIM,IAAM,qCACX,+BAA+B;AAAA,EAC7B,aAAa,UAAU,wBAAwB;AAAA,EAC/C,gBAAgB,CAAC,UAAU,MAAM;AACnC,CAAC;;;AClBH;AAAA;AAAA,aAAAC;AAAA,EAAA,sBAAAC;AAAA;;;ACAA,SAAS,KAAAC,WAAS;;;ACUX,SAAS,kCAGd;AACA,SAAO;AAAA,IACL,QAAQ,CAAC,gBAAgB,CAAC,EAAE,MAAM,YAAY,CAAC;AAAA,EACjD;AACF;;;ADqHO,IAAM,gCAAN,MAAM,uCACH,cAMV;AAAA,EACE,YAAY,UAA4C;AACtD,UAAM,EAAE,SAAS,CAAC;AAAA,EACpB;AAAA,EAES,WAAW;AAAA,EAEpB,IAAI,YAAY;AACd,WAAO,KAAK,SAAS;AAAA,EACvB;AAAA,EAEA,MAAM,QACJ,OACA,aAC2C;AAC3C,UAAM,MAAM,KAAK,SAAS,OAAO,IAAI,0BAA0B;AAC/D,UAAM,cAAc,YAAY,KAAK;AAErC,WAAO,yBAAyB;AAAA,MAC9B,OAAO,KAAK,SAAS,KAAK;AAAA,MAC1B,UAAU,KAAK,SAAS,KAAK;AAAA,MAC7B,MAAM,YACJ,cAAc;AAAA,QACZ,KAAK,IAAI;AAAA,UACP,eAAe,KAAK,SAAS,KAAK;AAAA,QACpC;AAAA,QACA,SAAS,IAAI,QAAQ;AAAA,UACnB,cAAc,YAAY;AAAA,UAC1B,YAAY,YAAY;AAAA,UACxB,KAAK,YAAY;AAAA,UACjB,QAAQ,YAAY;AAAA,QACtB,CAAC;AAAA,QACD,MAAM;AAAA,UACJ,QAAQ,KAAK,SAAS;AAAA,UACtB,OAAO,KAAK,SAAS;AAAA,UACrB,cAAc;AAAA,UACd,WAAW,KAAK,SAAS;AAAA,UACzB,sBAAsB,KAAK,SAAS;AAAA,UACpC,SAAS,KAAK,SAAS;AAAA,UACvB,SAAS,KAAK,SAAS;AAAA,UACvB,MAAM,KAAK,SAAS;AAAA,UACpB,OAAO,KAAK,SAAS;AAAA,UACrB,cAAc,KAAK,SAAS;AAAA,QAC9B;AAAA,QACA,uBAAuB;AAAA,QACvB,2BAA2B;AAAA,UACzB,UAAU,sCAAsC;AAAA,QAClD;AAAA,QACA;AAAA,MACF,CAAC;AAAA,IACL,CAAC;AAAA,EACH;AAAA,EAEA,IAAI,mBAA8D;AAChE,WAAO;AAAA,MACL,qBAAqB,KAAK,SAAS;AAAA,MACnC,QAAQ,KAAK,SAAS;AAAA,MACtB,OAAO,KAAK,SAAS;AAAA,MACrB,UAAU,KAAK,SAAS;AAAA,MACxB,oBAAoB,KAAK,SAAS;AAAA,MAClC,SAAS,KAAK,SAAS;AAAA,MACvB,MAAM,KAAK,SAAS;AAAA,MACpB,OAAO,KAAK,SAAS;AAAA,MACrB,aAAa,KAAK,SAAS;AAAA,IAC7B;AAAA,EACF;AAAA,EAEA,MAAM,iBACJ,QACA,aACA;AACA,UAAM,cAAc,MAAM,KAAK,QAAQ,QAAQ,WAAW;AAE1D,WAAO;AAAA,MACL;AAAA,MACA,cAAc,YAAY,UAAU,IAAI,CAAC,aAAa,SAAS,MAAM;AAAA,IACvE;AAAA,EACF;AAAA,EAEA,iBAAiB;AACf,WAAO,KAAK,mBAAmB,gCAAgC,CAAC;AAAA,EAClE;AAAA,EAEA,mBACE,gBAMA;AACA,WAAO,IAAI,mCAAmC;AAAA,MAC5C,OAAO;AAAA,MACP;AAAA,IACF,CAAC;AAAA,EACH;AAAA,EAEA,aAAa,oBAA+D;AAC1E,WAAO,IAAI;AAAA,MACT,OAAO,OAAO,CAAC,GAAG,KAAK,UAAU,kBAAkB;AAAA,IACrD;AAAA,EACF;AACF;AAEA,IAAM,yCAAyCC,IAAE,OAAO;AAAA,EACtD,WAAWA,IAAE;AAAA,IACXA,IAAE,OAAO;AAAA,MACP,QAAQA,IAAE,OAAO;AAAA,MACjB,MAAMA,IAAE,OAAO;AAAA,MACf,cAAcA,IAAE,KAAK,CAAC,WAAW,SAAS,kBAAkB,CAAC;AAAA,IAC/D,CAAC;AAAA,EACH;AACF,CAAC;;;ADlPM,SAASC,MACd,UAGA;AACA,SAAO,IAAI,0BAA0B,QAAQ;AAC/C;AAyBO,SAASC,gBAAe,UAA4C;AACzE,SAAO,IAAI,8BAA8B,QAAQ;AACnD;;;AGnCO,IAAM,6BAAN,cAAyC,oCAAoC;AAAA,EAClF,YAAY,WAAuD,CAAC,GAAG;AACrE,UAAM;AAAA,MACJ,GAAG;AAAA,MACH,iBAAiB;AAAA,QACf,UAAU;AAAA,QACV,MAAM;AAAA,QACN,MAAM;AAAA,QACN,MAAM;AAAA,MACR;AAAA,IACF,CAAC;AAAA,EACH;AACF;;;ACrBA;AAAA;AAAA,aAAAC;AAAA,EAAA,mBAAAC;AAAA;;;ACAA,SAAS,KAAAC,WAAS;AA2BX,IAAM,+BAAN,MAAM,sCACH,cAEV;AAAA,EACE,YAAY,UAAgD;AAC1D,UAAM,EAAE,SAAS,CAAC;AAAA,EACpB;AAAA,EAES,WAAW;AAAA,EACX,YAAY;AAAA,EAErB,MAAM,aACJ;AAAA,IACE;AAAA,IACA;AAAA,EACF,GAIA,SACA;AACA,UAAM,cAAc,MAAM,KAAK;AAAA,MAC7B;AAAA,QACE,eAAe,sBAAsB,QAAQ;AAAA,QAC7C,WAAW,+BAA+B,SAAS;AAAA,MACrD;AAAA,MACA;AAAA,IACF;AAEA,WAAO;AAAA,MACL;AAAA,MACA,eAAe,YAAY;AAAA,IAC7B;AAAA,EACF;AAAA,EAEA,MAAM,QACJ,OAIA,aACA;AACA,UAAM,EAAE,YAAY,IAAI,KAAK;AAC7B,UAAM,MAAM,KAAK,SAAS,OAAO,IAAI,2BAA2B;AAChE,UAAM,cAAc,YAAY,KAAK;AAErC,WAAO,yBAAyB;AAAA,MAC9B,OAAO,IAAI;AAAA,MACX,UAAU,IAAI;AAAA,MACd,MAAM,YAAY;AAChB,cAAM,WAAW,IAAI,SAAS;AAC9B,iBAAS;AAAA,UACP;AAAA,UACA,IAAI,KAAK,CAAC,MAAM,SAAS,CAAC;AAAA,UAC1B,SAAS,MAAM,aAAa;AAAA,QAC9B;AACA,iBAAS,OAAO,mBAAmB,MAAM;AAEzC,YAAI,eAAe,MAAM;AACvB,mBAAS,OAAO,eAAe,YAAY,SAAS,CAAC;AAAA,QACvD;AAEA,eAAO,UAAU;AAAA,UACf,KAAK,IAAI,YAAY,YAAY;AAAA,UACjC,SAAS,IAAI,QAAQ;AAAA,YACnB,cAAc,YAAY;AAAA,YAC1B,YAAY,YAAY;AAAA,YACxB,KAAK,YAAY;AAAA,YACjB,QAAQ,YAAY;AAAA,UACtB,CAAC;AAAA,UACD,MAAM;AAAA,YACJ,SAAS;AAAA,YACT,QAAQ,EAAE,YAAY;AAAA,UACxB;AAAA,UACA;AAAA,UACA;AAAA,UACA;AAAA,QACF,CAAC;AAAA,MACH;AAAA,IACF,CAAC;AAAA,EACH;AAAA,EAEA,IAAI,mBAAkE;AACpE,WAAO;AAAA,MACL,aAAa,KAAK,SAAS;AAAA,IAC7B;AAAA,EACF;AAAA,EAEA,aAAa,oBAA0D;AACrE,WAAO,IAAI;AAAA,MACT,OAAO,OAAO,CAAC,GAAG,KAAK,UAAU,kBAAkB;AAAA,IACrD;AAAA,EACF;AACF;AAEA,IAAM,oCAAoCC,IAAE,MAAM;AAAA,EAChDA,IAAE,OAAO,EAAE,MAAMA,IAAE,OAAO,EAAE,CAAC;AAAA,EAC7BA,IAAE,OAAO,EAAE,OAAOA,IAAE,OAAO,EAAE,CAAC;AAChC,CAAC;AAED,IAAM,4BAED,OAAO,EAAE,UAAU,KAAK,kBAAkB,MAAM;AACnD,QAAM,eAAe,MAAM,SAAS,KAAK;AAEzC,QAAM,eAAe,cAAc;AAAA,IACjC,MAAM;AAAA,IACN,QAAQ,UAAU,iCAAiC;AAAA,EACrD,CAAC;AAED,MAAI,CAAC,aAAa,SAAS;AACzB,UAAM,IAAI,aAAa;AAAA,MACrB,SAAS;AAAA,MACT,OAAO,aAAa;AAAA,MACpB,YAAY,SAAS;AAAA,MACrB;AAAA,MACA;AAAA,MACA;AAAA,IACF,CAAC;AAAA,EACH;AAEA,MAAI,WAAW,aAAa,OAAO;AACjC,UAAM,IAAI,aAAa;AAAA,MACrB,SAAS,aAAa,MAAM;AAAA,MAC5B,YAAY,SAAS;AAAA,MACrB;AAAA,MACA;AAAA,MACA;AAAA,IACF,CAAC;AAAA,EACH;AAEA,SAAO;AAAA,IACL,MAAM,aAAa,MAAM,KAAK,KAAK;AAAA,EACrC;AACF;AAEA,IAAM,wBAAuD,OAAO;AAAA,EAClE;AAAA,EACA;AAAA,EACA;AACF,MAAM;AACJ,QAAM,eAAe,MAAM,SAAS,KAAK;AAEzC,SAAO,IAAI,aAAa;AAAA,IACtB,SAAS;AAAA,IACT;AAAA,IACA;AAAA,IACA,YAAY,SAAS;AAAA,IACrB;AAAA,EACF,CAAC;AACH;;;ADtKO,SAASC,MAAI,UAAsD;AACxE,SAAO,IAAI,2BAA2B,QAAQ;AAChD;AAEO,SAASC,aACd,WAAiD,CAAC,GAClD;AACA,SAAO,IAAI,6BAA6B,QAAQ;AAClD;;;AEbO,IAAM,iCAAN,cAA6C,wBAAwB;AAAA,EAC1E,YAAY;AAAA,IACV,UAAU;AAAA,IACV;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,EACF,IAOI,CAAC,GAAG;AACN,UAAM;AAAA,MACJ;AAAA,MACA,SAAS;AAAA,QACP,eAAe,UAAU,WAAW;AAAA,UAClC,QAAQ;AAAA,UACR,yBAAyB;AAAA,UACzB,qBAAqB;AAAA,UACrB,aAAa;AAAA,QACf,CAAC,CAAC;AAAA,QACF,iBAAiB,UAAU,WAAW;AAAA,UACpC,QAAQ;AAAA,UACR,yBAAyB;AAAA,UACzB,qBAAqB;AAAA,UACrB,aAAa;AAAA,QACf,CAAC,CAAC;AAAA,MACJ;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,IACF,CAAC;AAAA,EACH;AACF;;;ACvCA,eAAsB,SACpB,WACA,OACA,SACmB;AACnB,SAAO,oBAAoB;AAAA,IACzB;AAAA,IACA,OAAO;AAAA,IACP,cAAc;AAAA,IACd,SAAS,CAACC,aAAY,UAAU,SAAS,OAAOA,QAAO;AAAA,IACvD,mBAAmB;AAAA,IACnB,oBAAoB;AAAA,EACtB,CAAC;AACH;;;ACZO,SAAS,iBAAiB;AAAA,EAC/B;AACF,GAEkB;AAChB,SAAO,OAAO,EAAE,MAAAC,OAAK,MAAwBA,OAAK,MAAM,SAAS;AACnE;;;ACPA,SAAS,iBAAiB;AAAA,EACxB;AAAA,EACA;AACF,GAGkB;AAChB,MAAI,SAAS,SAAS,cAAc;AAClC,WAAO,MAAM,QAAQ,QAAQ,IAAI,CAAC,SAAS,KAAK,EAAE,CAAC,IAAI,CAAC,QAAQ;AAAA,EAClE;AAEA,QAAM,OAAO,KAAK,KAAK,SAAS,SAAS,CAAC;AAC1C,QAAM,OAAO,SAAS,MAAM,GAAG,IAAI;AACnC,QAAM,QAAQ,SAAS,MAAM,IAAI;AAEjC,SAAO;AAAA,IACL,GAAG,iBAAiB;AAAA,MAClB,UAAU;AAAA,MACV;AAAA,IACF,CAAC;AAAA,IACD,GAAG,iBAAiB;AAAA,MAClB,UAAU;AAAA,MACV;AAAA,IACF,CAAC;AAAA,EACH;AACF;AAMO,IAAM,mBACX,CAAC;AAAA,EACC;AACF,MAGA,OAAO,EAAE,MAAAC,OAAK,MACZ,iBAAiB;AAAA,EACf,cAAc;AAAA,EACd,UAAUA;AACZ,CAAC;AAOE,IAAM,eACX,CAAC;AAAA,EACC;AAAA,EACA;AACF,MAIA,OAAO,EAAE,MAAAA,OAAK,MACZ,iBAAiB;AAAA,EACf,cAAc;AAAA,EACd,WAAW,MAAM,UAAU,kBAAkBA,MAAI,GAAG;AACtD,CAAC;;;AC7DL,eAAsB,gBACpB,eACA,QACkB;AAClB,QAAM,aAAa,MAAM,QAAQ;AAAA,IAC/B,OAAO,IAAI,CAAC,UAAU,eAAe,eAAe,KAAK,CAAC;AAAA,EAC5D;AACA,SAAO,WAAW,KAAK;AACzB;AAEA,eAAsB,eACpB,eACA,OACkB;AAClB,QAAM,QAAQ,MAAM,cAAc,KAAK;AACvC,SAAO,MAAM,IAAI,CAACC,YAAU;AAAA,IAC1B,GAAG;AAAA,IACH,MAAAA;AAAA,EACF,EAAE;AACJ;;;ACtBO,IAAM,4BAAN,cAAwC,MAAM;AAAA,EAC1C;AAAA,EACA;AAAA,EACA;AAAA,EAET,YAAY;AAAA,IACV;AAAA,IACA;AAAA,EACF,GAGG;AACD;AAAA,MACE,oBAAoB,QAAQ,4BACX,KAAK,UAAU,UAAU,CAAC;AAAA,IAC7C;AAEA,SAAK,OAAO;AAEZ,SAAK,WAAW;AAChB,SAAK,aAAa;AAAA,EACpB;AAAA,EAEA,SAAS;AACP,WAAO;AAAA,MACL,MAAM,KAAK;AAAA,MACX,SAAS,KAAK;AAAA,MACd,OAAO,KAAK;AAAA,MACZ,OAAO,KAAK;AAAA,MAEZ,UAAU,KAAK;AAAA,MACf,WAAW,KAAK;AAAA,IAClB;AAAA,EACF;AACF;;;ACzBO,IAAM,OAAN,MAEP;AAAA;AAAA;AAAA;AAAA;AAAA,EAKW;AAAA;AAAA;AAAA;AAAA,EAKA;AAAA;AAAA;AAAA;AAAA;AAAA,EAMA;AAAA;AAAA;AAAA;AAAA,EAKA;AAAA;AAAA;AAAA;AAAA,EAKA;AAAA,EAKT,YAAY;AAAA,IACV;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,EACF,GAMG;AACD,SAAK,OAAO;AACZ,SAAK,cAAc;AACnB,SAAK,aAAa;AAClB,SAAK,aAAa;AAClB,SAAK,UAAU;AAAA,EACjB;AACF;;;AChDO,IAAM,sBAAN,cAKG,KAA+B;AAAA,EACvC,YAAY;AAAA,IACV,OAAO;AAAA;AAAA,IACP;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,EACF,GAOG;AACD,UAAM;AAAA,MACJ;AAAA,MACA;AAAA,MACA;AAAA,MACA,SAAS,OAAO,OAAO,YACrB,eAAe;AAAA,QACb;AAAA,QACA,QAAQ;AAAA,QACR,QAAQ,OAAO,KAAK;AAAA,QACpB,GAAG;AAAA,MACL,CAAC;AAAA,IACL,CAAC;AAAA,EACH;AACF;;;ACxCO,IAAM,mCAAN,cAA+C,MAAM;AAAA,EACjD;AAAA,EACA;AAAA,EACA;AAAA,EAET,YAAY;AAAA,IACV;AAAA,IACA;AAAA,IACA;AAAA,EACF,GAIG;AACD;AAAA,MACE,wCAAwC,QAAQ;AAAA,aAChC,KAAK,UAAU,IAAI,CAAC;AAAA,iBAChB,gBAAgB,KAAK,CAAC;AAAA,IAC5C;AAEA,SAAK,OAAO;AAEZ,SAAK,WAAW;AAChB,SAAK,QAAQ;AACb,SAAK,OAAO;AAAA,EACd;AAAA,EAEA,SAAS;AACP,WAAO;AAAA,MACL,MAAM,KAAK;AAAA,MACX,SAAS,KAAK;AAAA,MACd,OAAO,KAAK;AAAA,MACZ,OAAO,KAAK;AAAA,MAEZ,UAAU,KAAK;AAAA,MACf,MAAM,KAAK;AAAA,IACb;AAAA,EACF;AACF;;;AC1CO,IAAM,gBAAN,cAA4B,MAAM;AAAA,EAC9B;AAAA,EACA;AAAA,EAET,YAAY;AAAA,IACV;AAAA,IACA;AAAA,IACA,UAAU,gBAAgB,KAAK;AAAA,EACjC,GAIG;AACD,UAAM,uBAAuB,SAAS,IAAI,aAAa,OAAO,EAAE;AAEhE,SAAK,OAAO;AAEZ,SAAK,WAAW;AAChB,SAAK,QAAQ;AAAA,EACf;AAAA,EAEA,SAAS;AACP,WAAO;AAAA,MACL,MAAM,KAAK;AAAA,MACX,OAAO,KAAK;AAAA,MACZ,SAAS,KAAK;AAAA,MACd,OAAO,KAAK;AAAA,MAEZ,UAAU,KAAK;AAAA,IACjB;AAAA,EACF;AACF;;;AChCO,IAAM,0BAAN,cAAsC,MAAM;AAAA,EACxC;AAAA,EACA;AAAA,EAET,YAAY,EAAE,UAAU,MAAM,GAAyC;AACrE;AAAA,MACE,yCAAyC,QAAQ,qBAC7B,gBAAgB,KAAK,CAAC;AAAA,IAC5C;AAEA,SAAK,OAAO;AAEZ,SAAK,WAAW;AAChB,SAAK,QAAQ;AAAA,EACf;AAAA,EAEA,SAAS;AACP,WAAO;AAAA,MACL,MAAM,KAAK;AAAA,MACX,SAAS,KAAK;AAAA,MACd,OAAO,KAAK;AAAA,MACZ,OAAO,KAAK;AAAA,MAEZ,UAAU,KAAK;AAAA,IACjB;AAAA,EACF;AACF;;;AC1BO,IAAM,qBAAN,cAAiC,MAAM;AAAA,EACnC;AAAA,EACA;AAAA,EACA;AAAA,EAET,YAAY;AAAA,IACV;AAAA,IACA;AAAA,IACA;AAAA,IACA,UAAU,gBAAgB,KAAK;AAAA,EACjC,GAKG;AACD,UAAM,yBAAyB,QAAQ,MAAM,OAAO,EAAE;AAEtD,SAAK,OAAO;AAEZ,SAAK,WAAW;AAChB,SAAK,QAAQ;AACb,SAAK,QAAQ;AAAA,EACf;AAAA,EAEA,SAAS;AACP,WAAO;AAAA,MACL,MAAM,KAAK;AAAA,MACX,OAAO,KAAK;AAAA,MACZ,SAAS,KAAK;AAAA,MACd,OAAO,KAAK;AAAA,MAEZ,UAAU,KAAK;AAAA,MACf,OAAO,KAAK;AAAA,IACd;AAAA,EACF;AACF;;;ACtCA,SAAS,KAAAC,WAAS;AAKlB,IAAM,qBAAqB;AAAA,EACzBC,IAAE,OAAO;AAAA,IACP,SAASA,IAAE;AAAA,MACTA,IAAE,OAAO;AAAA,QACP,OAAOA,IAAE,OAAO;AAAA,QAChB,MAAMA,IAAE,OAAO,EAAE,IAAI;AAAA,QACrB,SAASA,IAAE,OAAO;AAAA,MACpB,CAAC;AAAA,IACH;AAAA,EACF,CAAC;AACH;AAGA,IAAM,mBAAmB,CAAC;AAAA;AAAA,EAExB;AAAA,IACEA,IAAE,OAAO;AAAA,MACP,OAAOA,IAAE,OAAO,EAAE,SAAS,WAAW;AAAA,IACxC,CAAC;AAAA,EACH;AAAA;AAuCK,IAAM,gBAAN,cAAiD,KAItD;AAAA,EAIA,YAAY;AAAA,IACV;AAAA,IACA;AAAA,IACA,mBAAmB;AAAA,IACnB;AAAA,EACF,GAQG;AACD,UAAM;AAAA,MACJ;AAAA,MACA;AAAA,MACA,YAAY,iBAAiB,gBAAgB;AAAA,MAC7C,YAAY;AAAA,MACZ;AAAA,IACF,CAAC;AAAA,EACH;AACF;;;AC7FA,SAAS,UAAUC,iBAAgB;AA+CnC,eAAsB,YAA8C;AAAA,EAClE;AAAA,EACA;AAAA,EACA;AAAA,EACA,GAAG;AACL,GAUE;AACA,QAAM,eAAe,MAAM,cAAc,EAAE,MAAM,MAAM,GAAG,QAAQ,CAAC;AACnE,SAAO,eAAe,eAAe,aAAa;AACpD;AAGA,eAAe,cAAgD;AAAA,EAC7D;AAAA,EACA;AAAA,EACA,GAAG;AACL,GAMG;AACD,QAAM,MAAM,MAAM,OAAO,SAAS,GAAG;AAErC,QAAM,cAAc,IAAI,oBAAoB;AAAA,IAC1C,WAAW;AAAA,MACT,GAAG,sBAAsB,SAAS,WAAW,aAAa,CAAC;AAAA,MAC3D,GAAG,qBAAqB;AAAA,MACxB,GAAI,KAAK,oBAAoB,OAAO,CAAC,IAAI,gBAAgB,IAAI,CAAC;AAAA,MAC9D,GAAI,SAAS,aAAa,CAAC;AAAA,IAC7B;AAAA,IACA,cAAc,KAAK;AAAA,EACrB,CAAC;AAED,QAAM,sBAAsB,yBAAyB;AAErD,QAAM,WAAW;AAAA,IACf,cAAc;AAAA,IAEd,QAAQ,QAAQC,UAAS,CAAC;AAAA,IAC1B,cAAc,SAAS;AAAA,IACvB,OAAO,KAAK;AAAA,IACZ,WAAW,KAAK;AAAA,IAChB,QAAQ,KAAK;AAAA,IACb,YAAY,SAAS;AAAA,IAErB,UAAU,KAAK;AAAA,IACf,OAAO;AAAA,EACT;AAEA,cAAY,OAAO;AAAA,IACjB,GAAG;AAAA,IACH,WAAW;AAAA,IACX,WAAW,oBAAoB;AAAA,IAC/B,gBAAgB,oBAAoB;AAAA,EACtC,CAAC;AAED,QAAM,SAAS,MAAM;AAAA,IAAQ,MAC3B,KAAK,QAAQ,MAAM;AAAA,MACjB,cAAc,SAAS;AAAA,MACvB,QAAQ,SAAS;AAAA,MACjB,YAAY,SAAS;AAAA,MACrB,SAAS,SAAS;AAAA,MAClB,WAAW,SAAS;AAAA,MACpB;AAAA,IACF,CAAC;AAAA,EACH;AAEA,QAAM,iBAAiB;AAAA,IACrB,GAAG;AAAA,IACH,WAAW;AAAA,IACX,WAAW,oBAAI,KAAK;AAAA,IACpB,gBAAgB,oBAAoB;AAAA,IACpC,iBAAiB,oBAAI,KAAK;AAAA,IAC1B,cAAc,oBAAoB;AAAA,EACpC;AAEA,MAAI,CAAC,OAAO,IAAI;AACd,QAAI,OAAO,WAAW;AACpB,kBAAY,OAAO;AAAA,QACjB,GAAG;AAAA,QACH,QAAQ;AAAA,UACN,QAAQ;AAAA,QACV;AAAA,MACF,CAAC;AAED,YAAM,IAAI,WAAW;AAAA,IACvB;AAEA,gBAAY,OAAO;AAAA,MACjB,GAAG;AAAA,MACH,QAAQ;AAAA,QACN,QAAQ;AAAA,QACR,OAAO,OAAO;AAAA,MAChB;AAAA,IACF,CAAC;AAED,UAAM,IAAI,mBAAmB;AAAA,MAC3B,UAAU,KAAK;AAAA,MACf,OAAO;AAAA,MACP,OAAO,OAAO;AAAA;AAAA,MAEd,SAAU,OAAO,OAAe;AAAA,IAClC,CAAC;AAAA,EACH;AAEA,QAAM,SAAS,OAAO;AAEtB,cAAY,OAAO;AAAA,IACjB,GAAG;AAAA,IACH,QAAQ;AAAA,MACN,QAAQ;AAAA,MACR,OAAO;AAAA,IACT;AAAA,EACF,CAAC;AAED,SAAO;AAAA,IACL;AAAA,IACA,UAAU;AAAA,EACZ;AACF;;;ACvIA,eAAsB,iBAKpB;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA,GAAG;AACL,GAYE;AAEA,QAAM,iBACJ,OAAO,WAAW,aACb,OAA8D,IAAI,IACnE;AAEN,QAAM,eAAe,MAAM,oBAAoB;AAAA,IAC7C,cAAc;AAAA,IACd,OAAO;AAAA,IACP;AAAA,IACA;AAAA,IACA,kBAAkB,OAAOC,aAAY;AACnC,UAAI;AACF,cAAM,SAAS,MAAM,MAAM;AAAA,UACzB;AAAA,UACA;AAAA,UACAA;AAAA,QACF;AAEA,cAAM,WAAW,OAAO;AAExB,YAAI,aAAa,MAAM;AACrB,gBAAM,IAAI,wBAAwB;AAAA,YAChC,UAAU,KAAK;AAAA,YACf,OAAO;AAAA,UACT,CAAC;AAAA,QACH;AAEA,cAAM,cAAc,KAAK,WAAW,SAAS,SAAS,IAAI;AAE1D,YAAI,CAAC,YAAY,SAAS;AACxB,gBAAM,IAAI,iCAAiC;AAAA,YACzC,UAAU,KAAK;AAAA,YACf,MAAM,SAAS;AAAA,YACf,OAAO,YAAY;AAAA,UACrB,CAAC;AAAA,QACH;AAEA,eAAO;AAAA,UACL,aAAa,OAAO;AAAA,UACpB,gBAAgB;AAAA,YACd,IAAI,SAAS;AAAA,YACb,MAAM,KAAK;AAAA,YACX,MAAM,YAAY;AAAA,UACpB;AAAA,UACA,OAAO,OAAO;AAAA,QAChB;AAAA,MACF,SAAS,OAAO;AACd,YACE,iBAAiB,oCACjB,iBAAiB,yBACjB;AACA,gBAAM;AAAA,QACR;AAEA,cAAM,IAAI,wBAAwB;AAAA,UAChC,UAAU,KAAK;AAAA,UACf,OAAO;AAAA,QACT,CAAC;AAAA,MACH;AAAA,IACF;AAAA,EACF,CAAC;AAED,SAAO,eACH;AAAA,IACE,UAAU,aAAa;AAAA,IACvB,aAAa,aAAa;AAAA,IAC1B,UAAU,aAAa;AAAA,EACzB,IACA,aAAa;AACnB;;;ACxIA,SAAS,cAAc;AAMvB,IAAM,sBAAsB,CAAC,SAC3B;AAAA,EACE,iCAAiC,KAAK,IAAI;AAAA,EAC1C,KAAK,eAAe,OAChB,yBAAyB,KAAK,WAAW,KACzC;AAAA,EACJ,oCAAoC,KAAK;AAAA,IACvC,KAAK,WAAW,cAAc;AAAA,EAChC,CAAC;AAAA,EACD;AAAA,EACA;AACF,EACG,OAAO,OAAO,EACd,KAAK,IAAI;AAEP,IAAM,qBAAqB;AAAA,EAChC,KAAK;AAAA,IACH;AAAA,EACF,IAEI,CAAC,GAAsD;AACzD,WAAO;AAAA,MACL,aAAa,QAAgB,MAAuC;AAClE,eAAO;AAAA,UACL,QAAQC,oBAAmB,EAAE,MAAM,WAAW,CAAC;AAAA,UAC/C,aAAa;AAAA,QACf;AAAA,MACF;AAAA,MACA;AAAA,MACA,gBAAgB,CAAC,EAAE,OAAO,OAAO,MAAM,MAAM,eAAe,MAAM;AAAA,IACpE;AAAA,EACF;AAAA,EAEA,YAAY;AAAA,IACV;AAAA,EACF,IAEI,CAAC,GAAiE;AACpE,WAAO;AAAA,MACL,aACE,QACA,MACmB;AACnB,eAAO;AAAA,UACL,QAAQA,oBAAmB;AAAA,YACzB,sBAAsB,OAAO;AAAA,YAC7B;AAAA,YACA;AAAA,UACF,CAAC;AAAA,UACD,aAAa,OAAO;AAAA,QACtB;AAAA,MACF;AAAA,MACA;AAAA,MACA,gBAAgB,CAAC,EAAE,OAAO,OAAO,MAAM,MAAM,eAAe,MAAM;AAAA,IACpE;AAAA,EACF;AACF;AAEA,SAASA,oBAAmB;AAAA,EAC1B;AAAA,EACA,aAAa;AAAA,EACb;AACF,GAIG;AACD,SAAO;AAAA,IACL;AAAA,IACA,wBAAwB,OAAO,KAAK;AAAA,IACpC,WAAW,IAAI;AAAA,EACjB,EACG,OAAO,OAAO,EACd,KAAK,IAAI;AACd;AAEA,SAAS,gBAAgB,UAAkB;AACzC,SAAO,EAAE,IAAI,OAAO,GAAG,MAAM,UAAU,EAAE,MAAM,SAAS,CAAC,EAAE;AAC7D;;;ACnBA,eAAsB,kBAGpB;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA,GAAG;AACL,GAeE;AAEA,QAAM,iBACJ,OAAO,WAAW,aACb,OAAsC,KAAK,IAC5C;AAEN,QAAM,eAAe,MAAM,oBAMzB;AAAA,IACA,cAAc;AAAA,IACd,OAAO;AAAA,IACP;AAAA,IACA;AAAA,IACA,kBAAkB,OAAOC,aAAY;AACnC,YAAM,SAAS,MAAM,MAAM;AAAA,QACzB;AAAA,QACA;AAAA,QACAA;AAAA,MACF;AAEA,YAAM,EAAE,MAAAC,QAAM,WAAW,aAAa,IAAI;AAG1C,UAAI,gBAAgB,MAAM;AACxB,eAAO;AAAA,UACL,aAAa,OAAO;AAAA,UACpB,gBAAgB,EAAE,MAAAA,QAAM,WAAW,KAAK;AAAA,UACxC,OAAO,OAAO;AAAA,QAChB;AAAA,MACF;AAGA,YAAM,YAAY,aAAa,IAAI,CAAC,gBAAgB;AAClD,cAAM,OAAO,MAAM,KAAK,CAACC,UAASA,MAAK,SAAS,YAAY,IAAI;AAEhE,YAAI,QAAQ,QAAW;AACrB,gBAAM,IAAI,0BAA0B;AAAA,YAClC,UAAU,YAAY;AAAA,YACtB,YAAY,YAAY;AAAA,UAC1B,CAAC;AAAA,QACH;AAEA,cAAM,cAAc,KAAK,WAAW,SAAS,YAAY,IAAI;AAE7D,YAAI,CAAC,YAAY,SAAS;AACxB,gBAAM,IAAI,iCAAiC;AAAA,YACzC,UAAU,KAAK;AAAA,YACf,MAAM,YAAY;AAAA,YAClB,OAAO,YAAY;AAAA,UACrB,CAAC;AAAA,QACH;AAEA,eAAO;AAAA,UACL,IAAI,YAAY;AAAA,UAChB,MAAM,KAAK;AAAA,UACX,MAAM,YAAY;AAAA,QACpB;AAAA,MACF,CAAC;AAED,aAAO;AAAA,QACL,aAAa,OAAO;AAAA,QACpB,gBAAgB;AAAA,UACd,MAAAD;AAAA,UACA;AAAA,QACF;AAAA,QACA,OAAO,OAAO;AAAA,MAChB;AAAA,IACF;AAAA,EACF,CAAC;AAED,SAAO,eAAe,eAAe,aAAa;AACpD;;;AC7JA,eAAsB,oBAGpB,MACA,UACA,SAOA;AACA,MAAI;AACF,WAAO;AAAA,MACL,MAAM,SAAS;AAAA,MACf;AAAA,MACA,MAAM,SAAS;AAAA,MACf,IAAI;AAAA,MACJ,QAAQ,MAAM,YAAY,EAAE,MAAM,MAAM,SAAS,MAAM,GAAG,QAAQ,CAAC;AAAA,IACrE;AAAA,EACF,SAAS,OAAO;AAEd,QAAI,iBAAiB,SAAS,MAAM,SAAS,cAAc;AACzD,YAAM;AAAA,IACR;AAEA,WAAO;AAAA,MACL,MAAM,SAAS;AAAA,MACf;AAAA,MACA,MAAM,SAAS;AAAA,MACf,IAAI;AAAA,MACJ,QAAQ,IAAI,cAAc;AAAA,QACxB;AAAA,QACA,OAAO,iBAAiB,qBAAqB,MAAM,QAAQ;AAAA,MAC7D,CAAC;AAAA,IACH;AAAA,EACF;AACF;;;ACxBA,eAAsB,QAKpB;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA,GAAG;AACL,GAUE;AAEA,QAAM,iBACJ,OAAO,WAAW,aACb,OAAkC,IAAI,IACvC;AAEN,SAAO,oBAAoB;AAAA,IACzB;AAAA,IACA,OAAO;AAAA,IACP,cAAc;AAAA,IACd,SAAS,OAAOE,aACd;AAAA,MACE;AAAA,MACA,MAAM,iBAKJ,EAAE,OAAO,MAAM,QAAQ,gBAAgB,GAAGA,SAAQ,CAAC;AAAA,MACrDA;AAAA,IACF;AAAA,EACJ,CAAC;AACH;;;ACvBA,eAAsB,SAGpB;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA,GAAG;AACL,GAOG;AAED,QAAM,iBACJ,OAAO,WAAW,aACb,OAAoC,KAAK,IAC1C;AAEN,SAAO,oBAAoB;AAAA,IACzB;AAAA,IACA,OAAO;AAAA,IACP,cAAc;AAAA,IACd,SAAS,OAAOC,aAAY;AAC1B,YAAM,gBAAgB,MAAM,kBAAkB;AAAA,QAC5C;AAAA,QACA;AAAA,QACA,QAAQ;AAAA,QACR,cAAc;AAAA,QACd,GAAGA;AAAA,MACL,CAAC;AAED,YAAM,EAAE,WAAW,MAAAC,OAAK,IAAI;AAG5B,UAAI,aAAa,MAAM;AACrB,eAAO,EAAE,MAAAA,QAAM,aAAa,KAAK;AAAA,MACnC;AAGA,YAAM,cAAc,MAAM,QAAQ;AAAA,QAChC,UAAU,IAAI,OAAO,aAAa;AAChC,gBAAM,OAAO,MAAM,KAAK,CAACC,UAASA,MAAK,SAAS,SAAS,IAAI;AAE7D,cAAI,QAAQ,MAAM;AAChB,mBAAO;AAAA,cACL,MAAM,SAAS;AAAA,cACf;AAAA,cACA,MAAM,SAAS;AAAA,cACf,IAAI;AAAA,cACJ,QAAQ,IAAI,cAAc;AAAA,gBACxB,SAAS,sBAAsB,SAAS,IAAI;AAAA,gBAC5C;AAAA,cACF,CAAC;AAAA,YACH;AAAA,UACF;AAEA,iBAAO,MAAM;AAAA,YACX;AAAA,YACA;AAAA,YAIAF;AAAA,UACF;AAAA,QACF,CAAC;AAAA,MACH;AAEA,aAAO;AAAA,QACL,MAAAC;AAAA,QACA;AAAA,MACF;AAAA,IACF;AAAA,EACF,CAAC;AACH;;;ACvHA,IAAM,cAAc,IAAI,YAAY;AAE7B,SAAS,wBAAwB,QAAgC;AACtE,SAAO,IAAI,eAAe;AAAA,IACxB,MAAM,MAAM,YAAY;AACtB,UAAI;AACF,yBAAiB,SAAS,QAAQ;AAChC,qBAAW;AAAA,YACT,YAAY,OAAO,SAAS,KAAK,UAAU,KAAK,CAAC;AAAA;AAAA,CAAM;AAAA,UACzD;AAAA,QACF;AAAA,MACF,UAAE;AACA,mBAAW,MAAM;AAAA,MACnB;AAAA,IACF;AAAA,EACF,CAAC;AACH;;;ACDO,IAAM,uBAAN,MAAM,sBAEb;AAAA,EACmB;AAAA,EACA;AAAA,EAIA;AAAA,EAEjB,YAAY;AAAA,IACV;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,EACF,GAG0C;AACxC,SAAK,cAAc;AACnB,SAAK,iBAAiB;AACtB,SAAK,WAAW;AAAA,MACd;AAAA,MACA;AAAA,MACA;AAAA,IACF;AAAA,EACF;AAAA,EAEA,MAAM,SAAS,OAAc,SAA8C;AACzE,UAAM,YAAY,MAAM,MAAM;AAAA,MAC5B,OAAO,KAAK;AAAA,MACZ,OAAO;AAAA,MACP,GAAG;AAAA,IACL,CAAC;AAED,UAAM,cAAc,MAAM,KAAK,YAAY,cAAc;AAAA,MACvD,aAAa;AAAA,MACb,YAAY,KAAK,SAAS,cAAc;AAAA,MACxC,qBAAqB,KAAK,SAAS;AAAA,MACnC,QAAQ,KAAK,UAAU;AAAA,IACzB,CAAC;AAED,WAAO,YAAY,IAAI,CAAC,SAAS,KAAK,IAAI;AAAA,EAC5C;AAAA,EAEA,aACE,oBACM;AACN,WAAO,IAAI;AAAA,MACT,OAAO,OAAO,CAAC,GAAG,KAAK,UAAU,oBAAoB;AAAA,QACnD,aAAa,KAAK;AAAA,QAClB,gBAAgB,KAAK;AAAA,MACvB,CAAC;AAAA,IACH;AAAA,EACF;AACF;;;ACvEA,SAAS,KAAAE,WAAS;AAclB,IAAM,iBAAiB;AAAA,EACrBC,IAAE;AAAA,IACAA,IAAE,OAAO;AAAA,MACP,IAAIA,IAAE,OAAO;AAAA,MACb,QAAQA,IAAE,MAAMA,IAAE,OAAO,CAAC;AAAA,MAC1B,MAAMA,IAAE,QAAQ;AAAA,IAClB,CAAC;AAAA,EACH;AACF;AAOO,IAAM,oBAAN,MAAM,mBAGb;AAAA,EACE,aAAa,YAAkB;AAAA,IAC7B;AAAA,IACA;AAAA,EACF,GAGG;AAED,UAAMC,QAAO,UAAU,EAAE,MAAM,gBAAgB,QAAQ,eAAe,CAAC;AAEvE,QAAI,UAAU,MAAM;AAElB,iBAAW,SAASA,OAAM;AACxB,cAAM,mBAAmB,OAAO,SAAS,MAAM,IAAI;AACnD,YAAI,CAAC,iBAAiB,SAAS;AAC7B,gBAAM,iBAAiB;AAAA,QACzB;AAAA,MACF;AAAA,IACF;AAEA,UAAM,cAAc,IAAI,mBAAwB;AAEhD,gBAAY;AAAA,MACVA;AAAA,IAKF;AAEA,WAAO;AAAA,EACT;AAAA,EAEiB,UAAoC,oBAAI,IAAI;AAAA,EAE7D,MAAM,WACJ,MAKA;AACA,eAAW,SAAS,MAAM;AACxB,WAAK,QAAQ,IAAI,MAAM,IAAI,KAAK;AAAA,IAClC;AAAA,EACF;AAAA,EAEA,MAAM,cAAc;AAAA,IAClB;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,EACF,GAKoE;AAClE,UAAM,UAAU,CAAC,GAAG,KAAK,QAAQ,OAAO,CAAC,EACtC,OAAO,CAAC,UAAU,SAAS,MAAM,IAAI,KAAK,IAAI,EAC9C,IAAI,CAAC,WAAW;AAAA,MACf,IAAI,MAAM;AAAA,MACV,YAAY,iBAAiB,MAAM,QAAQ,WAAW;AAAA,MACtD,MAAM,MAAM;AAAA,IACd,EAAE,EACD;AAAA,MACC,CAAC,UACC,uBAAuB,UACvB,MAAM,cAAc,UACpB,MAAM,aAAa;AAAA,IACvB;AAEF,YAAQ,KAAK,CAAC,GAAG,MAAM,EAAE,aAAa,EAAE,UAAU;AAElD,WAAO,QAAQ,MAAM,GAAG,UAAU;AAAA,EACpC;AAAA,EAEA,YAAoB;AAClB,WAAO,KAAK,UAAU,CAAC,GAAG,KAAK,QAAQ,OAAO,CAAC,CAAC;AAAA,EAClD;AAAA,EAEA,UAAmC;AACjC,WAAO;AAAA,EACT;AACF;;;ACrHA,SAAS,UAAUC,iBAAgB;AAUnC,eAAsB,sBACpB;AAAA,EACE;AAAA,EACA;AAAA,EACA,aAAaC;AAAA,EACb;AAAA,EACA;AAAA,EACA;AACF,GAQA,SACA;AACA,SAAO,oBAAoB;AAAA,IACzB;AAAA,IACA,OAAO;AAAA,IACP,cAAc;AAAA,IACd,mBAAmB;AAAA,IACnB,SAAS,OAAOC,aAAY;AAE1B,YAAM,aAAa,MAAM,UAAU;AAAA,QACjC,OAAO;AAAA,QACP,QAAQ,QAAQ,IAAI,eAAe;AAAA,QACnC,GAAGA;AAAA,MACL,CAAC;AAED,YAAM,YAAY;AAAA,QAChB,QAAQ,IAAI,CAAC,QAAQ,OAAO;AAAA,UAC1B,IAAI,QAAQ,QAAQ,CAAC,KAAK,WAAW;AAAA,UACrC,QAAQ,WAAW,CAAC;AAAA,UACpB,MAAM;AAAA,QACR,EAAE;AAAA,MACJ;AAAA,IACF;AAAA,EACF,CAAC;AACH;","names":["createId","text","v","error","createId","options","text","zodSchema","text","createId","createId","options","options","options","text","options","createId","createId","finishMetadata","text","options","options","result","textGenerationResults","text","options","SecureJSON","SecureJSON","ignored","text","textEncoder","options","options","text","text","text","chat","instruction","text","text","instruction","chat","text","chat","instruction","text","END_SEGMENT","text","instruction","chat","content","chat","instruction","text","BEGIN_SEGMENT","END_SEGMENT","BEGIN_INSTRUCTION","END_INSTRUCTION","text","instruction","chat","chat","instruction","text","segmentStart","segment","text","instruction","chat","chat","instruction","text","text","instruction","chat","chat","instruction","text","text","instruction","chat","chat","instruction","text","text","instruction","chat","options","text","failedResponseHandler","successfulResponseHandler","z","z","z","z","Api","z","z","text","z","text","z","z","process","text","instruction","chat","z","Api","Api","z","WebSocket","error","text","z","Api","z","z","Api","TextEmbedder","TextGenerator","z","z","z","z","Api","TextGenerator","TextEmbedder","z","z","z","chat","instruction","text","DEFAULT_SYSTEM_MESSAGE","text","instruction","chat","z","text","z","z","Api","TextEmbedder","Tokenizer","z","text","z","Api","TextEmbedder","Tokenizer","Api","SpeechGenerator","z","text","z","Api","SpeechGenerator","z","text","instruction","chat","z","z","successfulResponseHandler","text","instruction","chat","z","Api","TextEmbedder","z","z","Api","TextEmbedder","z","text","instruction","chat","z","z","text","instruction","chat","z","z","Alpaca","ChatML","Llama2","Mistral","NeuralChat","Synthia","Text","Vicuna","Text","Mistral","ChatML","Llama2","NeuralChat","Alpaca","Synthia","Vicuna","Text","z","Api","ChatTextGenerator","CompletionTextGenerator","TextEmbedder","z","z","Api","CompletionTextGenerator","ChatTextGenerator","TextEmbedder","z","z","z","z","z","z","z","z","SecureJSON","text","instruction","chat","text","instruction","chat","SecureJSON","text","text","instruction","chat","text","instruction","chat","Api","ChatTextGenerator","CompletionTextGenerator","ImageGenerator","SpeechGenerator","TextEmbedder","Tokenizer","z","z","text","z","openAITextEmbeddingResponseSchema","z","z","z","Api","CompletionTextGenerator","ChatTextGenerator","TextEmbedder","SpeechGenerator","ImageGenerator","Tokenizer","text","instruction","chat","text","instruction","chat","ChatTextGenerator","CompletionTextGenerator","TextEmbedder","CompletionTextGenerator","ChatTextGenerator","TextEmbedder","z","z","Api","ImageGenerator","z","z","Api","ImageGenerator","Api","Transcriber","z","z","Api","Transcriber","options","text","text","text","z","z","createId","createId","options","createSystemPrompt","options","text","tool","options","options","text","tool","z","z","json","createId","createId","options"]}